{"source": "arxiv", "arxiv_id": "2512.08763v1", "url": "https://arxiv.org/abs/2512.08763v1", "title": "Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning", "authors": ["Jinfeng Xu", "Zheyu Chen", "Shuo Yang", "Jinze Li", "Hewei Wang", "Yijie Li", "Edith C. H. Ngai"], "abstract": "Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T16:12:21Z", "updated_at": "2025-12-09T16:12:21Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:31.565807+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08601v1", "url": "https://arxiv.org/abs/2512.08601v1", "title": "Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis", "authors": ["Orit Davidovich", "Shimrit Shtern", "Segev Wasserkrug", "Nimrod Megiddo"], "abstract": "Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.", "categories": ["stat.ML", "cs.LG", "math.OC"], "submitted_at": "2025-12-09T13:40:08Z", "updated_at": "2025-12-09T13:40:08Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:31.573149+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06912v2", "url": "https://arxiv.org/abs/2512.06912v2", "title": "Khalasi: Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields", "authors": ["Rushiraj Gadhvi", "Sandeep Manjanna"], "abstract": "For centuries, khalasi (Gujarati for sailor) have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-07T16:36:31Z", "updated_at": "2025-12-09T03:48:56Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:31.593337+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08855v1", "url": "https://arxiv.org/abs/2512.08855v1", "title": "Reinforcement Learning From State and Temporal Differences", "authors": ["Lex Weaver", "Jonathan Baxter"], "abstract": "TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T17:48:28Z", "updated_at": "2025-12-09T17:48:28Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:31.618652+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06791v1", "url": "https://arxiv.org/abs/2512.06791v1", "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games", "authors": ["Vedansh Sharma"], "abstract": "Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.", "categories": ["cs.LG", "cs.GT", "math.OC"], "submitted_at": "2025-12-07T11:11:36Z", "updated_at": "2025-12-07T11:11:36Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:31.621160+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06252v1", "url": "https://arxiv.org/abs/2512.06252v1", "title": "Learning Without Time-Based Embodiment Resets in Soft-Actor Critic", "authors": ["Homayoon Farrahi", "A. Rupam Mahmood"], "abstract": "When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $γ$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T02:53:55Z", "updated_at": "2025-12-06T02:53:55Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.929, "collected_at": "2025-12-10T03:47:31.626711+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05320v1", "url": "https://arxiv.org/abs/2512.05320v1", "title": "Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay", "authors": ["Mehmet Efe Lorasdagi", "Dogan Can Cicek", "Furkan Burak Mutlu", "Suleyman Serdar Kozat"], "abstract": "Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.\n  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.\n  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.\n  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.\n  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.", "categories": ["cs.LG"], "submitted_at": "2025-12-04T23:37:29Z", "updated_at": "2025-12-04T23:37:29Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.629934+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08485v1", "url": "https://arxiv.org/abs/2512.08485v1", "title": "Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning", "authors": ["Junnan Qiu", "Jie Li"], "abstract": "Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T11:04:37Z", "updated_at": "2025-12-09T11:04:37Z", "rl_tags": ["deep_rl", "offline_rl"], "attention_score": 1.1, "collected_at": "2025-12-10T03:47:31.654368+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05291v1", "url": "https://arxiv.org/abs/2512.05291v1", "title": "Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces", "authors": ["Na Li", "Hangguan Shan", "Wei Ni", "Wenjie Zhang", "Xinyu Li"], "abstract": "Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.", "categories": ["cs.LG"], "submitted_at": "2025-12-04T22:28:43Z", "updated_at": "2025-12-04T22:28:43Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.657372+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04601v1", "url": "https://arxiv.org/abs/2512.04601v1", "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space", "authors": ["Joey Hong", "Kang Liu", "Zhan Ling", "Jiecao Chen", "Sergey Levine"], "abstract": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-04T09:21:44Z", "updated_at": "2025-12-04T09:21:44Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.886, "collected_at": "2025-12-10T03:47:31.660115+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04559v1", "url": "https://arxiv.org/abs/2512.04559v1", "title": "Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function", "authors": ["Hyeongyu Kang", "Jaewoo Lee", "Woocheol Shin", "Kiyoung Om", "Jinkyoo Park"], "abstract": "Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \\textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-04T08:21:52Z", "updated_at": "2025-12-04T08:21:52Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.661672+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03729v1", "url": "https://arxiv.org/abs/2512.03729v1", "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing", "authors": ["Samantha Chapin", "Kenneth Stewart", "Roxana Leontie", "Carl Glen Henshaw"], "abstract": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.", "categories": ["cs.RO", "cs.LG", "eess.SY"], "submitted_at": "2025-12-03T12:16:52Z", "updated_at": "2025-12-03T12:16:52Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.843, "collected_at": "2025-12-10T03:47:31.664696+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08463v1", "url": "https://arxiv.org/abs/2512.08463v1", "title": "Using reinforcement learning to probe the role of feedback in skill acquisition", "authors": ["Antonio Terpin", "Raffaello D'Andrea"], "abstract": "Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.", "categories": ["cs.AI", "cs.LG", "cs.RO", "eess.SY"], "submitted_at": "2025-12-09T10:37:42Z", "updated_at": "2025-12-09T10:37:42Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:31.671775+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08341v1", "url": "https://arxiv.org/abs/2512.08341v1", "title": "Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks", "authors": ["Thai Duong Nguyen", "Ngoc-Tan Nguyen", "Thanh-Dao Nguyen", "Nguyen Van Huynh", "Dinh-Hieu Tran", "Symeon Chatzinotas"], "abstract": "The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.", "categories": ["cs.NI", "cs.LG", "cs.MA"], "submitted_at": "2025-12-09T08:11:21Z", "updated_at": "2025-12-09T08:11:21Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:31.674790+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08012v1", "url": "https://arxiv.org/abs/2512.08012v1", "title": "Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care", "authors": ["Aryaman Bansal", "Divya Sharma"], "abstract": "In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.\n  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T20:09:15Z", "updated_at": "2025-12-08T20:09:15Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:31.676792+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07761v1", "url": "https://arxiv.org/abs/2512.07761v1", "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "abstract": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-08T17:42:59Z", "updated_at": "2025-12-08T17:42:59Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:31.679279+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07710v1", "url": "https://arxiv.org/abs/2512.07710v1", "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "abstract": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-08T16:57:43Z", "updated_at": "2025-12-08T16:57:43Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:31.683942+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07528v1", "url": "https://arxiv.org/abs/2512.07528v1", "title": "Model-Based Reinforcement Learning Under Confounding", "authors": ["Nishanth Venkatesh", "Andreas A. Malikopoulos"], "abstract": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T13:02:00Z", "updated_at": "2025-12-08T13:02:00Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:31.686180+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07417v1", "url": "https://arxiv.org/abs/2512.07417v1", "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning", "authors": ["Giray Önür", "Azita Dabiri", "Bart De Schutter"], "abstract": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T10:52:00Z", "updated_at": "2025-12-08T10:52:00Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:31.689197+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07342v1", "url": "https://arxiv.org/abs/2512.07342v1", "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning", "authors": ["Chen Gong", "Zheng Liu", "Kecen Li", "Tianhao Wang"], "abstract": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.\n  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.", "categories": ["cs.CR", "cs.LG"], "submitted_at": "2025-12-08T09:29:24Z", "updated_at": "2025-12-08T09:29:24Z", "rl_tags": ["deep_rl", "offline_rl", "exploration", "general_dl"], "attention_score": 1.257, "collected_at": "2025-12-10T03:47:31.691197+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06956v1", "url": "https://arxiv.org/abs/2512.06956v1", "title": "Statistical analysis of Inverse Entropy-regularized Reinforcement Learning", "authors": ["Denis Belomestny", "Alexey Naumov", "Sergey Samsonov"], "abstract": "Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.", "categories": ["stat.ML", "cs.LG", "math.ST"], "submitted_at": "2025-12-07T18:26:19Z", "updated_at": "2025-12-07T18:26:19Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:31.692720+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06925v1", "url": "https://arxiv.org/abs/2512.06925v1", "title": "Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features", "authors": ["Aseer Al Faisal"], "abstract": "Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "submitted_at": "2025-12-07T17:08:12Z", "updated_at": "2025-12-07T17:08:12Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:31.695720+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06920v1", "url": "https://arxiv.org/abs/2512.06920v1", "title": "Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models", "authors": ["Alexandr Plashchinsky"], "abstract": "We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T16:58:22Z", "updated_at": "2025-12-07T16:58:22Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:31.697720+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06917v1", "url": "https://arxiv.org/abs/2512.06917v1", "title": "Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis", "authors": ["Clifford F", "Devika Jay", "Abhishek Sarkar", "Satheesh K Perepu", "Santhosh G S", "Kaushik Dey", "Balaraman Ravindran"], "abstract": "As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a \"radical term\" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful \"Why this, and not that?\" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T16:52:08Z", "updated_at": "2025-12-07T16:52:08Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:31.699723+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06533v1", "url": "https://arxiv.org/abs/2512.06533v1", "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning", "authors": ["Ming Chen", "Sheng Tang", "Rong-Xi Tan", "Ziniu Li", "Jiacheng Chen", "Ke Xue", "Chao Qian"], "abstract": "Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-06T18:57:38Z", "updated_at": "2025-12-06T18:57:38Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:31.703365+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06471v1", "url": "https://arxiv.org/abs/2512.06471v1", "title": "Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control", "authors": ["Nathan P. Lawrence", "Ali Mesbah"], "abstract": "Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-06T15:28:35Z", "updated_at": "2025-12-06T15:28:35Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:31.704386+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06392v1", "url": "https://arxiv.org/abs/2512.06392v1", "title": "RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs", "authors": ["Runlong Zhou", "Lefan Zhang", "Shang-Chen Wu", "Kelvin Zou", "Hanzhi Zhou", "Ke Ye", "Yihao Feng", "Dong Yin", "Alex Guillen Garcia", "Dmytro Babych", "Rohit Chatterjee", "Matthew Hopkins", "Xiang Kong", "Chang Lan", "Lezhi Li", "Yiping Ma", "Daniele Molinari", "Senyu Tong", "Yanchao Sun", "Thomas Voice", "Jianyu Wang", "Chong Wang", "Simon Wang", "Floris Weers", "Yechen Xu", "Guolin Yin", "Muyang Yu", "Yi Zhang", "Zheng Zhou", "Danyang Zhuo", "Ruoming Pang", "Cheng Leong"], "abstract": "Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-06T10:48:51Z", "updated_at": "2025-12-06T10:48:51Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:31.706378+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06351v1", "url": "https://arxiv.org/abs/2512.06351v1", "title": "LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing", "authors": ["Zhiying Yang", "Fang Liu", "Wei Zhang", "Xin Lou", "Malcolm Yoke Hean Low", "Boon Ping Gan"], "abstract": "This paper presents \\textsc{Luca}, a \\underline{l}arge language model (LLM)-\\underline{u}pgraded graph reinforcement learning framework for \\underline{c}arbon-\\underline{a}ware flexible job shop scheduling. \\textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \\textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \\textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\\% and up to 12.2\\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \\textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-06T08:53:30Z", "updated_at": "2025-12-06T08:53:30Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:31.709380+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06274v1", "url": "https://arxiv.org/abs/2512.06274v1", "title": "Networked Restless Multi-Arm Bandits with Reinforcement Learning", "authors": ["Hanmo Zhang", "Zenghui Sun", "Kai Wang"], "abstract": "Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-06T03:53:25Z", "updated_at": "2025-12-06T03:53:25Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:31.711907+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06250v1", "url": "https://arxiv.org/abs/2512.06250v1", "title": "Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning", "authors": ["Chris Tava"], "abstract": "Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\\times$16 to 128$\\times$128 $\\times$ 10 unique mazes $\\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\\% threshold baselines. Results show 23-55\\% improvements in completion time, 83\\% reduction in runtime variance, and 71\\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\\% improvement for 16$\\times$16 mazes, 34\\% for 32$\\times$32, and 55\\% for 64$\\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T02:50:32Z", "updated_at": "2025-12-06T02:50:32Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.929, "collected_at": "2025-12-10T03:47:31.715433+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06244v1", "url": "https://arxiv.org/abs/2512.06244v1", "title": "Auto-exploration for online reinforcement learning", "authors": ["Caleb Ju", "Guanghui Lan"], "abstract": "The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.", "categories": ["cs.LG", "cs.AI", "math.OC"], "submitted_at": "2025-12-06T02:04:50Z", "updated_at": "2025-12-06T02:04:50Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.929, "collected_at": "2025-12-10T03:47:31.718429+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06218v1", "url": "https://arxiv.org/abs/2512.06218v1", "title": "Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration", "authors": ["Huizhen Yu", "Yi Wan", "Richard S. Sutton"], "abstract": "This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.", "categories": ["cs.LG", "math.OC"], "submitted_at": "2025-12-05T23:49:07Z", "updated_at": "2025-12-05T23:49:07Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:31.720941+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06204v1", "url": "https://arxiv.org/abs/2512.06204v1", "title": "Quantifying Memory Use in Reinforcement Learning with Temporal Range", "authors": ["Rodney Lafuente-Mercado", "Daniela Rus", "T. Konstantin Rusch"], "abstract": "How much does a trained RL policy actually use its past observations? We propose \\emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\\partial y_s/\\partial x_t\\in\\mathbb{R}^{c\\times d}$ averaged over final timesteps $s\\in\\{t+1,\\dots,T\\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-05T22:58:09Z", "updated_at": "2025-12-05T22:58:09Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:31.724490+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06102v1", "url": "https://arxiv.org/abs/2512.06102v1", "title": "JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning", "authors": ["Ufuk Çakır", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "abstract": "Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-05T19:06:07Z", "updated_at": "2025-12-05T19:06:07Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:31.725485+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05753v1", "url": "https://arxiv.org/abs/2512.05753v1", "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning", "authors": ["Wencheng Cai", "Xuchao Gao", "Congying Han", "Mingqiang Li", "Tiande Guo"], "abstract": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-05T14:39:50Z", "updated_at": "2025-12-05T14:39:50Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:31.732570+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05591v1", "url": "https://arxiv.org/abs/2512.05591v1", "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning", "authors": ["Zhenpeng Su", "Leiyu Pan", "Minxuan Lv", "Tiehua Mei", "Zijia Lin", "Yuntao Li", "Wenping Hu", "Ruiming Tang", "Kun Gai", "Guorui Zhou"], "abstract": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \\textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-05T10:26:32Z", "updated_at": "2025-12-05T10:26:32Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.929, "collected_at": "2025-12-10T03:47:31.739145+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05207v1", "url": "https://arxiv.org/abs/2512.05207v1", "title": "Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem", "authors": ["Ali Al Housseini", "Cristina Rottondi", "Omran Ayoub"], "abstract": "Virtual Network Embedding (VNE) is a key enabler of network slicing, yet most formulations assume that each Virtual Network Request (VNR) has a fixed topology. Recently, VNE with Alternative topologies (VNEAP) was introduced to capture malleable VNRs, where each request can be instantiated using one of several functionally equivalent topologies that trade resources differently. While this flexibility enlarges the feasible space, it also introduces an additional decision layer, making dynamic embedding more challenging. This paper proposes HRL-VNEAP, a hierarchical reinforcement learning approach for VNEAP under dynamic arrivals. A high-level policy selects the most suitable alternative topology (or rejects the request), and a low-level policy embeds the chosen topology onto the substrate network. Experiments on realistic substrate topologies under multiple traffic loads show that naive exploitation strategies provide only modest gains, whereas HRL-VNEAP consistently achieves the best performance across all metrics. Compared to the strongest tested baselines, HRL-VNEAP improves acceptance ratio by up to \\textbf{20.7\\%}, total revenue by up to \\textbf{36.2\\%}, and revenue-over-cost by up to \\textbf{22.1\\%}. Finally, we benchmark against an MILP formulation on tractable instances to quantify the remaining gap to optimality and motivate future work on learning- and optimization-based VNEAP solutions.", "categories": ["cs.NI", "cs.LG", "cs.MA"], "submitted_at": "2025-12-04T19:22:40Z", "updated_at": "2025-12-04T19:22:40Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.743695+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05105v1", "url": "https://arxiv.org/abs/2512.05105v1", "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "eess.SP"], "submitted_at": "2025-12-04T18:59:18Z", "updated_at": "2025-12-04T18:59:18Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.746528+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05100v1", "url": "https://arxiv.org/abs/2512.05100v1", "title": "Structured Document Translation via Format Reinforcement Learning", "authors": ["Haiyue Song", "Johannes Eschbach-Dymanus", "Hour Kaing", "Sumire Honda", "Hideki Tanaka", "Bianka Buschbeck", "Masao Utiyama"], "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-04T18:58:30Z", "updated_at": "2025-12-04T18:58:30Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.748002+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04958v1", "url": "https://arxiv.org/abs/2512.04958v1", "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning", "authors": ["Roberto Cipollone", "Luca Iocchi", "Matteo Leonetti"], "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-04T16:26:56Z", "updated_at": "2025-12-04T16:26:56Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.753091+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04949v1", "url": "https://arxiv.org/abs/2512.04949v1", "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent", "authors": ["Leyang Shen", "Yang Zhang", "Chun Kai Ling", "Xiaoyan Zhao", "Tat-Seng Chua"], "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-12-04T16:15:46Z", "updated_at": "2025-12-04T16:15:46Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.756474+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04918v1", "url": "https://arxiv.org/abs/2512.04918v1", "title": "Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty", "authors": ["Kailiang Liu", "Ying Chen", "Ralf Borndörfer", "Thorsten Koch"], "abstract": "Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.", "categories": ["cs.LG"], "submitted_at": "2025-12-04T15:47:08Z", "updated_at": "2025-12-04T15:47:08Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.759022+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04697v1", "url": "https://arxiv.org/abs/2512.04697v1", "title": "Continuous-time reinforcement learning for optimal switching over multiple regimes", "authors": ["Yijie Huang", "Mengge Li", "Xiang Yu", "Zhou Zhou"], "abstract": "This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.", "categories": ["math.OC", "cs.LG", "q-fin.CP"], "submitted_at": "2025-12-04T11:48:07Z", "updated_at": "2025-12-04T11:48:07Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.761039+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04653v1", "url": "https://arxiv.org/abs/2512.04653v1", "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control", "authors": ["Pouria Yazdani", "Arash Rezaali", "Monireh Abdoos"], "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "submitted_at": "2025-12-04T10:26:43Z", "updated_at": "2025-12-04T10:26:43Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:31.764087+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04368v1", "url": "https://arxiv.org/abs/2512.04368v1", "title": "AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning", "authors": ["Praveen Anugula", "Avdhesh Kumar Bhardwaj", "Navin Chhibber", "Rohit Tewari", "Sunil Khemka", "Piyush Ranjan"], "abstract": "Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.\n  Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF"], "submitted_at": "2025-12-04T01:31:46Z", "updated_at": "2025-12-04T01:31:46Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:31.766084+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04341v1", "url": "https://arxiv.org/abs/2512.04341v1", "title": "Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism", "authors": ["Tianwei Ni", "Esther Derman", "Vineet Jain", "Vincent Taboga", "Siamak Ravanbakhsh", "Pierre-Luc Bacon"], "abstract": "Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting planning horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale the principle to realistic tasks, identifying key design choices, such as layer normalization in the world model and adaptive long-horizon planning, that mitigate compounding error and value overestimation. These yield our practical algorithm, Neubay, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, Neubay generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with planning horizons of several hundred steps, challenging common belief. Finally, we characterize when Neubay is preferable to conservatism, laying the foundation for a new direction in offline and model-based RL.", "categories": ["cs.LG"], "submitted_at": "2025-12-04T00:07:08Z", "updated_at": "2025-12-04T00:07:08Z", "rl_tags": ["deep_rl", "offline_rl"], "attention_score": 0.843, "collected_at": "2025-12-10T03:47:31.768085+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04332v2", "url": "https://arxiv.org/abs/2512.04332v2", "title": "Data-regularized Reinforcement Learning for Diffusion Models at Scale", "authors": ["Haotian Ye", "Kaiwen Zheng", "Jiashu Xu", "Puheng Li", "Huayu Chen", "Jiaqi Han", "Sheng Liu", "Qinsheng Zhang", "Hanzi Mao", "Zekun Hao", "Prithvijit Chattopadhyay", "Dinghao Yang", "Liang Feng", "Maosheng Liao", "Junjie Bai", "Ming-Yu Liu", "James Zou", "Stefano Ermon"], "abstract": "Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.", "categories": ["cs.LG"], "submitted_at": "2025-12-03T23:45:07Z", "updated_at": "2025-12-06T23:45:13Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:31.771598+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03973v1", "url": "https://arxiv.org/abs/2512.03973v1", "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning", "authors": ["Franki Nguimatsia Tiofack", "Théotime Le Hellard", "Fabian Schramm", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "abstract": "Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-03T17:05:58Z", "updated_at": "2025-12-03T17:05:58Z", "rl_tags": ["deep_rl", "offline_rl"], "attention_score": 0.843, "collected_at": "2025-12-10T03:47:31.773609+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03911v1", "url": "https://arxiv.org/abs/2512.03911v1", "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "authors": ["Kenneth Stewart", "Roxana Leontie", "Samantha Chapin", "Joe Hays", "Sumit Bam Shrestha", "Carl Glen Henshaw"], "abstract": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "submitted_at": "2025-12-03T15:56:39Z", "updated_at": "2025-12-03T15:56:39Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:31.774609+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03891v1", "url": "https://arxiv.org/abs/2512.03891v1", "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning", "authors": ["Ying-Kuan Tsai", "Yi-Ping Chen", "Vispi Karkaria", "Wei Chen"], "abstract": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-03T15:41:35Z", "updated_at": "2025-12-03T15:41:35Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:31.778611+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03805v1", "url": "https://arxiv.org/abs/2512.03805v1", "title": "Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA", "authors": ["Tai Nguyen", "Phong Le", "André Biedenkapp", "Carola Doerr", "Nguyen Dang"], "abstract": "Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.", "categories": ["cs.LG"], "submitted_at": "2025-12-03T13:54:41Z", "updated_at": "2025-12-03T13:54:41Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.843, "collected_at": "2025-12-10T03:47:31.781265+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03525v1", "url": "https://arxiv.org/abs/2512.03525v1", "title": "Adaptive sampling using variational autoencoder and reinforcement learning", "authors": ["Adil Rasheed", "Mikael Aleksander Jansen Shahly", "Muhammad Faisal Aftab"], "abstract": "Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.", "categories": ["cs.LG"], "submitted_at": "2025-12-03T07:32:25Z", "updated_at": "2025-12-03T07:32:25Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:31.791901+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08931v1", "url": "https://arxiv.org/abs/2512.08931v1", "title": "Astra: General Interactive World Model with Autoregressive Denoising", "authors": ["Yixuan Zhu", "Jiaqi Feng", "Wenzhao Zheng", "Yuan Gao", "Xin Tao", "Pengfei Wan", "Jie Zhou", "Jiwen Lu"], "abstract": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T18:59:57Z", "updated_at": "2025-12-09T18:59:57Z", "rl_tags": ["exploration"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:32.712838+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08659v1", "url": "https://arxiv.org/abs/2512.08659v1", "title": "An Agentic AI System for Multi-Framework Communication Coding", "authors": ["Bohao Yang", "Rui Yang", "Joshua M. Biro", "Haoyuan Wang", "Jessica L. Handley", "Brianna Richardson", "Sophia Bessias", "Nicoleta Economou-Zavlanos", "Armando D. Bedoya", "Monica Agrawal", "Michael M. Zavlanos", "Anand Chowdhury", "Raj M. Ratwani", "Kai Sun", "Kathryn I. Pollak", "Michael J. Pencina", "Chuan Hong"], "abstract": "Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.", "categories": ["cs.CL", "cs.LG"], "submitted_at": "2025-12-09T14:46:16Z", "updated_at": "2025-12-09T14:46:16Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:32.719852+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08271v1", "url": "https://arxiv.org/abs/2512.08271v1", "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation", "authors": ["Srijan Dokania", "Dharini Raghavan"], "abstract": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.", "categories": ["cs.RO", "cs.CV", "cs.LG", "eess.IV"], "submitted_at": "2025-12-09T05:59:38Z", "updated_at": "2025-12-09T05:59:38Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:32.725906+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08216v1", "url": "https://arxiv.org/abs/2512.08216v1", "title": "Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation", "authors": ["Aneesh Rangnekar", "Harini Veeraraghavan"], "abstract": "Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.", "categories": ["eess.IV", "cs.CV", "cs.LG"], "submitted_at": "2025-12-09T03:49:50Z", "updated_at": "2025-12-09T03:49:50Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:32.732409+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08108v1", "url": "https://arxiv.org/abs/2512.08108v1", "title": "Scalable Offline Model-Based RL with Action Chunks", "authors": ["Kwanyoung Park", "Seohong Park", "Youngwoon Lee", "Sergey Levine"], "abstract": "In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \\emph{action-chunk} model that predicts a future state from a sequence of actions (an \"action chunk\") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \\textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T23:26:29Z", "updated_at": "2025-12-08T23:26:29Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:32.739426+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08029v1", "url": "https://arxiv.org/abs/2512.08029v1", "title": "CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space", "authors": ["Tianxingjian Ding", "Yuanhao Zou", "Chen Chen", "Mubarak Shah", "Yu Tian"], "abstract": "Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\\%, and significantly surpasses all other medical-specific large language models.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-08T20:42:10Z", "updated_at": "2025-12-08T20:42:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:32.745750+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07564v1", "url": "https://arxiv.org/abs/2512.07564v1", "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "authors": ["Kassoum Sanogo", "Renzo Ardiccioni"], "abstract": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-08T13:58:46Z", "updated_at": "2025-12-08T13:58:46Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:32.746751+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07437v1", "url": "https://arxiv.org/abs/2512.07437v1", "title": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models", "authors": ["Chenwei Shi", "Xueyu Luan"], "abstract": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO"], "submitted_at": "2025-12-08T11:13:15Z", "updated_at": "2025-12-08T11:13:15Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:32.758900+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07314v1", "url": "https://arxiv.org/abs/2512.07314v1", "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling", "authors": ["Yuxiao Luo", "Songming Zhang", "Sijie Ruan", "Siran Chen", "Kang Liu", "Yang Xu", "Yu Zheng", "Ling Yin"], "abstract": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-08T08:57:55Z", "updated_at": "2025-12-08T08:57:55Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:32.761915+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07079v1", "url": "https://arxiv.org/abs/2512.07079v1", "title": "Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation", "authors": ["Anton Morgunov", "Victor S. Batista"], "abstract": "Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between \"solvability\" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a \"complexity cliff\" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T01:26:39Z", "updated_at": "2025-12-08T01:26:39Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:32.767587+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07051v1", "url": "https://arxiv.org/abs/2512.07051v1", "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation", "authors": ["Adnan Munir", "Shujaat Khan"], "abstract": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-07T23:57:00Z", "updated_at": "2025-12-07T23:57:00Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:32.769582+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06983v1", "url": "https://arxiv.org/abs/2512.06983v1", "title": "On Memory: A comparison of memory mechanisms in world models", "authors": ["Eli J. Laird", "Corey Clark"], "abstract": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-07T20:29:20Z", "updated_at": "2025-12-07T20:29:20Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:32.772625+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06714v1", "url": "https://arxiv.org/abs/2512.06714v1", "title": "A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting", "authors": ["Tony Salloom", "Okyay Kaynak", "Wei He"], "abstract": "Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "submitted_at": "2025-12-07T08:08:49Z", "updated_at": "2025-12-07T08:08:49Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:32.781135+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06649v1", "url": "https://arxiv.org/abs/2512.06649v1", "title": "Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning", "authors": ["Camellia Zakaria", "Aryan Sadeghi", "Weaam Jaafar", "Junshi Xu", "Alex Mariakakis", "Marianne Hatzopoulou"], "abstract": "Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.", "categories": ["cs.LG", "cs.CV", "cs.CY", "cs.ET"], "submitted_at": "2025-12-07T04:14:28Z", "updated_at": "2025-12-07T04:14:28Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:32.784690+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06563v1", "url": "https://arxiv.org/abs/2512.06563v1", "title": "Deep Manifold Part 2: Neural Network Mathematics", "authors": ["Max Y. Ma", "Gen-Hua Shi"], "abstract": "This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-06T20:44:24Z", "updated_at": "2025-12-06T20:44:24Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:32.786688+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06417v1", "url": "https://arxiv.org/abs/2512.06417v1", "title": "Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator", "authors": ["Yifan Sun", "Lei Cheng", "Jianlong Li", "Peter Gerstoft"], "abstract": "Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.", "categories": ["cs.LG", "cs.SD"], "submitted_at": "2025-12-06T12:24:15Z", "updated_at": "2025-12-06T12:24:15Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:32.788687+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06259v1", "url": "https://arxiv.org/abs/2512.06259v1", "title": "Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling", "authors": ["Yash Choudhary", "Preeti Rao", "Pushpak Bhattacharyya"], "abstract": "Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.", "categories": ["cs.SD", "cs.AI", "cs.LG"], "submitted_at": "2025-12-06T03:07:43Z", "updated_at": "2025-12-06T03:07:43Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:32.791736+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05920v1", "url": "https://arxiv.org/abs/2512.05920v1", "title": "NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction", "authors": ["Jiawen Yang", "Yihui Cao", "Xuanyu Tian", "Yuyao Zhang", "Hongjiang Wei"], "abstract": "Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-05T17:56:44Z", "updated_at": "2025-12-05T17:56:44Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:32.794170+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05765v1", "url": "https://arxiv.org/abs/2512.05765v1", "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics", "authors": ["Edward Y. Chang"], "abstract": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-05T14:51:17Z", "updated_at": "2025-12-05T14:51:17Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:32.798174+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05722v1", "url": "https://arxiv.org/abs/2512.05722v1", "title": "Teaching Language Models Mechanistic Explainability Through Arrow-Pushing", "authors": ["Théo A. Neukomm", "Zlatko Jončev", "Philippe Schwaller"], "abstract": "Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\\% top-3 accuracy on elementary step prediction and scores that surpass 73\\% on mech-USPTO-31k, and 93\\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.", "categories": ["cs.LG", "physics.chem-ph"], "submitted_at": "2025-12-05T13:57:50Z", "updated_at": "2025-12-05T13:57:50Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:32.803683+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05361v1", "url": "https://arxiv.org/abs/2512.05361v1", "title": "FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability", "authors": ["Ziheng Guo", "Fang Wu", "Maoxiong Zhao", "Chaoqun Fang", "Yang Bu"], "abstract": "We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.", "categories": ["physics.optics", "cs.LG", "physics.comp-ph"], "submitted_at": "2025-12-05T02:00:20Z", "updated_at": "2025-12-05T02:00:20Z", "rl_tags": ["general_dl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:32.808684+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05112v1", "url": "https://arxiv.org/abs/2512.05112v1", "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation", "authors": ["Dongzhi Jiang", "Renrui Zhang", "Haodong Li", "Zhuofan Zong", "Ziyu Guo", "Jun He", "Claire Guo", "Junyan Ye", "Rongyao Fang", "Weijia Li", "Rui Liu", "Hongsheng Li"], "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-04T18:59:53Z", "updated_at": "2025-12-04T18:59:53Z", "rl_tags": ["general_dl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:32.811731+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05089v1", "url": "https://arxiv.org/abs/2512.05089v1", "title": "The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception", "authors": ["Eduardo Di Santi"], "abstract": "Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.\n  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.\n  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.\n  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.", "categories": ["cs.LG", "math.OC"], "submitted_at": "2025-12-04T18:54:07Z", "updated_at": "2025-12-04T18:54:07Z", "rl_tags": ["general_dl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:32.815431+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04276v1", "url": "https://arxiv.org/abs/2512.04276v1", "title": "The Geometry of Benchmarks: A New Path Toward AGI", "authors": ["Przemyslaw Chojecki"], "abstract": "Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.", "categories": ["cs.AI", "cs.LG", "math.ST"], "submitted_at": "2025-12-03T21:34:09Z", "updated_at": "2025-12-03T21:34:09Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:32.825009+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03787v1", "url": "https://arxiv.org/abs/2512.03787v1", "title": "Adaptive Identification and Modeling of Clinical Pathways with Process Mining", "authors": ["Francesco Vitale", "Nicola Mazzocca"], "abstract": "Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.", "categories": ["cs.LG"], "submitted_at": "2025-12-03T13:37:37Z", "updated_at": "2025-12-03T13:37:37Z", "rl_tags": ["general_dl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:32.842212+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03759v1", "url": "https://arxiv.org/abs/2512.03759v1", "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective", "authors": ["Jingyang Ou", "Jiaqi Han", "Minkai Xu", "Shaoxuan Xu", "Jianwen Xie", "Stefano Ermon", "Yi Wu", "Chongxuan Li"], "abstract": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-03T13:05:32Z", "updated_at": "2025-12-03T13:05:32Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:32.853619+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03570v1", "url": "https://arxiv.org/abs/2512.03570v1", "title": "Machine Learning to Predict Slot Usage in TSCH Wireless Sensor Networks", "authors": ["Stefano Scanzio", "Gabriele Formis", "Tullio Facchinetti", "Gianluca Cena"], "abstract": "Wireless sensor networks (WSNs) are employed across a wide range of industrial applications where ultra-low power consumption is a critical prerequisite. At the same time, these systems must maintain a certain level of determinism to ensure reliable and predictable operation. In this view, time slotted channel hopping (TSCH) is a communication technology that meets both conditions, making it an attractive option for its usage in industrial WSNs. This work proposes the use of machine learning to learn the traffic pattern generated in networks based on the TSCH protocol, in order to turn nodes into a deep sleep state when no transmission is planned and thus to improve the energy efficiency of the WSN. The ability of machine learning models to make good predictions at different network levels in a typical tree network topology was analyzed in depth, showing how their capabilities degrade while approaching the root of the tree. The application of these models on simulated data based on an accurate modeling of wireless sensor nodes indicates that the investigated algorithms can be suitably used to further and substantially reduce the power consumption of a TSCH network.", "categories": ["cs.NI", "cs.AI", "cs.LG"], "submitted_at": "2025-12-03T08:50:02Z", "updated_at": "2025-12-03T08:50:02Z", "rl_tags": ["general_dl"], "attention_score": 0.643, "collected_at": "2025-12-10T03:47:32.873334+00:00"}
{"source": "arxiv", "arxiv_id": "2512.03400v1", "url": "https://arxiv.org/abs/2512.03400v1", "title": "Better World Models Can Lead to Better Post-Training Performance", "authors": ["Prakhar Gupta", "Henry Conklin", "Sarah-Jane Leslie", "Andrew Lee"], "abstract": "In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-03T03:13:20Z", "updated_at": "2025-12-03T03:13:20Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-12-10T03:47:32.877341+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07419v1", "url": "https://arxiv.org/abs/2512.07419v1", "title": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models", "authors": ["Haidong Kang", "Jun Du", "Lihong Lin"], "abstract": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-08T10:52:55Z", "updated_at": "2025-12-08T10:52:55Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:34.385548+00:00"}
{"source": "arxiv", "arxiv_id": "2512.04752v1", "url": "https://arxiv.org/abs/2512.04752v1", "title": "RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting", "authors": ["Siqi Wang", "Hailong Yang", "Junjie Zhu", "Xuezhu Wang", "Yufan Xu", "Depei Qian"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.", "categories": ["cs.LG"], "submitted_at": "2025-12-04T12:41:49Z", "updated_at": "2025-12-04T12:41:49Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-12-10T03:47:34.404655+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08923v1", "url": "https://arxiv.org/abs/2512.08923v1", "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs", "authors": ["Angela van Sprang", "Laurens Samson", "Ana Lucic", "Erman Acar", "Sennay Ghebreab", "Yuki M. Asano"], "abstract": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T18:57:07Z", "updated_at": "2025-12-09T18:57:07Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.825206+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08920v1", "url": "https://arxiv.org/abs/2512.08920v1", "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer", "authors": ["Jessica Yin", "Haozhi Qi", "Youngsun Wi", "Sayantan Kundu", "Mike Lambeta", "William Yang", "Changhao Wang", "Tingfan Wu", "Jitendra Malik", "Tess Hellebrekers"], "abstract": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-09T18:56:30Z", "updated_at": "2025-12-09T18:56:30Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.828206+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08914v1", "url": "https://arxiv.org/abs/2512.08914v1", "title": "SAQ: Stabilizer-Aware Quantum Error Correction Decoder", "authors": ["David Zenati", "Eliya Nachmani"], "abstract": "Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods. We introduce SAQ-Decoder, a unified framework combining transformer-based learning with constraint aware post-processing that achieves both near Maximum Likelihood (ML) accuracy and linear computational scalability with respect to the syndrome size. Our approach combines a dual-stream transformer architecture that processes syndromes and logical information with asymmetric attention patterns, and a novel differentiable logical loss that directly optimizes Logical Error Rates (LER) through smooth approximations over finite fields. SAQ-Decoder achieves near-optimal performance, with error thresholds of 10.99% (independent noise) and 18.6% (depolarizing noise) on toric codes that approach the ML bounds of 11.0% and 18.9% while outperforming existing neural and classical baselines in accuracy, complexity, and parameter efficiency. Our findings establish that learned decoders can simultaneously achieve competitive decoding accuracy and computational efficiency, addressing key requirements for practical fault-tolerant quantum computing systems.", "categories": ["quant-ph", "cs.AI"], "submitted_at": "2025-12-09T18:51:35Z", "updated_at": "2025-12-09T18:51:35Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.836724+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08896v1", "url": "https://arxiv.org/abs/2512.08896v1", "title": "Open Polymer Challenge: Post-Competition Report", "authors": ["Gang Liu", "Sobin Alosious", "Subhamoy Mahajan", "Eric Inae", "Yihan Zhu", "Yuhan Liu", "Renzheng Zhang", "Jiaxin Xu", "Addison Howard", "Ying Li", "Tengfei Luo", "Meng Jiang"], "abstract": "Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T18:38:15Z", "updated_at": "2025-12-09T18:38:15Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.850931+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08895v1", "url": "https://arxiv.org/abs/2512.08895v1", "title": "Unsupervised Learning of Density Estimates with Topological Optimization", "authors": ["Suina Tanweer", "Firas A. Khasawneh"], "abstract": "Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-09T18:35:51Z", "updated_at": "2025-12-09T18:35:51Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.857455+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08894v1", "url": "https://arxiv.org/abs/2512.08894v1", "title": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training", "authors": ["Jakub Krajewski", "Amitis Shidani", "Dan Busbridge", "Sam Wiseman", "Jason Ramapuram"], "abstract": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-12-09T18:33:48Z", "updated_at": "2025-12-09T18:33:48Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.871137+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08892v1", "url": "https://arxiv.org/abs/2512.08892v1", "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders", "authors": ["Guangzhi Xiong", "Zhenghao He", "Bohan Liu", "Sanchit Sinha", "Aidong Zhang"], "abstract": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-09T18:33:22Z", "updated_at": "2025-12-09T18:33:22Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.876669+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08889v1", "url": "https://arxiv.org/abs/2512.08889v1", "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers", "authors": ["Damiano Marsili", "Georgia Gkioxari"], "abstract": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T18:30:23Z", "updated_at": "2025-12-09T18:30:23Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.882735+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08885v1", "url": "https://arxiv.org/abs/2512.08885v1", "title": "Explainable Anomaly Detection for Industrial IoT Data Streams", "authors": ["Ana Rita Paupério", "Diogo Risca", "Afonso Lourenço", "Goreti Marreiros", "Ricardo Martins"], "abstract": "Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T18:20:35Z", "updated_at": "2025-12-09T18:20:35Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.885348+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08882v1", "url": "https://arxiv.org/abs/2512.08882v1", "title": "Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks", "authors": ["Mohamed Elmahallawy", "Asma Jodeiri Akbarfam"], "abstract": "The rise of space AI is reshaping government and industry through applications such as disaster detection, border surveillance, and climate monitoring, powered by massive data from commercial and governmental low Earth orbit (LEO) satellites. Federated satellite learning (FSL) enables joint model training without sharing raw data, but suffers from slow convergence due to intermittent connectivity and introduces critical trust challenges--where biased or falsified updates can arise across satellite constellations, including those injected through cyberattacks on inter-satellite or satellite-ground communication links. We propose OrbitChain, a blockchain-backed framework that empowers trustworthy multi-vendor collaboration in LEO networks. OrbitChain (i) offloads consensus to high-altitude platforms (HAPs) with greater computational capacity, (ii) ensures transparent, auditable provenance of model updates from different orbits owned by different vendors, and (iii) prevents manipulated or incomplete contributions from affecting global FSL model aggregation. Extensive simulations show that OrbitChain reduces computational and communication overhead while improving privacy, security, and global model accuracy. Its permissioned proof-of-authority ledger finalizes over 1000 blocks with sub-second latency (0.16,s, 0.26,s, 0.35,s for 1-of-5, 3-of-5, and 5-of-5 quorums). Moreover, OrbitChain reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor, demonstrating its effectiveness for real-time, multi-vendor learning. Our code is available at https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git", "categories": ["cs.CR", "cs.LG"], "submitted_at": "2025-12-09T18:16:34Z", "updated_at": "2025-12-09T18:16:34Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.890859+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08879v1", "url": "https://arxiv.org/abs/2512.08879v1", "title": "DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process", "authors": ["Mohammad Abu-Shaira", "Ajita Rattani", "Weishi Shi"], "abstract": "Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T18:12:38Z", "updated_at": "2025-12-09T18:12:38Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.896382+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08875v1", "url": "https://arxiv.org/abs/2512.08875v1", "title": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation", "authors": ["Joshua Ward", "Bochao Gu", "Chi-Hua Wang", "Guang Cheng"], "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T18:06:31Z", "updated_at": "2025-12-09T18:06:31Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.905109+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08873v1", "url": "https://arxiv.org/abs/2512.08873v1", "title": "Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning", "authors": ["Jing Jie Tan", "Anissa Mokraoui", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum"], "abstract": "Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.", "categories": ["cs.CV", "cs.AI", "cs.HC"], "submitted_at": "2025-12-09T18:05:59Z", "updated_at": "2025-12-09T18:05:59Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.911606+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08870v1", "url": "https://arxiv.org/abs/2512.08870v1", "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents", "authors": ["Xiang Chen", "Yuling Shi", "Qizhen Lan", "Yuchao Qiu", "Xiaodong Gu"], "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T18:04:41Z", "updated_at": "2025-12-09T18:04:41Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.913613+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08869v1", "url": "https://arxiv.org/abs/2512.08869v1", "title": "Differentially Private Synthetic Data Generation Using Context-Aware GANs", "authors": ["Anantaa Kotal", "Anupam Joshi"], "abstract": "The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "submitted_at": "2025-12-09T18:02:34Z", "updated_at": "2025-12-09T18:02:34Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.919616+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08868v1", "url": "https://arxiv.org/abs/2512.08868v1", "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce", "authors": ["Rui Min", "Zile Qiao", "Ze Xu", "Jiawen Zhai", "Wenyu Gao", "Xuanzhong Chen", "Haozhen Sun", "Zhen Zhang", "Xinyu Wang", "Hong Zhou", "Wenbiao Yin", "Xuan Zhou", "Yong Jiang", "Haicheng Liu", "Liang Ding", "Ling Zou", "Yi R.", " Fung", "Yalong Li", "Pengjun Xie"], "abstract": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T18:00:26Z", "updated_at": "2025-12-09T18:00:26Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.926350+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08862v1", "url": "https://arxiv.org/abs/2512.08862v1", "title": "Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety", "authors": ["Mohamed Elmahallawy", "Sanjay Madria", "Samuel Frimpong"], "abstract": "Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring.", "categories": ["cs.CR", "cs.LG"], "submitted_at": "2025-12-09T17:53:19Z", "updated_at": "2025-12-09T17:53:19Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.931871+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08859v1", "url": "https://arxiv.org/abs/2512.08859v1", "title": "Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data", "authors": ["Lars Ole Häusler", "Lena Uhlenberg", "Göran Köber", "Diyora Salimova", "Oliver Amft"], "abstract": "We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T17:51:01Z", "updated_at": "2025-12-09T17:51:01Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.934386+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08854v1", "url": "https://arxiv.org/abs/2512.08854v1", "title": "Generation is Required for Data-Efficient Perception", "authors": ["Jack Brady", "Bernhard Schölkopf", "Thomas Kipf", "Simon Buchholz", "Wieland Brendel"], "abstract": "It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-09T17:47:28Z", "updated_at": "2025-12-09T17:47:28Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.967000+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08833v1", "url": "https://arxiv.org/abs/2512.08833v1", "title": "Interpolation in Knowledge Representation", "authors": ["Jean Christoph Jung", "Patrick Koopmann", "Matthias Knorr"], "abstract": "Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.", "categories": ["cs.AI", "cs.LO"], "submitted_at": "2025-12-09T17:21:30Z", "updated_at": "2025-12-09T17:21:30Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:36.991982+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08832v1", "url": "https://arxiv.org/abs/2512.08832v1", "title": "Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models", "authors": ["Huzaifa Arif", "Pin-Yu Chen", "Alex Gittens", "James Diffenderfer", "Bhavya Kailkhura"], "abstract": "With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T17:20:56Z", "updated_at": "2025-12-09T17:20:56Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.002784+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08829v1", "url": "https://arxiv.org/abs/2512.08829v1", "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models", "authors": ["Hongyuan Tao", "Bencheng Liao", "Shaoyu Chen", "Haoran Yin", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "abstract": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T17:18:32Z", "updated_at": "2025-12-09T17:18:32Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.007787+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08828v1", "url": "https://arxiv.org/abs/2512.08828v1", "title": "Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference", "authors": ["Swaraj Bose", "Walter Dempsey"], "abstract": "Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS).", "categories": ["stat.ME", "stat.ML"], "submitted_at": "2025-12-09T17:18:09Z", "updated_at": "2025-12-09T17:18:09Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.013610+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08826v1", "url": "https://arxiv.org/abs/2512.08826v1", "title": "CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale", "authors": ["Shahar Sarfaty", "Adi Haviv", "Uri Hacohen", "Niva Elkin-Koren", "Roi Livni", "Amit H. Bermano"], "abstract": "The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T17:15:32Z", "updated_at": "2025-12-09T17:15:32Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.019620+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08820v1", "url": "https://arxiv.org/abs/2512.08820v1", "title": "Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning", "authors": ["Yi Zhang", "Chun-Wun Cheng", "Junyi He", "Ke Yu", "Yushun Tang", "Carola-Bibiane Schönlieb", "Zhihai He", "Angelica I. Aviles-Rivero"], "abstract": "Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \\textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T17:12:22Z", "updated_at": "2025-12-09T17:12:22Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.023709+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08819v1", "url": "https://arxiv.org/abs/2512.08819v1", "title": "Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis", "authors": ["Ferdinand Kapl", "Emmanouil Angelis", "Tobias Höppe", "Kaitlin Maile", "Johannes von Oswald", "Nino Scherrer", "Stefan Bauer"], "abstract": "Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T17:12:04Z", "updated_at": "2025-12-09T17:12:04Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.037380+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08812v1", "url": "https://arxiv.org/abs/2512.08812v1", "title": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation", "authors": ["Anna Jordanous"], "abstract": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.", "categories": ["cs.SD", "cs.AI"], "submitted_at": "2025-12-09T17:05:36Z", "updated_at": "2025-12-09T17:05:36Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.048447+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08810v1", "url": "https://arxiv.org/abs/2512.08810v1", "title": "Multicalibration for LLM-based Code Generation", "authors": ["Viola Campos", "Robin Kuschnereit", "Adrian Ulges"], "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.", "categories": ["cs.SE", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T17:04:01Z", "updated_at": "2025-12-09T17:04:01Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.057135+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08809v1", "url": "https://arxiv.org/abs/2512.08809v1", "title": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration", "authors": ["Yi Liu", "Weixiang Han", "Chengjun Cai", "Xingliang Yuan", "Cong Wang"], "abstract": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T17:03:59Z", "updated_at": "2025-12-09T17:03:59Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.065646+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08805v1", "url": "https://arxiv.org/abs/2512.08805v1", "title": "Identifying counterfactual probabilities using bivariate distributions and uplift modeling", "authors": ["Théo Verhelst", "Gianluca Bontempi"], "abstract": "Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., \"Would this customer still have churned had we given them a marketing offer?\"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T16:59:38Z", "updated_at": "2025-12-09T16:59:38Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.071693+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08802v1", "url": "https://arxiv.org/abs/2512.08802v1", "title": "Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework", "authors": ["Sadegh Momeni", "Ge Zhang", "Birkett Huber", "Hamza Harkous", "Sam Lipton", "Benoit Seguin", "Yanis Pavlidis"], "abstract": "Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.\n  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-09T16:58:08Z", "updated_at": "2025-12-09T16:58:08Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.078704+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08798v1", "url": "https://arxiv.org/abs/2512.08798v1", "title": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?", "authors": ["Jeongwhan Choi", "Woosung Kang", "Minseo Kim", "Jongwoo Kim", "Noseong Park"], "abstract": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T16:51:30Z", "updated_at": "2025-12-09T16:51:30Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.090760+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08789v1", "url": "https://arxiv.org/abs/2512.08789v1", "title": "MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance", "authors": ["Chaewon Kim", "Seoyeon Lee", "Jonghyuk Park"], "abstract": "Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T16:40:10Z", "updated_at": "2025-12-09T16:40:10Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.103044+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08786v1", "url": "https://arxiv.org/abs/2512.08786v1", "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs", "authors": ["Mahmoud Srewa", "Tianyu Zhao", "Salma Elmalaki"], "abstract": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-09T16:39:32Z", "updated_at": "2025-12-09T16:39:32Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.113317+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08777v1", "url": "https://arxiv.org/abs/2512.08777v1", "title": "Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages", "authors": ["David Samuel", "Lilja Øvrelid", "Erik Velldal", "Andrey Kutuzov"], "abstract": "We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-09T16:31:48Z", "updated_at": "2025-12-09T16:31:48Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.119327+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08774v1", "url": "https://arxiv.org/abs/2512.08774v1", "title": "Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps", "authors": ["Seoyeon Lee", "Gwangyeol Yu", "Chaewon Kim", "Jonghyuk Park"], "abstract": "Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T16:30:31Z", "updated_at": "2025-12-09T16:30:31Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.132778+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08772v1", "url": "https://arxiv.org/abs/2512.08772v1", "title": "De novo generation of functional terpene synthases using TpsGPT", "authors": ["Hamsini Ramanathan", "Roman Bushuiev", "Matouš Soldát", "Jirí Kohout", "Téo Hebra", "Joshua David Smith", "Josef Sivic", "Tomáš Pluskal"], "abstract": "Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T16:29:53Z", "updated_at": "2025-12-09T16:29:53Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.141295+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08769v1", "url": "https://arxiv.org/abs/2512.08769v1", "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows", "authors": ["Eranga Bandara", "Ross Gore", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Xueping Liang", "Safdar H. Bouk", "Amin Hass", "Sachini Rajapakse", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T16:23:05Z", "updated_at": "2025-12-09T16:23:05Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.158664+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08767v1", "url": "https://arxiv.org/abs/2512.08767v1", "title": "Data-Driven Dynamic Parameter Learning of manipulator robots", "authors": ["Mohammed Elseiagy", "Tsige Tadesse Alemayoh", "Ranulfo Bezerra", "Shotaro Kojima", "Kazunori Ohno"], "abstract": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-09T16:15:58Z", "updated_at": "2025-12-09T16:15:58Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.167736+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08755v1", "url": "https://arxiv.org/abs/2512.08755v1", "title": "Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments", "authors": ["Dongdong Yang", "Bin Li", "Jiguang He"], "abstract": "Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T16:06:09Z", "updated_at": "2025-12-09T16:06:09Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.183814+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08743v1", "url": "https://arxiv.org/abs/2512.08743v1", "title": "Towards Foundation Models with Native Multi-Agent Intelligence", "authors": ["Shuyue Hu", "Haoyang Yan", "Yiqun Zhang", "Yang Chen", "Dongzhan Zhou", "Lei Bai"], "abstract": "Foundation models (FMs) are increasingly assuming the role of the \"brain\" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.", "categories": ["cs.AI", "cs.MA"], "submitted_at": "2025-12-09T15:51:36Z", "updated_at": "2025-12-09T15:51:36Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.196620+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08740v1", "url": "https://arxiv.org/abs/2512.08740v1", "title": "Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance", "authors": ["Yiming Lu"], "abstract": "Currently, there exists a fundamental divide between the \"cognitive black box\" (implicit intuition) of human experts and the \"computational black box\" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of \"human-AI collaborative cognitive enhancement,\" aiming to transform the dual black boxes into a composable, auditable, and extensible \"functional white-box\" system through structured \"meta-interaction.\" The core breakthrough lies in the \"plug-and-play cognitive framework\"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from \"AI as a tool\" to \"AI as a thinking partner.\" This work not only provides the first engineering proof for \"cognitive equity\" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through \"transparency of interaction protocols\" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T15:50:15Z", "updated_at": "2025-12-09T15:50:15Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.217836+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07820v1", "url": "https://arxiv.org/abs/2512.07820v1", "title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces", "authors": ["Prithila Angkan", "Amin Jalali", "Paul Hungler", "Ali Etemad"], "abstract": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.", "categories": ["cs.HC", "cs.LG"], "submitted_at": "2025-12-08T18:54:11Z", "updated_at": "2025-12-08T18:54:11Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.423843+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08733v1", "url": "https://arxiv.org/abs/2512.08733v1", "title": "Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting", "authors": ["Kuniko Paxton", "Zeinab Dehghani", "Koorosh Aslansefat", "Dhavalkumar Thakker", "Yiannis Papadopoulos"], "abstract": "Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T15:45:20Z", "updated_at": "2025-12-09T15:45:20Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.428840+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08732v1", "url": "https://arxiv.org/abs/2512.08732v1", "title": "Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data", "authors": ["Udesh Habaraduwa", "Andrei Lixandru"], "abstract": "The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.", "categories": ["cs.LG", "q-bio.SC"], "submitted_at": "2025-12-09T15:44:03Z", "updated_at": "2025-12-09T15:44:03Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.438052+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07818v1", "url": "https://arxiv.org/abs/2512.07818v1", "title": "Provable Long-Range Benefits of Next-Token Prediction", "authors": ["Xinyuan Cao", "Santosh S. Vempala"], "abstract": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "submitted_at": "2025-12-08T18:51:54Z", "updated_at": "2025-12-08T18:51:54Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.442298+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07814v2", "url": "https://arxiv.org/abs/2512.07814v2", "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "categories": ["cs.SE", "cs.AI", "cs.CR"], "submitted_at": "2025-12-08T18:47:40Z", "updated_at": "2025-12-09T03:23:33Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.446919+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07810v1", "url": "https://arxiv.org/abs/2512.07810v1", "title": "Auditing Games for Sandbagging", "authors": ["Jordan Taylor", "Sid Black", "Dillon Bowen", "Thomas Read", "Satvik Golechha", "Alex Zelenka-Martin", "Oliver Makins", "Connor Kissane", "Kola Ayonrinde", "Jacob Merizian", "Samuel Marks", "Chris Cundy", "Joseph Bloom"], "abstract": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .", "categories": ["cs.AI"], "submitted_at": "2025-12-08T18:44:44Z", "updated_at": "2025-12-08T18:44:44Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.449921+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08724v1", "url": "https://arxiv.org/abs/2512.08724v1", "title": "Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search", "authors": ["Manos Plitsis", "Giorgos Bouritsas", "Vassilis Katsouros", "Yannis Panagakis"], "abstract": "Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T15:39:04Z", "updated_at": "2025-12-09T15:39:04Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.453858+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08715v1", "url": "https://arxiv.org/abs/2512.08715v1", "title": "Multi-domain performance analysis with scores tailored to user preferences", "authors": ["Sébastien Piérard", "Adrien Deliège", "Marc Van Droogenbroeck"], "abstract": "The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.", "categories": ["cs.PF", "cs.AI", "cs.CV", "cs.LG"], "submitted_at": "2025-12-09T15:29:53Z", "updated_at": "2025-12-09T15:29:53Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.459869+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08713v1", "url": "https://arxiv.org/abs/2512.08713v1", "title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning", "authors": ["Ekhi Azurmendi", "Xabier Arregi", "Oier Lopez de Lacalle"], "abstract": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-09T15:28:35Z", "updated_at": "2025-12-09T15:28:35Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.461876+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07808v1", "url": "https://arxiv.org/abs/2512.07808v1", "title": "LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout", "authors": ["M. A. Farooq", "G. Di Guglielmo", "A. Rajagopala", "N. Tran", "V. A. Chhabria", "A. Arora"], "abstract": "Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.", "categories": ["quant-ph", "cs.LG"], "submitted_at": "2025-12-08T18:41:13Z", "updated_at": "2025-12-08T18:41:13Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.467479+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07805v1", "url": "https://arxiv.org/abs/2512.07805v1", "title": "Group Representational Position Encoding", "authors": ["Yifan Zhang", "Zixiang Chen", "Yifeng Liu", "Zhen Qin", "Huizhuo Yuan", "Kangping Xu", "Yang Yuan", "Quanquan Gu", "Andrew Chi-Chih Yao"], "abstract": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-12-08T18:39:13Z", "updated_at": "2025-12-08T18:39:13Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.473343+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07801v2", "url": "https://arxiv.org/abs/2512.07801v2", "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "authors": ["Raunak Jain", "Mudita Khurana"], "abstract": "LLM-based agents are increasingly deployed for expert decision support, yet human-AI teams in high-stakes settings do not yet reliably outperform the best individual. We argue this complementarity gap reflects a fundamental mismatch: current agents are trained as answer engines, not as partners in the collaborative sensemaking through which experts actually make decisions. Sensemaking (the ability to co-construct causal explanations, surface uncertainties, and adapt goals) is the key capability that current training pipelines do not explicitly develop or evaluate. We propose Collaborative Causal Sensemaking (CCS) as a research agenda to develop this capability from the ground up, spanning new training environments that reward collaborative thinking, representations for shared human-AI mental models, and evaluation centred on trust and complementarity. These directions can advance MAS research toward agents that think with their human partners rather than for them.", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "submitted_at": "2025-12-08T18:30:41Z", "updated_at": "2025-12-09T09:33:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.481864+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08705v1", "url": "https://arxiv.org/abs/2512.08705v1", "title": "Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design", "authors": ["Jannik Graebner", "Ryne Beeson"], "abstract": "Preliminary mission design of low-thrust spacecraft trajectories in the Circular Restricted Three-Body Problem is a global search characterized by a complex objective landscape and numerous local minima. Formulating the problem as sampling from an unnormalized distribution supported on neighborhoods of locally optimal solutions, provides the opportunity to deploy Markov chain Monte Carlo methods and generative machine learning. In this work, we extend our previous self-supervised diffusion model fine-tuning framework to employ gradient-informed Markov chain Monte Carlo. We compare two algorithms - the Metropolis-Adjusted Langevin Algorithm and Hamiltonian Monte Carlo - both initialized from a distribution learned by a diffusion model. Derivatives of an objective function that balances fuel consumption, time of flight and constraint violations are computed analytically using state transition matrices. We show that incorporating the gradient drift term accelerates mixing and improves convergence of the Markov chain for a multi-revolution transfer in the Saturn-Titan system. Among the evaluated methods, MALA provides the best trade-off between performance and computational cost. Starting from samples generated by a baseline diffusion model trained on a related transfer, MALA explicitly targets Pareto-optimal solutions. Compared to a random walk Metropolis algorithm, it increases the feasibility rate from 17.34% to 63.01% and produces a denser, more diverse coverage of the Pareto front. By fine-tuning a diffusion model on the generated samples and associated reward values with reward-weighted likelihood maximization, we learn the global solution structure of the problem and eliminate the need for a tedious separate data generation phase.", "categories": ["eess.SY", "cs.LG", "math.OC"], "submitted_at": "2025-12-09T15:21:11Z", "updated_at": "2025-12-09T15:21:11Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.494340+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07796v1", "url": "https://arxiv.org/abs/2512.07796v1", "title": "Large Causal Models from Large Language Models", "authors": ["Sridhar Mahadevan"], "abstract": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T18:28:04Z", "updated_at": "2025-12-08T18:28:04Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.501655+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07795v1", "url": "https://arxiv.org/abs/2512.07795v1", "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "abstract": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "categories": ["cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-08T18:26:58Z", "updated_at": "2025-12-08T18:26:58Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.519353+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08699v1", "url": "https://arxiv.org/abs/2512.08699v1", "title": "An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals", "authors": ["Chenglong Duan", "Dazhong Wu"], "abstract": "Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T15:15:46Z", "updated_at": "2025-12-09T15:15:46Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.522379+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08674v1", "url": "https://arxiv.org/abs/2512.08674v1", "title": "Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology", "authors": ["Rongzhao Zhang", "Junqiao Wang", "Shuyun Yang", "Mouxiao Bian", "Chao Ding", "Yuwei Bai", "Chihao Zhang", "Yuguang Shen", "Lei Wang", "Lei Zheng", "Qiujuan Yan", "Yun Zhong", "Meiling Liu", "Jiwei Yu", "Zheng Wang", "Jie Xu", "Meng Luo"], "abstract": "Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.", "categories": ["cs.AI", "cs.MA"], "submitted_at": "2025-12-09T14:56:40Z", "updated_at": "2025-12-09T14:56:40Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.526395+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08671v1", "url": "https://arxiv.org/abs/2512.08671v1", "title": "DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning", "authors": ["Huzaifa Arif"], "abstract": "Recent work \\cite{arifgroup} introduced Federated Proximal Gradient \\textbf{(\\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \\textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \\texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \\textbf{DS \\texttt{FedProxGrad}} (Decay Step Size \\texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \\cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\\liminf_{r\\to\\infty} \\mathbb{E}[\\|\\nabla F(\\mathbf{x}^r)\\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-09T14:55:21Z", "updated_at": "2025-12-09T14:55:21Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.530894+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07785v1", "url": "https://arxiv.org/abs/2512.07785v1", "title": "Automating High Energy Physics Data Analysis with LLM-Powered Agents", "authors": ["Eli Gendreau-Distler", "Joshua Ho", "Dongwon Kim", "Luc Tomas Le Pottier", "Haichen Wang", "Chengxi Yang"], "abstract": "We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.", "categories": ["physics.data-an", "cs.AI", "cs.LG", "hep-ex"], "submitted_at": "2025-12-08T18:13:13Z", "updated_at": "2025-12-08T18:13:13Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.535424+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07782v1", "url": "https://arxiv.org/abs/2512.07782v1", "title": "GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory", "authors": ["Jiaxu Liu", "Yuhe Bai", "Christos-Savvas Bouganis"], "abstract": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\\underline{Gated} (\\underline{F}lash) \\underline{W}indowed \\underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T18:11:06Z", "updated_at": "2025-12-08T18:11:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.538418+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07770v1", "url": "https://arxiv.org/abs/2512.07770v1", "title": "Distribution-informed Online Conformal Prediction", "authors": ["Dongjian Hu", "Junxi Wu", "Shu-Tao Xia", "Changliang Zou"], "abstract": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-08T17:51:49Z", "updated_at": "2025-12-08T17:51:49Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.542451+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08667v1", "url": "https://arxiv.org/abs/2512.08667v1", "title": "Direct transfer of optimized controllers to similar systems using dimensionless MPC", "authors": ["Josip Kir Hromatko", "Shambhuraj Sawant", "Šandor Ileš", "Sébastien Gros"], "abstract": "Scaled model experiments are commonly used in various engineering fields to reduce experimentation costs and overcome constraints associated with full-scale systems. The relevance of such experiments relies on dimensional analysis and the principle of dynamic similarity. However, transferring controllers to full-scale systems often requires additional tuning. In this paper, we propose a method to enable a direct controller transfer using dimensionless model predictive control, tuned automatically for closed-loop performance. With this reformulation, the closed-loop behavior of an optimized controller transfers directly to a new, dynamically similar system. Additionally, the dimensionless formulation allows for the use of data from systems of different scales during parameter optimization. We demonstrate the method on a cartpole swing-up and a car racing problem, applying either reinforcement learning or Bayesian optimization for tuning the controller parameters. Software used to obtain the results in this paper is publicly available at https://github.com/josipkh/dimensionless-mpcrl.", "categories": ["eess.SY", "cs.LG"], "submitted_at": "2025-12-09T14:52:15Z", "updated_at": "2025-12-09T14:52:15Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.544463+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08657v1", "url": "https://arxiv.org/abs/2512.08657v1", "title": "Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain", "authors": ["Renato Cordeiro Ferreira", "Aditya Dhinavahi", "Rowanne Trapmann", "Willem-Jan van den Heuvel"], "abstract": "ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.", "categories": ["cs.SE", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T14:43:23Z", "updated_at": "2025-12-09T14:43:23Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.554000+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07766v1", "url": "https://arxiv.org/abs/2512.07766v1", "title": "Formalized Hopfield Networks and Boltzmann Machines", "authors": ["Matteo Cipollina", "Michail Karatarakis", "Freek Wiedijk"], "abstract": "Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.", "categories": ["cs.LG", "cs.LO"], "submitted_at": "2025-12-08T17:48:31Z", "updated_at": "2025-12-08T17:48:31Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.558009+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07755v1", "url": "https://arxiv.org/abs/2512.07755v1", "title": "Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion", "authors": ["Brenda Anague", "Bamdad Hosseini", "Issa Karambal", "Jean Medard Ngnotchouye"], "abstract": "Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-08T17:38:49Z", "updated_at": "2025-12-08T17:38:49Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.566507+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08639v1", "url": "https://arxiv.org/abs/2512.08639v1", "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning", "authors": ["Huilin Xu", "Zhuoyang Liu", "Yixiang Luomei", "Feng Xu"], "abstract": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T14:25:24Z", "updated_at": "2025-12-09T14:25:24Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.569500+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08629v1", "url": "https://arxiv.org/abs/2512.08629v1", "title": "See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm", "authors": ["Haoyu Zhao", "Weizhong Ding", "Yuhao Yang", "Zheng Tian", "Linyi Yang", "Kun Shao", "Jun Wang"], "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.", "categories": ["cs.AI", "cs.CV", "cs.HC"], "submitted_at": "2025-12-09T14:14:37Z", "updated_at": "2025-12-09T14:14:37Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.571016+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08613v1", "url": "https://arxiv.org/abs/2512.08613v1", "title": "Protein Secondary Structure Prediction Using Transformers", "authors": ["Manzi Kevin Maxime"], "abstract": "Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T13:58:47Z", "updated_at": "2025-12-09T13:58:47Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.574102+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08609v1", "url": "https://arxiv.org/abs/2512.08609v1", "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models", "authors": ["Hui Wang", "Yang Liu", "Xiaoyu Zhang", "Chaoxu Mu"], "abstract": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T13:54:18Z", "updated_at": "2025-12-09T13:54:18Z", "rl_tags": ["exploration"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.576112+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08606v1", "url": "https://arxiv.org/abs/2512.08606v1", "title": "Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning", "authors": ["Zhenyu Zhang", "Guangyao Chen", "Yixiong Zou", "Zhimeng Huang", "Yuhua Li"], "abstract": "The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of \"emptiness\" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T13:51:05Z", "updated_at": "2025-12-09T13:51:05Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.580101+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07741v1", "url": "https://arxiv.org/abs/2512.07741v1", "title": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data", "authors": ["Agnes Norbury", "George Fairs", "Alexandra L. Georgescu", "Matthew M. Nour", "Emilia Molimpakis", "Stefano Goria"], "abstract": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.", "categories": ["cs.LG", "cs.SD"], "submitted_at": "2025-12-08T17:28:09Z", "updated_at": "2025-12-08T17:28:09Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.583616+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07737v1", "url": "https://arxiv.org/abs/2512.07737v1", "title": "A scalable and real-time neural decoder for topological quantum codes", "authors": ["Andrew W. Senior", "Thomas Edlich", "Francisco J. H. Heras", "Lei M. Zhang", "Oscar Higgott", "James S. Spencer", "Taylor Applebaum", "Sam Blackwell", "Justin Ledford", "Akvilė Žemgulytė", "Augustin Žídek", "Noah Shutty", "Andrew Cowie", "Yin Li", "George Holland", "Peter Brooks", "Charlie Beattie", "Michael Newman", "Alex Davies", "Cody Jones", "Sergio Boixo", "Hartmut Neven", "Pushmeet Kohli", "Johannes Bausch"], "abstract": "Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.", "categories": ["quant-ph", "cs.LG"], "submitted_at": "2025-12-08T17:24:59Z", "updated_at": "2025-12-08T17:24:59Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.586612+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07730v1", "url": "https://arxiv.org/abs/2512.07730v1", "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "authors": ["Sangha Park", "Seungryong Yoo", "Jisoo Mok", "Sungroh Yoon"], "abstract": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T17:20:07Z", "updated_at": "2025-12-08T17:20:07Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.588618+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07729v1", "url": "https://arxiv.org/abs/2512.07729v1", "title": "Improving action classification with brain-inspired deep networks", "authors": ["Aidas Aglinskas", "Stefano Anzellotti"], "abstract": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T17:19:47Z", "updated_at": "2025-12-08T17:19:47Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.593366+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08596v1", "url": "https://arxiv.org/abs/2512.08596v1", "title": "Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality", "authors": ["Wicaksono Febriantoro", "Qi Zhou", "Wannapon Suraworachet", "Sahan Bulathwela", "Andrea Gauthier", "Eva Millan", "Mutlu Cukurova"], "abstract": "The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.", "categories": ["cs.CY", "cs.AI"], "submitted_at": "2025-12-09T13:34:33Z", "updated_at": "2025-12-09T13:34:33Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.599381+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08592v1", "url": "https://arxiv.org/abs/2512.08592v1", "title": "The SMART+ Framework for AI Systems", "authors": ["Laxmiraju Kandikatla", "Branislav Radeljic"], "abstract": "Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.", "categories": ["cs.AI", "cs.CY", "cs.HC", "eess.SY"], "submitted_at": "2025-12-09T13:33:14Z", "updated_at": "2025-12-09T13:33:14Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.604430+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07724v1", "url": "https://arxiv.org/abs/2512.07724v1", "title": "The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic", "authors": ["Zhengzheng Tang"], "abstract": "The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap \"from stochastic ions to deterministic floats,\" we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.", "categories": ["cs.ET", "cs.AI"], "submitted_at": "2025-12-08T17:15:46Z", "updated_at": "2025-12-08T17:15:46Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.607436+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07723v1", "url": "https://arxiv.org/abs/2512.07723v1", "title": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity", "authors": ["Yonggeon Lee", "Jibin Hwang", "Alfred Malengo Kondoro", "Juhyun Song", "Youngtae Noh"], "abstract": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T17:14:32Z", "updated_at": "2025-12-08T17:14:32Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.611962+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08591v1", "url": "https://arxiv.org/abs/2512.08591v1", "title": "Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset", "authors": ["Charles Rios", "Longzhen Han", "Almas Baimagambetov", "Nikolaos Polatidis"], "abstract": "Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.", "categories": ["cs.LG", "cs.NE"], "submitted_at": "2025-12-09T13:32:41Z", "updated_at": "2025-12-09T13:32:41Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.652655+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07705v1", "url": "https://arxiv.org/abs/2512.07705v1", "title": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models", "authors": ["Saroj Gopali", "Bipin Chhetri", "Deepika Giri", "Sima Siami-Namini", "Akbar Siami Namin"], "abstract": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.\n  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.\n  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T16:52:46Z", "updated_at": "2025-12-08T16:52:46Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.662744+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07703v1", "url": "https://arxiv.org/abs/2512.07703v1", "title": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation", "authors": ["Leo Fillioux", "Enzo Ferrante", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis"], "abstract": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-08T16:50:21Z", "updated_at": "2025-12-08T16:50:21Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.674532+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08580v1", "url": "https://arxiv.org/abs/2512.08580v1", "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning", "authors": ["Peijun Tang", "Shangjin Xie", "Binyan Sun", "Baifu Huang", "Kuncheng Luo", "Haotian Yang", "Weiqi Jin", "Jianan Wang"], "abstract": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-09T13:19:37Z", "updated_at": "2025-12-09T13:19:37Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.701874+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07702v1", "url": "https://arxiv.org/abs/2512.07702v1", "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "authors": ["Sangha Park", "Eunji Kim", "Yeongtak Oh", "Jooyoung Choi", "Sungroh Yoon"], "abstract": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T16:49:19Z", "updated_at": "2025-12-08T16:49:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.708902+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07697v1", "url": "https://arxiv.org/abs/2512.07697v1", "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks", "authors": ["Aileen Liao", "Dong-Ki Kim", "Max Olan Smith", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "abstract": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-08T16:38:14Z", "updated_at": "2025-12-08T16:38:14Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.715667+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08577v1", "url": "https://arxiv.org/abs/2512.08577v1", "title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery", "authors": ["Yuna Kato", "Shohei Mori", "Hideo Saito", "Yoshifumi Takatsume", "Hiroki Kajita", "Mariko Isogawa"], "abstract": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "submitted_at": "2025-12-09T13:15:32Z", "updated_at": "2025-12-09T13:15:32Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.718671+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08567v1", "url": "https://arxiv.org/abs/2512.08567v1", "title": "A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks", "authors": ["Nader Sadek", "Mirette Moawad", "Christina Naguib", "Mariam Elzahaby"], "abstract": "Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T13:05:54Z", "updated_at": "2025-12-09T13:05:54Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.727733+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07684v1", "url": "https://arxiv.org/abs/2512.07684v1", "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks", "authors": ["Zihan Chen", "Lanyu Yu"], "abstract": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.", "categories": ["cs.CL", "cs.AI", "cs.SI"], "submitted_at": "2025-12-08T16:22:40Z", "updated_at": "2025-12-08T16:22:40Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.735379+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07676v1", "url": "https://arxiv.org/abs/2512.07676v1", "title": "A Bootstrap Perspective on Stochastic Gradient Descent", "authors": ["Hongjian Lan", "Yucong Liu", "Florian Schäfer"], "abstract": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-08T16:10:56Z", "updated_at": "2025-12-08T16:10:56Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.738378+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07674v1", "url": "https://arxiv.org/abs/2512.07674v1", "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "abstract": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T16:09:10Z", "updated_at": "2025-12-08T16:09:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.741902+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08548v1", "url": "https://arxiv.org/abs/2512.08548v1", "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations", "authors": ["Yuchi Zhang", "Churui Sun", "Shiqi Liang", "Diyuan Liu", "Chao Ji", "Wei-Nan Zhang", "Ting Liu"], "abstract": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-09T12:45:12Z", "updated_at": "2025-12-09T12:45:12Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.745004+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08545v1", "url": "https://arxiv.org/abs/2512.08545v1", "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks", "authors": ["Indrajit Kar", "Kalathur Chenchu Kishore Kumar"], "abstract": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "submitted_at": "2025-12-09T12:40:39Z", "updated_at": "2025-12-09T12:40:39Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.749003+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08542v1", "url": "https://arxiv.org/abs/2512.08542v1", "title": "A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation", "authors": ["Zhigang Jia", "Duan Wang", "Hengkai Wang", "Yajun Xie", "Meixiang Zhao", "Xiaoyu Zhao"], "abstract": "Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.", "categories": ["cs.CV", "cs.AI", "math.NA"], "submitted_at": "2025-12-09T12:39:39Z", "updated_at": "2025-12-09T12:39:39Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.751013+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08536v1", "url": "https://arxiv.org/abs/2512.08536v1", "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans", "authors": ["Tammy Zhong", "Yang Song", "Maurice Pagnucco"], "abstract": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T12:34:54Z", "updated_at": "2025-12-09T12:34:54Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.753066+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07921v1", "url": "https://arxiv.org/abs/2512.07921v1", "title": "DeepCode: Open Agentic Coding", "authors": ["Zongwei Li", "Zhonghang Li", "Zirui Guo", "Xubin Ren", "Chao Huang"], "abstract": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.", "categories": ["cs.SE", "cs.AI"], "submitted_at": "2025-12-08T16:07:13Z", "updated_at": "2025-12-08T16:07:13Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.757227+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07667v1", "url": "https://arxiv.org/abs/2512.07667v1", "title": "Depth-Wise Activation Steering for Honest Language Models", "authors": ["Gracjan Góral", "Marysia Winkels", "Steven Basart"], "abstract": "Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T16:03:06Z", "updated_at": "2025-12-08T16:03:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.759238+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07652v1", "url": "https://arxiv.org/abs/2512.07652v1", "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research", "authors": ["Hamad Almazrouei", "Mariam Al Nasseri", "Maha Alzaabi"], "abstract": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T15:45:40Z", "updated_at": "2025-12-08T15:45:40Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.762760+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07650v1", "url": "https://arxiv.org/abs/2512.07650v1", "title": "Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation", "authors": ["Fuyuan Lyu", "Zhentai Chen", "Jingyan Jiang", "Lingjie Li", "Xing Tang", "Xiuqiang He", "Xue Liu"], "abstract": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.", "categories": ["cs.IR", "cs.LG"], "submitted_at": "2025-12-08T15:41:10Z", "updated_at": "2025-12-08T15:41:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.764448+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08518v1", "url": "https://arxiv.org/abs/2512.08518v1", "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking", "authors": ["Nadezhda Kushina", "Ko Watanabe", "Aarthi Kannan", "Ashita Ashok", "Andreas Dengel", "Karsten Berns"], "abstract": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.", "categories": ["cs.RO", "cs.AI", "cs.HC"], "submitted_at": "2025-12-09T12:08:21Z", "updated_at": "2025-12-09T12:08:21Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.766447+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08513v1", "url": "https://arxiv.org/abs/2512.08513v1", "title": "Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice", "authors": ["Masahiro Kato"], "abstract": "We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.", "categories": ["econ.EM", "cs.LG", "math.ST", "stat.ME", "stat.ML"], "submitted_at": "2025-12-09T11:58:27Z", "updated_at": "2025-12-09T11:58:27Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.769452+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08512v1", "url": "https://arxiv.org/abs/2512.08512v1", "title": "A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles", "authors": ["Jiang Liu", "Yan Qin", "Wei Dai", "Chau Yuen"], "abstract": "Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T11:54:09Z", "updated_at": "2025-12-09T11:54:09Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.770960+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08510v1", "url": "https://arxiv.org/abs/2512.08510v1", "title": "Data-Efficient Learning of Anomalous Diffusion with Wavelet Representations: Enabling Direct Learning from Experimental Trajectories", "authors": ["Gongyi Wang", "Yu Zhang", "Zihan Huang"], "abstract": "Machine learning (ML) has become a versatile tool for analyzing anomalous diffusion trajectories, yet most existing pipelines are trained on large collections of simulated data. In contrast, experimental trajectories, such as those from single-particle tracking (SPT), are typically scarce and may differ substantially from the idealized models used for simulation, leading to degradation or even breakdown of performance when ML methods are applied to real data. To address this mismatch, we introduce a wavelet-based representation of anomalous diffusion that enables data-efficient learning directly from experimental recordings. This representation is constructed by applying six complementary wavelet families to each trajectory and combining the resulting wavelet modulus scalograms. We first evaluate the wavelet representation on simulated trajectories from the andi-datasets benchmark, where it clearly outperforms both feature-based and trajectory-based methods with as few as 1000 training trajectories and still retains an advantage on large training sets. We then use this representation to learn directly from experimental SPT trajectories of fluorescent beads diffusing in F-actin networks, where the wavelet representation remains superior to existing alternatives for both diffusion-exponent regression and mesh-size classification. In particular, when predicting the diffusion exponents of experimental trajectories, a model trained on 1200 experimental tracks using the wavelet representation achieves significantly lower errors than state-of-the-art deep learning models trained purely on $10^6$ simulated trajectories. We associate this data efficiency with the emergence of distinct scale fingerprints disentangling underlying diffusion mechanisms in the wavelet spectra.", "categories": ["physics.bio-ph", "cond-mat.soft", "cs.LG", "physics.data-an"], "submitted_at": "2025-12-09T11:52:23Z", "updated_at": "2025-12-09T11:52:23Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.773058+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08508v1", "url": "https://arxiv.org/abs/2512.08508v1", "title": "Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening", "authors": ["Gengmo Zhou", "Feng Yu", "Wenda Wang", "Zhifeng Gao", "Guolin Ke", "Zhewei Wei", "Zhen Wang"], "abstract": "Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.", "categories": ["q-bio.BM", "cs.LG"], "submitted_at": "2025-12-09T11:49:24Z", "updated_at": "2025-12-09T11:49:24Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.775052+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07647v1", "url": "https://arxiv.org/abs/2512.07647v1", "title": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance", "authors": ["Georgios Tzachristas", "Lei Deng", "Ioannis Tzachristas", "Gong Zhang", "Renhai Chen"], "abstract": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=τ\\|μ_{\\mathrm{tail}}-μ_{\\mathrm{head}}\\|_2$ with $τ=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\leτ\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approxΦ_c(σ+Φ^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T15:36:41Z", "updated_at": "2025-12-08T15:36:41Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.779056+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07631v1", "url": "https://arxiv.org/abs/2512.07631v1", "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "authors": ["Shahar Lutati"], "abstract": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "categories": ["cs.AI", "cs.CC", "cs.IT", "cs.LG"], "submitted_at": "2025-12-08T15:21:52Z", "updated_at": "2025-12-08T15:21:52Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.780055+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07627v1", "url": "https://arxiv.org/abs/2512.07627v1", "title": "Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization", "authors": ["Maximos Kaliakatsos-Papakostas", "Konstantinos Soiledis", "Theodoros Tsamis", "Dimos Makris", "Vassilis Katsouros", "Emilios Cambouropoulos"], "abstract": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.", "categories": ["cs.SD", "cs.AI", "cs.SC"], "submitted_at": "2025-12-08T15:16:33Z", "updated_at": "2025-12-08T15:16:33Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.783329+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07624v1", "url": "https://arxiv.org/abs/2512.07624v1", "title": "Time Series Foundation Models for Process Model Forecasting", "authors": ["Yongbo Yu", "Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "abstract": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T15:08:50Z", "updated_at": "2025-12-08T15:08:50Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.785342+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08503v1", "url": "https://arxiv.org/abs/2512.08503v1", "title": "Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models", "authors": ["Jiaming Zhang", "Che Wang", "Yang Cao", "Longtao Huang", "Wei Yang Bryan Lim"], "abstract": "Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \\textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \\textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\\% improvement in tract-level protection (33.8\\% vs 19.4\\%) and nearly doubling block-level protection (33.5\\% vs 16.8\\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T11:35:51Z", "updated_at": "2025-12-09T11:35:51Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.787341+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08499v1", "url": "https://arxiv.org/abs/2512.08499v1", "title": "Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction", "authors": ["Waleed Razzaq", "Yun-Bo Zhao"], "abstract": "Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T11:30:41Z", "updated_at": "2025-12-09T11:30:41Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.788334+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08493v1", "url": "https://arxiv.org/abs/2512.08493v1", "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?", "authors": ["Dyna Soumhane Ouchebara", "Stéphane Dupont"], "abstract": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-09T11:15:13Z", "updated_at": "2025-12-09T11:15:13Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.791862+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08492v1", "url": "https://arxiv.org/abs/2512.08492v1", "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance", "authors": ["Aliaksei Kaliutau"], "abstract": "Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the \"Semantic Trap\" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T11:11:37Z", "updated_at": "2025-12-09T11:11:37Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.794257+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07612v1", "url": "https://arxiv.org/abs/2512.07612v1", "title": "PCMind-2.1-Kaiyuan-2B Technical Report", "authors": ["Kairong Luo", "Zhenbo Sun", "Xinyu Shi", "Shengqi Chen", "Bowen Yu", "Yunyi Chen", "Chenyi Dang", "Hengtao Tao", "Hui Wang", "Fangming Liu", "Kaifeng Lyu", "Wenguang Chen"], "abstract": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T15:00:10Z", "updated_at": "2025-12-08T15:00:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.801781+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07611v1", "url": "https://arxiv.org/abs/2512.07611v1", "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "authors": ["Yongsheng Lian"], "abstract": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-08T14:58:19Z", "updated_at": "2025-12-08T14:58:19Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.806400+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07608v1", "url": "https://arxiv.org/abs/2512.07608v1", "title": "Metric-Fair Prompting: Treating Similar Samples Similarly", "authors": ["Jing Wang", "Jie Shen", "Xing Niu", "Tong Zhang", "Jeremy Weiss"], "abstract": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T14:56:46Z", "updated_at": "2025-12-08T14:56:46Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.808391+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08478v1", "url": "https://arxiv.org/abs/2512.08478v1", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "authors": ["Yuning Gong", "Yifei Liu", "Yifan Zhan", "Muyao Niu", "Xueying Li", "Yuanjun Liao", "Jiaming Chen", "Yuanyuan Gao", "Jiaqi Chen", "Minming Chen", "Li Zhou", "Yuning Zhang", "Wei Wang", "Xiaoqing Hou", "Huaxi Huang", "Shixiang Tang", "Le Ma", "Dingwen Zhang", "Xue Yang", "Junchi Yan", "Yanchi Zhang", "Yinqiang Zheng", "Xiao Sun", "Zhihang Zhong"], "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "categories": ["cs.CV", "cs.AI", "cs.GR"], "submitted_at": "2025-12-09T10:54:58Z", "updated_at": "2025-12-09T10:54:58Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.811917+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08477v1", "url": "https://arxiv.org/abs/2512.08477v1", "title": "ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention", "authors": ["Huiguo He", "Pengyu Yan", "Ziqi Yi", "Weizhi Zhong", "Zheng Liu", "Yejun Tang", "Huan Yang", "Kun Gai", "Guanbin Li", "Lianwen Jin"], "abstract": "Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T10:51:45Z", "updated_at": "2025-12-09T10:51:45Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.813967+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08475v1", "url": "https://arxiv.org/abs/2512.08475v1", "title": "Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability", "authors": ["Weiqi Guan", "Junlin He"], "abstract": "The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.\n  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.\n  Key contributions:\n  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.\n  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.\n  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T10:49:01Z", "updated_at": "2025-12-09T10:49:01Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.815960+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07583v1", "url": "https://arxiv.org/abs/2512.07583v1", "title": "Complementary Learning Approach for Text Classification using Large Language Models", "authors": ["Navid Asgari", "Benjamin M. Cole"], "abstract": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T14:26:31Z", "updated_at": "2025-12-08T14:26:31Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.823000+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07578v1", "url": "https://arxiv.org/abs/2512.07578v1", "title": "$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations", "authors": ["Dongseok Kim", "Hyoungsun Choi", "Mohamed Jismy Aashik Rasool", "Gisung Oh"], "abstract": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.", "categories": ["stat.ML", "cs.LG", "stat.ME"], "submitted_at": "2025-12-08T14:14:01Z", "updated_at": "2025-12-08T14:14:01Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.826553+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07576v1", "url": "https://arxiv.org/abs/2512.07576v1", "title": "R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation", "authors": ["Xuecheng Li", "Weikuan Jia", "Komildzhon Sharipov", "Sharipov Hotam Beknazarovich", "Farzona S. Ataeva", "Qurbonaliev Alisher", "Yuanjie Zheng"], "abstract": "Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "submitted_at": "2025-12-08T14:12:52Z", "updated_at": "2025-12-08T14:12:52Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.829545+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07569v1", "url": "https://arxiv.org/abs/2512.07569v1", "title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting", "authors": ["Joel Ekstrand", "Tor Mattsson", "Zahra Taghiyarrenani", "Slawomir Nowaczyk", "Jens Lundström", "Mikael Lindén"], "abstract": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T14:02:31Z", "updated_at": "2025-12-08T14:02:31Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.834100+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08462v1", "url": "https://arxiv.org/abs/2512.08462v1", "title": "Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata", "authors": ["Danial Jafarzadeh Jazi", "Maryam Hajiesmaeili"], "abstract": "Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T10:35:23Z", "updated_at": "2025-12-09T10:35:23Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.836095+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08459v1", "url": "https://arxiv.org/abs/2512.08459v1", "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset", "authors": ["Gary Ackerman", "Theodore Wilson", "Zachary Kallenborn", "Olivia Shoemaker", "Anna Wetzel", "Hayley Peterson", "Abigail Danfora", "Jenna LaTourette", "Brandon Behlendorf", "Douglas Clifford"], "abstract": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.ET"], "submitted_at": "2025-12-09T10:31:02Z", "updated_at": "2025-12-09T10:31:02Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.839096+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08451v1", "url": "https://arxiv.org/abs/2512.08451v1", "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process", "authors": ["Gary Ackerman", "Zachary Kallenborn", "Anna Wetzel", "Hayley Peterson", "Jenna LaTourette", "Olivia Shoemaker", "Brandon Behlendorf", "Sheriff Almakki", "Doug Clifford", "Noah Sheinbaum"], "abstract": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.ET"], "submitted_at": "2025-12-09T10:24:25Z", "updated_at": "2025-12-09T10:24:25Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.842626+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08449v1", "url": "https://arxiv.org/abs/2512.08449v1", "title": "From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change", "authors": ["Yong-Woon Kim"], "abstract": "This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T10:21:02Z", "updated_at": "2025-12-09T10:21:02Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.844627+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07568v1", "url": "https://arxiv.org/abs/2512.07568v1", "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "authors": ["Xuecheng Li", "Weikuan Jia", "Alisher Kurbonaliev", "Qurbonaliev Alisher", "Khudzhamkulov Rustam", "Ismoilov Shuhratjon", "Eshmatov Javhariddin", "Yuanjie Zheng"], "abstract": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "categories": ["cs.CV", "cs.AI", "eess.IV"], "submitted_at": "2025-12-08T14:01:16Z", "updated_at": "2025-12-08T14:01:16Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.848629+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07558v1", "url": "https://arxiv.org/abs/2512.07558v1", "title": "ReLaX: Reasoning with Latent Exploration for Large Reasoning Models", "authors": ["Shimin Zhang", "Xianwei Chen", "Yufan Shen", "Ziyuan Ye", "Jibin Wu"], "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-08T13:48:33Z", "updated_at": "2025-12-08T13:48:33Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 1.057, "collected_at": "2025-12-10T03:47:37.857180+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08445v1", "url": "https://arxiv.org/abs/2512.08445v1", "title": "Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts", "authors": ["Madhav Gupta", "Vishak Prasad C", "Ganesh Ramakrishnan"], "abstract": "Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-09T10:19:07Z", "updated_at": "2025-12-09T10:19:07Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.861185+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08444v1", "url": "https://arxiv.org/abs/2512.08444v1", "title": "Learned iterative networks: An operator learning perspective", "authors": ["Andreas Hauptmann", "Ozan Öktem"], "abstract": "Learned image reconstruction has become a pillar in computational imaging and inverse problems. Among the most successful approaches are learned iterative networks, which are formulated by unrolling classical iterative optimisation algorithms for solving variational problems. While the underlying algorithm is usually formulated in the functional analytic setting, learned approaches are often viewed as purely discrete. In this chapter we present a unified operator view for learned iterative networks. Specifically, we formulate a learned reconstruction operator, defining how to compute, and separately the learning problem, which defines what to compute. In this setting we present common approaches and show that many approaches are closely related in their core. We review linear as well as nonlinear inverse problems in this framework and present a short numerical study to conclude.", "categories": ["eess.IV", "cs.LG", "math.FA", "math.NA", "math.OC"], "submitted_at": "2025-12-09T10:18:51Z", "updated_at": "2025-12-09T10:18:51Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.863455+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08443v1", "url": "https://arxiv.org/abs/2512.08443v1", "title": "Fully Decentralized Certified Unlearning", "authors": ["Hithem Lamri", "Michail Maniatakos"], "abstract": "Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\\varepsilon,δ)$ network-unlearning certificates on client views via subsampled Gaussian $Rényi$ DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy--utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\\varepsilon,δ)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\\approx 10\\%$).", "categories": ["cs.LG"], "submitted_at": "2025-12-09T10:15:15Z", "updated_at": "2025-12-09T10:15:15Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.867462+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07557v1", "url": "https://arxiv.org/abs/2512.07557v1", "title": "On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series", "authors": ["Jitendra K. Tugnait"], "abstract": "Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.", "categories": ["stat.ML", "cs.LG", "eess.SP"], "submitted_at": "2025-12-08T13:48:25Z", "updated_at": "2025-12-08T13:48:25Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.872486+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07544v1", "url": "https://arxiv.org/abs/2512.07544v1", "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue", "authors": ["Kyungro Lee", "Dongha Choi", "Hyunju Lee"], "abstract": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T13:25:00Z", "updated_at": "2025-12-08T13:25:00Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.874493+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07542v1", "url": "https://arxiv.org/abs/2512.07542v1", "title": "RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems", "authors": ["Jad Mounayer", "Sebastian Rodriguez", "Jerome Tomezyk", "Chady Ghnatios", "Francisco Chinesta"], "abstract": "Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T13:23:12Z", "updated_at": "2025-12-08T13:23:12Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.877502+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07541v1", "url": "https://arxiv.org/abs/2512.07541v1", "title": "High-Dimensional Change Point Detection using Graph Spanning Ratio", "authors": ["Youngwen Sun", "Katerina Papagiannouli", "Vladimir Spokoiny"], "abstract": "Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-08T13:22:25Z", "updated_at": "2025-12-08T13:22:25Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.879494+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08436v1", "url": "https://arxiv.org/abs/2512.08436v1", "title": "Beyond Wave Variables: A Data-Driven Ensemble Approach for Enhanced Teleoperation Transparency and Stability", "authors": ["Nour Mitiche", "Farid Ferguene", "Mourad Oussalah"], "abstract": "Time delays in communication channels present significant challenges for bilateral teleoperation systems, affecting both transparency and stability. Although traditional wave variable-based methods for a four-channel architecture ensure stability via passivity, they remain vulnerable to wave reflections and disturbances like variable delays and environmental noise. This article presents a data-driven hybrid framework that replaces the conventional wave-variable transform with an ensemble of three advanced sequence models, each optimized separately via the state-of-the-art Optuna optimizer, and combined through a stacking meta-learner. The base predictors include an LSTM augmented with Prophet for trend correction, an LSTM-based feature extractor paired with clustering and a random forest for improved regression, and a CNN-LSTM model for localized and long-term dynamics. Experimental validation was performed in Python using data generated from the baseline system implemented in MATLAB/Simulink. The results show that our optimized ensemble achieves a transparency comparable to the baseline wave-variable system under varying delays and noise, while ensuring stability through passivity constraints.", "categories": ["eess.SY", "cs.LG"], "submitted_at": "2025-12-09T10:06:05Z", "updated_at": "2025-12-09T10:06:05Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.883590+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08411v1", "url": "https://arxiv.org/abs/2512.08411v1", "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems", "authors": ["Mingwei Li", "Xiaoyuan Zhang", "Chengwei Yang", "Zilong Zheng", "Yaodong Yang"], "abstract": "Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.", "categories": ["cs.AI", "cs.RO"], "submitted_at": "2025-12-09T09:40:34Z", "updated_at": "2025-12-09T09:40:34Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.885598+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08404v1", "url": "https://arxiv.org/abs/2512.08404v1", "title": "Are generative AI text annotations systematically biased?", "authors": ["Sjoerd B. Stolwijk", "Mark Boukes", "Damian Trilling"], "abstract": "This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-09T09:36:43Z", "updated_at": "2025-12-09T09:36:43Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.887600+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08379v1", "url": "https://arxiv.org/abs/2512.08379v1", "title": "DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals", "authors": ["Kaiwei Liu", "Yuting He", "Bufang Yang", "Mu Yuan", "Chun Man Victor Wong", "Ho Pong Andrew Sze", "Zhenyu Yan", "Hongkai Chen"], "abstract": "Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T09:06:17Z", "updated_at": "2025-12-09T09:06:17Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.889598+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07540v1", "url": "https://arxiv.org/abs/2512.07540v1", "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation", "authors": ["Boxuan Lyu", "Haiyue Song", "Hidetaka Kamigaito", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Kotaro Funakoshi", "Manabu Okumura"], "abstract": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T13:21:44Z", "updated_at": "2025-12-08T13:21:44Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.892665+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07539v2", "url": "https://arxiv.org/abs/2512.07539v2", "title": "FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting", "authors": ["Qingyuan Yang", "Shizhuo Deng", "Dongyue Chen", "Da Teng", "Zehua Gan"], "abstract": "Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T13:18:20Z", "updated_at": "2025-12-09T06:21:22Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.896665+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07533v1", "url": "https://arxiv.org/abs/2512.07533v1", "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection", "authors": ["Yuzhou Nie", "Hongwei Li", "Chengquan Guo", "Ruizhe Jiang", "Zhun Wang", "Bo Li", "Dawn Song", "Wenbo Guo"], "abstract": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-08T13:06:23Z", "updated_at": "2025-12-08T13:06:23Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.898665+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08376v1", "url": "https://arxiv.org/abs/2512.08376v1", "title": "A Distribution Testing Approach to Clustering Distributions", "authors": ["Gunjan Kumar", "Yash Pote", "Jonathan Scarlett"], "abstract": "We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\\varepsilon)$ (up to an $O(\\log k)$ factor) for all regimes.", "categories": ["cs.DS", "cs.IT", "math.ST", "stat.ML"], "submitted_at": "2025-12-09T09:01:41Z", "updated_at": "2025-12-09T09:01:41Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.905704+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08371v1", "url": "https://arxiv.org/abs/2512.08371v1", "title": "A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research", "authors": ["Simon Chung", "Colby J. Vorland", "Donna L. Maney", "Andrew W. Brown"], "abstract": "Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-09T08:54:20Z", "updated_at": "2025-12-09T08:54:20Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.909705+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08366v1", "url": "https://arxiv.org/abs/2512.08366v1", "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making", "authors": ["Wentao Zhang", "Qunbo Wang", "Tao Zhang", "Junsheng Wu", "Hongping Gan", "Yang Liu", "Ling Dai", "Shizhuang Deng", "Shuntong Sun"], "abstract": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T08:44:59Z", "updated_at": "2025-12-09T08:44:59Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.912757+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07522v1", "url": "https://arxiv.org/abs/2512.07522v1", "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "authors": ["Sebastian Sztwiertnia", "Felix Friedrich", "Kristian Kersting", "Patrick Schramowski", "Björn Deiseroth"], "abstract": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T12:59:24Z", "updated_at": "2025-12-08T12:59:24Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.915347+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07519v1", "url": "https://arxiv.org/abs/2512.07519v1", "title": "Machine Learning: Progress and Prospects", "authors": ["Alexander Gammerman"], "abstract": "This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.\n  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of \"simplicity\" known as \"Ockham's razor\" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that \"we learn some things only by doing things\".\n  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T12:57:16Z", "updated_at": "2025-12-08T12:57:16Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.918356+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07515v1", "url": "https://arxiv.org/abs/2512.07515v1", "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "Guangquan Zhang"], "abstract": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T12:50:41Z", "updated_at": "2025-12-08T12:50:41Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.920856+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07509v1", "url": "https://arxiv.org/abs/2512.07509v1", "title": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces", "authors": ["Nikita Gabdullin"], "abstract": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "submitted_at": "2025-12-08T12:46:39Z", "updated_at": "2025-12-08T12:46:39Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.924339+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08365v1", "url": "https://arxiv.org/abs/2512.08365v1", "title": "Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging", "authors": ["Yi Pan", "Wenbo Qian", "Dedong Xie", "Ruiyan Hu", "Yigong Hu", "Baris Kasikci"], "abstract": "The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.\n  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.", "categories": ["cs.DC", "cs.LG"], "submitted_at": "2025-12-09T08:41:16Z", "updated_at": "2025-12-09T08:41:16Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.926347+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08360v1", "url": "https://arxiv.org/abs/2512.08360v1", "title": "Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata", "authors": ["Ali Sakour"], "abstract": "Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "submitted_at": "2025-12-09T08:36:54Z", "updated_at": "2025-12-09T08:36:54Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.930855+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08345v1", "url": "https://arxiv.org/abs/2512.08345v1", "title": "The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations", "authors": ["Benedikt Mangold"], "abstract": "Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled \"sociological sandbox\". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with \"toxic\" system prompts. Our results demonstrate a statistically significant increase of approximately 25\\% in the duration of conversations involving toxic participants. We propose that this \"latency of toxicity\" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "submitted_at": "2025-12-09T08:17:35Z", "updated_at": "2025-12-09T08:17:35Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.933138+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08344v1", "url": "https://arxiv.org/abs/2512.08344v1", "title": "Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions", "authors": ["Tien Cuong Bui"], "abstract": "Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.", "categories": ["cs.AI", "cs.IT", "cs.LG"], "submitted_at": "2025-12-09T08:13:31Z", "updated_at": "2025-12-09T08:13:31Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.936929+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07501v1", "url": "https://arxiv.org/abs/2512.07501v1", "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "abstract": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "categories": ["cs.SE", "cs.AI"], "submitted_at": "2025-12-08T12:35:10Z", "updated_at": "2025-12-08T12:35:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.939923+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07497v2", "url": "https://arxiv.org/abs/2512.07497v2", "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations", "authors": ["JV Roig"], "abstract": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.", "categories": ["cs.AI", "cs.SE"], "submitted_at": "2025-12-08T12:27:15Z", "updated_at": "2025-12-09T08:55:59Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.940925+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07490v1", "url": "https://arxiv.org/abs/2512.07490v1", "title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent", "authors": ["Zhiyu Liu", "Zhi Han", "Yandong Tang", "Jun Fan", "Yao Wang"], "abstract": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.", "categories": ["cs.LG", "math.OC"], "submitted_at": "2025-12-08T12:17:40Z", "updated_at": "2025-12-08T12:17:40Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.945353+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07487v1", "url": "https://arxiv.org/abs/2512.07487v1", "title": "Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility", "authors": ["David M. Allison", "Stephen Herzog"], "abstract": "A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.", "categories": ["cs.CY", "cs.AI", "cs.ET"], "submitted_at": "2025-12-08T12:14:51Z", "updated_at": "2025-12-08T12:14:51Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.947355+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08343v1", "url": "https://arxiv.org/abs/2512.08343v1", "title": "Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach", "authors": ["Caner Erden", "Alparslan Serhat Demir", "Abdullah Hulusi Kokcam", "Talas Fikret Kurnaz", "Ugur Dagdeviren"], "abstract": "Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T08:13:04Z", "updated_at": "2025-12-09T08:13:04Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.951877+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08340v1", "url": "https://arxiv.org/abs/2512.08340v1", "title": "Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye", "authors": ["Abdullah Hulusi Kökçam", "Uğur Dağdeviren", "Talas Fikret Kurnaz", "Alparslan Serhat Demir", "Caner Erden"], "abstract": "The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-09T08:09:55Z", "updated_at": "2025-12-09T08:09:55Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.960114+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07486v1", "url": "https://arxiv.org/abs/2512.07486v1", "title": "Materium: An Autoregressive Approach for Material Generation", "authors": ["Niklas Dobberstein", "Jan Hamaekers"], "abstract": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "submitted_at": "2025-12-08T12:14:42Z", "updated_at": "2025-12-08T12:14:42Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.966650+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07482v1", "url": "https://arxiv.org/abs/2512.07482v1", "title": "From Real-World Traffic Data to Relevant Critical Scenarios", "authors": ["Florian Lüttner", "Nicole Neis", "Daniel Stadler", "Robin Moss", "Mirjam Fehling-Kaschek", "Matthias Pfriem", "Alexander Stolz", "Jens Ziehn"], "abstract": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-08T12:07:15Z", "updated_at": "2025-12-08T12:07:15Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.968643+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07472v1", "url": "https://arxiv.org/abs/2512.07472v1", "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "abstract": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-08T11:57:13Z", "updated_at": "2025-12-08T11:57:13Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.971155+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07917v1", "url": "https://arxiv.org/abs/2512.07917v1", "title": "CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation", "authors": ["Zhehao Dong", "Shanghai Du", "Zhen Lu", "Yue Yang"], "abstract": "Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.", "categories": ["cs.SE", "cs.AI", "physics.flu-dyn"], "submitted_at": "2025-12-08T11:42:32Z", "updated_at": "2025-12-08T11:42:32Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.974328+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08333v1", "url": "https://arxiv.org/abs/2512.08333v1", "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging", "authors": ["Yajat Yadav", "Zhiyuan Zhou", "Andrew Wagenmaker", "Karl Pertsch", "Sergey Levine"], "abstract": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-09T08:02:11Z", "updated_at": "2025-12-09T08:02:11Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.978326+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08329v1", "url": "https://arxiv.org/abs/2512.08329v1", "title": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models", "authors": ["Michael R. Martin", "Garrick Chan", "Kwan-Liu Ma"], "abstract": "Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T07:55:11Z", "updated_at": "2025-12-09T07:55:11Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.980836+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08327v1", "url": "https://arxiv.org/abs/2512.08327v1", "title": "Low Rank Support Quaternion Matrix Machine", "authors": ["Wang Chen", "Ziyan Luo", "Shuangyue Wang"], "abstract": "Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.", "categories": ["cs.CV", "cs.LG", "math.OC", "stat.ML"], "submitted_at": "2025-12-09T07:42:33Z", "updated_at": "2025-12-09T07:42:33Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.982930+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08326v1", "url": "https://arxiv.org/abs/2512.08326v1", "title": "Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships", "authors": ["Bin Wang", "Hui Li", "Liyang Zhang", "Qijia Zhuang", "Ao Yang", "Dong Zhang", "Xijun Luo", "Bing Lin"], "abstract": "Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-09T07:42:10Z", "updated_at": "2025-12-09T07:42:10Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:37.986921+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07463v1", "url": "https://arxiv.org/abs/2512.07463v1", "title": "Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification", "authors": ["Rongmei Liang", "Zizheng Liu", "Xiaofei Wu", "Jingwen Tu"], "abstract": "In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.", "categories": ["cs.LG", "stat.AP", "stat.CO"], "submitted_at": "2025-12-08T11:41:06Z", "updated_at": "2025-12-08T11:41:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.988920+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07462v1", "url": "https://arxiv.org/abs/2512.07462v1", "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "abstract": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "submitted_at": "2025-12-08T11:40:03Z", "updated_at": "2025-12-08T11:40:03Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.991934+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07458v1", "url": "https://arxiv.org/abs/2512.07458v1", "title": "Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems", "authors": ["Dmitrii Kapitan", "Pavel Ovchinnikov", "Konstantin Soldatov", "Petr Andriushchenko", "Vitalii Kapitan"], "abstract": "This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.", "categories": ["physics.comp-ph", "cond-mat.dis-nn", "cs.LG"], "submitted_at": "2025-12-08T11:33:24Z", "updated_at": "2025-12-08T11:33:24Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.994777+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07454v1", "url": "https://arxiv.org/abs/2512.07454v1", "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "authors": ["Amir Mohammad Akhlaghi", "Amirhossein Shabani", "Mostafa Abdolmaleki", "Saeed Reza Kheradpisheh"], "abstract": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T11:27:52Z", "updated_at": "2025-12-08T11:27:52Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.995779+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07453v1", "url": "https://arxiv.org/abs/2512.07453v1", "title": "Social welfare optimisation in well-mixed and structured populations", "authors": ["Van An Nguyen", "Vuong Khang Huynh", "Ho Nam Duong", "Huu Loi Bui", "Hai Anh Ha", "Quang Dung Le", "Le Quoc Dung Ngo", "Tan Dat Nguyen", "Ngoc Ngu Nguyen", "Hoai Thuong Nguyen", "Zhao Song", "Le Hong Trang", "The Anh Han"], "abstract": "Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.", "categories": ["physics.soc-ph", "cs.AI", "cs.MA", "math.OC", "nlin.AO"], "submitted_at": "2025-12-08T11:27:43Z", "updated_at": "2025-12-08T11:27:43Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:37.997778+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08317v1", "url": "https://arxiv.org/abs/2512.08317v1", "title": "GeoDM: Geometry-aware Distribution Matching for Dataset Distillation", "authors": ["Xuhui Li", "Zhengquan Luo", "Zihui Cui", "Zhiqiang Xu"], "abstract": "Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \\textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T07:31:57Z", "updated_at": "2025-12-09T07:31:57Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.001844+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08314v1", "url": "https://arxiv.org/abs/2512.08314v1", "title": "Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning", "authors": ["M Yashwanth", "Gaurav Kumar Nayak", "Harsh Rangwani", "Arya Singh", "R. Venkatesh Babu", "Anirban Chakraborty"], "abstract": "Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T07:23:16Z", "updated_at": "2025-12-09T07:23:16Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.003851+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08309v1", "url": "https://arxiv.org/abs/2512.08309v1", "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation", "authors": ["Alexander Goslin"], "abstract": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "submitted_at": "2025-12-09T07:10:35Z", "updated_at": "2025-12-09T07:10:35Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.006851+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08306v1", "url": "https://arxiv.org/abs/2512.08306v1", "title": "Jacobian Aligned Random Forests", "authors": ["Sarwesh Rauniyar"], "abstract": "Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T07:08:04Z", "updated_at": "2025-12-09T07:08:04Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.008851+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08305v1", "url": "https://arxiv.org/abs/2512.08305v1", "title": "Magnetic activity of ultracool dwarfs in the LAMOST DR11", "authors": ["Yue Xiang", "Shenghong Gu", "Dongtao Cao"], "abstract": "Ultracool dwarfs consist of lowest-mass stars and brown dwarfs. Their interior is fully convective, different from that of the partly-convective Sun-like stars. Magnetic field generation process beneath the surface of ultracool dwarfs is still poorly understood and controversial. To increase samples of active ultracool dwarfs significantly, we have identified 962 ultracool dwarfs in the latest LAMOST data release, DR11. We also simulate the Chinese Space Station Survey Telescope (CSST) low-resolution slitless spectra by degrading the LAMOST spectra. A semi-supervised machine learning approach with an autoencoder model is built to identify ultracool dwarfs with the simulated CSST spectra, which demonstrates the capability of the CSST all-sky slitless spectroscopic survey on the detection of ultracool dwarfs. Magnetic activity of the ultracool dwarfs is investigated by using the H$α$ line emission as a proxy. The rotational periods of 82 ultracool dwarfs are derived based on the Kepler/K2 light curves. We also derive the activity-rotation relation of the ultracool dwarfs, which is saturated around a Rossby number of 0.12.", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.LG"], "submitted_at": "2025-12-09T07:04:26Z", "updated_at": "2025-12-09T07:04:26Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.010850+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07450v1", "url": "https://arxiv.org/abs/2512.07450v1", "title": "Forget and Explain: Transparent Verification of GNN Unlearning", "authors": ["Imran Ahsan", "Hyunwook Yu", "Jinsung Kim", "Mucheol Kim"], "abstract": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T11:25:19Z", "updated_at": "2025-12-08T11:25:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.014905+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07436v1", "url": "https://arxiv.org/abs/2512.07436v1", "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "abstract": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T11:12:39Z", "updated_at": "2025-12-08T11:12:39Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.022130+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08300v1", "url": "https://arxiv.org/abs/2512.08300v1", "title": "rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection", "authors": ["Sijia Chen", "Baochun Li", "Di Niu"], "abstract": "Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T06:55:39Z", "updated_at": "2025-12-09T06:55:39Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.028146+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08296v1", "url": "https://arxiv.org/abs/2512.08296v1", "title": "Towards a Science of Scaling Agent Systems", "authors": ["Yubin Kim", "Ken Gu", "Chanwoo Park", "Chunjong Park", "Samuel Schmidgall", "A. Ali Heydari", "Yao Yan", "Zhihan Zhang", "Yuchen Zhuang", "Mark Malhotra", "Paul Pu Liang", "Hae Won Park", "Yuzhe Yang", "Xuhai Xu", "Yilun Du", "Shwetak Patel", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "abstract": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T06:52:21Z", "updated_at": "2025-12-09T06:52:21Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.033450+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07433v1", "url": "https://arxiv.org/abs/2512.07433v1", "title": "Mitigating Bias in Graph Hyperdimensional Computing", "authors": ["Yezi Liu", "William Youngwoo Chung", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "abstract": "Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\\approx 10\\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.", "categories": ["cs.LG", "cs.SI"], "submitted_at": "2025-12-08T11:09:24Z", "updated_at": "2025-12-08T11:09:24Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.036459+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07430v1", "url": "https://arxiv.org/abs/2512.07430v1", "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis", "authors": ["Yangle Li", "Danli Luo", "Haifeng Hu"], "abstract": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T11:04:00Z", "updated_at": "2025-12-08T11:04:00Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.038457+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07426v1", "url": "https://arxiv.org/abs/2512.07426v1", "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing", "authors": ["Karel Moens", "Matthew B. Blaschko", "Tinne Tuytelaars", "Bart Diricx", "Jonas De Vylder", "Mustafa Yousif"], "abstract": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T11:01:07Z", "updated_at": "2025-12-08T11:01:07Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.043041+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07425v1", "url": "https://arxiv.org/abs/2512.07425v1", "title": "Microseismic event classification with a lightweight Fourier Neural Operator model", "authors": ["Ayrat Abdullin", "Umair bin Waheed", "Leo Eisner", "Abdullatif Al-Shuhail"], "abstract": "Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.", "categories": ["physics.geo-ph", "cs.LG"], "submitted_at": "2025-12-08T10:57:36Z", "updated_at": "2025-12-08T10:57:36Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.045042+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08290v1", "url": "https://arxiv.org/abs/2512.08290v1", "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem", "authors": ["Shiva Gaire", "Srijan Gyawali", "Saroj Mishra", "Suman Niroula", "Dilip Thakur", "Umesh Yadav"], "abstract": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-09T06:39:21Z", "updated_at": "2025-12-09T06:39:21Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.049052+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08286v1", "url": "https://arxiv.org/abs/2512.08286v1", "title": "Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework", "authors": ["Liao Hu", "Qiteng Wu", "Ruoyu Qi"], "abstract": "The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.", "categories": ["cs.SE", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T06:34:28Z", "updated_at": "2025-12-09T06:34:28Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.051040+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08281v1", "url": "https://arxiv.org/abs/2512.08281v1", "title": "Probabilistic Multi-Agent Aircraft Landing Time Prediction", "authors": ["Kyungmin Kim", "Seokbin Yoon", "Keumjin Lee"], "abstract": "Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.", "categories": ["cs.MA", "cs.LG"], "submitted_at": "2025-12-09T06:27:26Z", "updated_at": "2025-12-09T06:27:26Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.052109+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08280v1", "url": "https://arxiv.org/abs/2512.08280v1", "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making", "authors": ["Haldun Balim", "Na Li", "Yilun Du"], "abstract": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.", "categories": ["cs.RO", "cs.AI", "eess.SY"], "submitted_at": "2025-12-09T06:26:02Z", "updated_at": "2025-12-09T06:26:02Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.056215+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07415v1", "url": "https://arxiv.org/abs/2512.07415v1", "title": "Data-driven Exploration of Mobility Interaction Patterns", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "abstract": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T10:50:24Z", "updated_at": "2025-12-08T10:50:24Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.071266+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08277v1", "url": "https://arxiv.org/abs/2512.08277v1", "title": "FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection", "authors": ["Yihan Liao", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Jialong Li"], "abstract": "Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.", "categories": ["cs.SE", "cs.LG"], "submitted_at": "2025-12-09T06:15:53Z", "updated_at": "2025-12-09T06:15:53Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.073804+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08274v1", "url": "https://arxiv.org/abs/2512.08274v1", "title": "gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs", "authors": ["Humera Sabir", "Fatima Farooq", "Ashraf Aboulnaga"], "abstract": "Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.", "categories": ["cs.LG", "cs.DB"], "submitted_at": "2025-12-09T06:08:37Z", "updated_at": "2025-12-09T06:08:37Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.075804+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08273v1", "url": "https://arxiv.org/abs/2512.08273v1", "title": "AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content", "authors": ["Thanh Vu", "Richi Nayak", "Thiru Balasubramaniam"], "abstract": "Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T06:03:25Z", "updated_at": "2025-12-09T06:03:25Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.080808+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07404v1", "url": "https://arxiv.org/abs/2512.07404v1", "title": "Do LLMs Trust the Code They Write?", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "abstract": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "categories": ["cs.SE", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T10:38:03Z", "updated_at": "2025-12-08T10:38:03Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.090838+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07400v1", "url": "https://arxiv.org/abs/2512.07400v1", "title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse", "authors": ["Giulia Lanzillotta", "Damiano Meier", "Thomas Hofmann"], "abstract": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T10:35:57Z", "updated_at": "2025-12-08T10:35:57Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.093343+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07393v1", "url": "https://arxiv.org/abs/2512.07393v1", "title": "Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects", "authors": ["Yann Bourdin", "Pierrick Legrand", "Fanny Roche"], "abstract": "This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T10:26:27Z", "updated_at": "2025-12-08T10:26:27Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.095344+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07390v1", "url": "https://arxiv.org/abs/2512.07390v1", "title": "Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood", "authors": ["Gilhyun Nam", "Taewon Kim", "Joonhyun Jeong", "Eunho Yang"], "abstract": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-08T10:23:44Z", "updated_at": "2025-12-08T10:23:44Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.098347+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08270v1", "url": "https://arxiv.org/abs/2512.08270v1", "title": "Reasoning Models Ace the CFA Exams", "authors": ["Jaisal Patel", "Yunzhe Chen", "Kaiwen He", "Keyi Wang", "David Li", "Kairong Xiao", "Xiao-Yang Liu"], "abstract": "Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.", "categories": ["cs.AI", "cs.CL", "q-fin.GN"], "submitted_at": "2025-12-09T05:57:19Z", "updated_at": "2025-12-09T05:57:19Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.108418+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08267v1", "url": "https://arxiv.org/abs/2512.08267v1", "title": "SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing", "authors": ["Yi Ni", "Xinkun Wang", "Han Zhang"], "abstract": "Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \\textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.\n  The framework is built upon three core mechanisms: (1) \\textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \\textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \\textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.\n  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T05:47:44Z", "updated_at": "2025-12-09T05:47:44Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.110908+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07375v1", "url": "https://arxiv.org/abs/2512.07375v1", "title": "LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples", "authors": ["Yezi Liu", "Hanning Chen", "Wenjun Huang", "Yang Ni", "Mohsen Imani"], "abstract": "Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-08T10:10:29Z", "updated_at": "2025-12-08T10:10:29Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.117998+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07374v1", "url": "https://arxiv.org/abs/2512.07374v1", "title": "Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning", "authors": ["Yezi Liu", "Hanning Chen", "Wenjun Huang", "Yang Ni", "Mohsen Imani"], "abstract": "Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-08T10:10:12Z", "updated_at": "2025-12-08T10:10:12Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.120997+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06926v1", "url": "https://arxiv.org/abs/2512.06926v1", "title": "Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise", "authors": ["Salma Albelali", "Moataz Ahmed"], "abstract": "Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T17:10:06Z", "updated_at": "2025-12-07T17:10:06Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.161711+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08264v1", "url": "https://arxiv.org/abs/2512.08264v1", "title": "Mathematical Foundations of Neural Tangents and Infinite-Width Networks", "authors": ["Rachana Mysore", "Preksha Girish", "Kavitha Jayaram", "Shrey Kumar", "Preksha Girish", "Shravan Sanjeev Bagal", "Kavitha Jayaram", "Shreya Aravind Shastry"], "abstract": "We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T05:41:40Z", "updated_at": "2025-12-09T05:41:40Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.163720+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08261v1", "url": "https://arxiv.org/abs/2512.08261v1", "title": "Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes", "authors": ["Yibowen Zhao", "Yinan Zhang", "Zhixiang Su", "Lizhen Cui", "Chunyan Miao"], "abstract": "Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T05:37:54Z", "updated_at": "2025-12-09T05:37:54Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.171763+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07371v1", "url": "https://arxiv.org/abs/2512.07371v1", "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning", "authors": ["Byungju Kim", "Jinu Pahk", "Chungwoo Lee", "Jaejoon Kim", "Jangha Lee", "Theo Taeyeong Kim", "Kyuhwan Shim", "Jun Ki Lee", "Byoung-Tak Zhang"], "abstract": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-08T10:08:33Z", "updated_at": "2025-12-08T10:08:33Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.179774+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07360v1", "url": "https://arxiv.org/abs/2512.07360v1", "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "authors": ["Qiming Huang", "Hao Ai", "Jianbo Jiao"], "abstract": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T10:00:36Z", "updated_at": "2025-12-08T10:00:36Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.181831+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06921v1", "url": "https://arxiv.org/abs/2512.06921v1", "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification", "authors": ["Ziyang Song", "Zelin Zang", "Xiaofan Ye", "Boqiang Xu", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "abstract": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "submitted_at": "2025-12-07T17:00:25Z", "updated_at": "2025-12-07T17:00:25Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.190283+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08257v1", "url": "https://arxiv.org/abs/2512.08257v1", "title": "Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability", "authors": ["Preksha Girish", "Rachana Mysore", "Mahanthesha U", "Shrey Kumar", "Misbah Fatimah Annigeri", "Tanish Jain"], "abstract": "Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.", "categories": ["cs.LG", "eess.IV"], "submitted_at": "2025-12-09T05:28:09Z", "updated_at": "2025-12-09T05:28:09Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.199818+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08256v1", "url": "https://arxiv.org/abs/2512.08256v1", "title": "Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations", "authors": ["Deepak Gupta", "Himanshu Pandey", "Ratikanta Behera"], "abstract": "This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.", "categories": ["cs.LG", "math.AP", "math.QA"], "submitted_at": "2025-12-09T05:27:15Z", "updated_at": "2025-12-09T05:27:15Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.201851+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08247v1", "url": "https://arxiv.org/abs/2512.08247v1", "title": "Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection", "authors": ["Haowen Zheng", "Hu Zhu", "Lu Deng", "Weihao Gu", "Yang Yang", "Yanyan Liang"], "abstract": "Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T05:01:29Z", "updated_at": "2025-12-09T05:01:29Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.204469+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08246v1", "url": "https://arxiv.org/abs/2512.08246v1", "title": "SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes", "authors": ["Nicholas Harner"], "abstract": "Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T05:00:01Z", "updated_at": "2025-12-09T05:00:01Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.207475+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07355v1", "url": "https://arxiv.org/abs/2512.07355v1", "title": "A Geometric Unification of Concept Learning with Concept Cones", "authors": ["Alexandre Rocchi--Henry", "Thomas Fel", "Gianni Franchi"], "abstract": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "categories": ["cs.AI", "cs.CV", "cs.LG"], "submitted_at": "2025-12-08T09:51:46Z", "updated_at": "2025-12-08T09:51:46Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.213229+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07351v1", "url": "https://arxiv.org/abs/2512.07351v1", "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection", "authors": ["Sayeem Been Zaman", "Wasimul Karim", "Arefin Ittesafun Abian", "Reem E. Mohamed", "Md Rafiqul Islam", "Asif Karim", "Sami Azam"], "abstract": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.", "categories": ["cs.CV", "cs.AI", "cs.SD"], "submitted_at": "2025-12-08T09:43:30Z", "updated_at": "2025-12-08T09:43:30Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.216228+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07344v1", "url": "https://arxiv.org/abs/2512.07344v1", "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding", "authors": ["Shengyuan Ye", "Bei Ouyang", "Tianyi Qian", "Liekang Zeng", "Mu Yuan", "Xiaowen Chu", "Weijie Hong", "Xu Chen"], "abstract": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.", "categories": ["cs.DC", "cs.AI"], "submitted_at": "2025-12-08T09:32:47Z", "updated_at": "2025-12-08T09:32:47Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.218234+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06914v1", "url": "https://arxiv.org/abs/2512.06914v1", "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions", "authors": ["Guanquan Shi", "Haohua Du", "Zhiqiang Wang", "Xiaoyu Liang", "Weiwenpei Liu", "Song Bian", "Zhenyu Guan"], "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.\n  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-07T16:41:02Z", "updated_at": "2025-12-07T16:41:02Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.238370+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08243v1", "url": "https://arxiv.org/abs/2512.08243v1", "title": "Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI", "authors": ["Saeeda Naz", "Saddam Hussain Khan"], "abstract": "A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-09T04:52:30Z", "updated_at": "2025-12-09T04:52:30Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.241896+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08241v1", "url": "https://arxiv.org/abs/2512.08241v1", "title": "Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning", "authors": ["Preksha Girish", "Rachana Mysore", "Mahanthesha U", "Shrey Kumar", "Shipra Prashant"], "abstract": "This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T04:48:50Z", "updated_at": "2025-12-09T04:48:50Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.246251+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08240v1", "url": "https://arxiv.org/abs/2512.08240v1", "title": "HybridToken-VLM: Hybrid Token Compression for Vision-Language Models", "authors": ["Jusheng Zhang", "Xiaoyang Guo", "Kaitong Cai", "Qinhan Lv", "Yijia Fan", "Wenhao Chai", "Jian Wang", "Keze Wang"], "abstract": "Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T04:48:38Z", "updated_at": "2025-12-09T04:48:38Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.254504+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07335v1", "url": "https://arxiv.org/abs/2512.07335v1", "title": "Machine learning in an expectation-maximisation framework for nowcasting", "authors": ["Paul Wilsens", "Katrien Antonio", "Gerda Claeskens"], "abstract": "Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-08T09:22:22Z", "updated_at": "2025-12-08T09:22:22Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.259569+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07332v1", "url": "https://arxiv.org/abs/2512.07332v1", "title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach", "authors": ["Zhengquan Luo", "Guy Tadmor", "Or Amar", "David Zeevi", "Zhiqiang Xu"], "abstract": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T09:20:06Z", "updated_at": "2025-12-08T09:20:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.267621+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06906v1", "url": "https://arxiv.org/abs/2512.06906v1", "title": "MINES: Explainable Anomaly Detection through Web API Invariant Inference", "authors": ["Wenjie Zhang", "Yun Lin", "Chun Fung Amos Kwok", "Xiwen Teoh", "Xiaofei Xie", "Frank Liauw", "Hongyu Zhang", "Jin Song Dong"], "abstract": "Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.", "categories": ["cs.SE", "cs.CR", "cs.DB", "cs.LG"], "submitted_at": "2025-12-07T16:13:35Z", "updated_at": "2025-12-07T16:13:35Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.278332+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06902v1", "url": "https://arxiv.org/abs/2512.06902v1", "title": "BabelCoder: Agentic Code Translation with Specification Alignment", "authors": ["Fazle Rabbi", "Soumit Kanti Saha", "Tri Minh Triet Pham", "Song Wang", "Jinqiu Yang"], "abstract": "As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.", "categories": ["cs.SE", "cs.AI"], "submitted_at": "2025-12-07T15:57:54Z", "updated_at": "2025-12-07T15:57:54Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.285359+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08238v1", "url": "https://arxiv.org/abs/2512.08238v1", "title": "SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality", "authors": ["Mahathir Monjur", "Shahriar Nirjon"], "abstract": "Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.", "categories": ["cs.SD", "cs.AI"], "submitted_at": "2025-12-09T04:39:50Z", "updated_at": "2025-12-09T04:39:50Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.291887+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08230v1", "url": "https://arxiv.org/abs/2512.08230v1", "title": "Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions", "authors": ["Eunice Yiu", "Kelsey Allen", "Shiry Ginosar", "Alison Gopnik"], "abstract": "Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called \"empowerment\" which maximizes mutual information between actions and their outcomes. \"Empowerment\" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.", "categories": ["cs.AI"], "submitted_at": "2025-12-09T04:14:48Z", "updated_at": "2025-12-09T04:14:48Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.295670+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07329v1", "url": "https://arxiv.org/abs/2512.07329v1", "title": "Two-dimensional RMSD projections for reaction path visualization and validation", "authors": ["Rohit Goswami"], "abstract": "Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "submitted_at": "2025-12-08T09:15:24Z", "updated_at": "2025-12-08T09:15:24Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.298664+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07328v1", "url": "https://arxiv.org/abs/2512.07328v1", "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "abstract": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T09:12:18Z", "updated_at": "2025-12-08T09:12:18Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.301688+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06885v1", "url": "https://arxiv.org/abs/2512.06885v1", "title": "JoPano: Unified Panorama Generation via Joint Modeling", "authors": ["Wancheng Feng", "Chen An", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Lukun Wang"], "abstract": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T15:19:26Z", "updated_at": "2025-12-07T15:19:26Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.315751+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06879v1", "url": "https://arxiv.org/abs/2512.06879v1", "title": "WisPaper: Your AI Scholar Search Engine", "authors": ["Li Ju", "Jun Zhao", "Mingxu Chai", "Ziyu Shen", "Xiangyang Wang", "Yage Geng", "Chunchun Ma", "Hao Peng", "Guangbin Li", "Tao Li", "Chengyong Liao", "Fu Wang", "Xiaolong Wang", "Junshen Chen", "Rui Gong", "Shijia Liang", "Feiyan Li", "Ming Zhang", "Kexin Tan", "Jujie Ye", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Yuankai Ying", "Yang Shi", "Yue Zhang", "Qi Zhang"], "abstract": "Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \\textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \\textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \\textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \\textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \\textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.", "categories": ["cs.IR", "cs.AI"], "submitted_at": "2025-12-07T15:10:20Z", "updated_at": "2025-12-07T15:10:20Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.321793+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06867v1", "url": "https://arxiv.org/abs/2512.06867v1", "title": "Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?", "authors": ["John Licato", "Stephen Steinle", "Brayden Hollis"], "abstract": "Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.", "categories": ["cs.AI"], "submitted_at": "2025-12-07T14:42:29Z", "updated_at": "2025-12-07T14:42:29Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.329800+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08228v1", "url": "https://arxiv.org/abs/2512.08228v1", "title": "MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models", "authors": ["Jusheng Zhang", "Kaitong Cai", "Xiaoyang Guo", "Sidi Liu", "Qinhan Lv", "Ruiqi Chen", "Jing Yang", "Yijia Fan", "Xiaofei Sun", "Jian Wang", "Ziliang Chen", "Liang Lin", "Keze Wang"], "abstract": "The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-09T04:13:31Z", "updated_at": "2025-12-09T04:13:31Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.333521+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08218v1", "url": "https://arxiv.org/abs/2512.08218v1", "title": "PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning", "authors": ["Ye Qin", "Jingchao Wang", "Yang Shi", "Haiying Huang", "Junxu Li", "Weijian Liu", "Tinghui Chen", "Jinghui Qin"], "abstract": "Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\\-world graphs poorly by fixed\\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\\-Euclidean pseudo\\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\\-Riemannian manifolds and derive a Pseudo\\-Riemannian Capsule Network (PR\\-CapsNet), which models data in pseudo\\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\\-CapsNet enhances the CapsNet with Adaptive Pseudo\\-Riemannian Tangent Space Routing by utilizing pseudo\\-Riemannian geometry. Unlike single\\-curvature or subspace\\-partitioning methods, PR\\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\\-Riemannian metric. It first deploys Pseudo\\-Riemannian Tangent Space Routing to decompose capsule states into spherical\\-temporal and Euclidean\\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\\-preserved Pseudo\\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\\-CapsNet outperforms SOTA models, validating PR\\-CapsNet's strong representation power for complex graph structures.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-09T03:54:51Z", "updated_at": "2025-12-09T03:54:51Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.336220+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08217v1", "url": "https://arxiv.org/abs/2512.08217v1", "title": "Correction of Decoupled Weight Decay", "authors": ["Jason Chuan-Chih Chou"], "abstract": "Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T03:52:54Z", "updated_at": "2025-12-09T03:52:54Z", "rl_tags": ["general_dl"], "attention_score": 0.9, "collected_at": "2025-12-10T03:47:38.342649+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07313v1", "url": "https://arxiv.org/abs/2512.07313v1", "title": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach", "authors": ["Bosun Kang", "Hyejun Park", "Chenglin Fan"], "abstract": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.", "categories": ["cs.LG", "cs.DS"], "submitted_at": "2025-12-08T08:56:25Z", "updated_at": "2025-12-08T08:56:25Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.348582+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07312v1", "url": "https://arxiv.org/abs/2512.07312v1", "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Yuhang Gu", "Wei Zhang"], "abstract": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.", "categories": ["cs.AR", "cs.AI", "cs.DC"], "submitted_at": "2025-12-08T08:56:10Z", "updated_at": "2025-12-08T08:56:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.351119+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07310v1", "url": "https://arxiv.org/abs/2512.07310v1", "title": "Towards a Relationship-Aware Transformer for Tabular Data", "authors": ["Andrei V. Konstantinov", "Valerii A. Zuev", "Lev V. Utkin"], "abstract": "Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T08:54:53Z", "updated_at": "2025-12-08T08:54:53Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.356398+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06866v1", "url": "https://arxiv.org/abs/2512.06866v1", "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior", "authors": ["Yulin Li", "Haokun Gui", "Ziyang Fan", "Junjie Wang", "Bin Kang", "Bin Chen", "Zhuotao Tian"], "abstract": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-07T14:42:10Z", "updated_at": "2025-12-07T14:42:10Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.358394+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06859v1", "url": "https://arxiv.org/abs/2512.06859v1", "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models", "authors": ["Ce Chi", "Xing Wang", "Zhendong Wang", "Xiaofan Liu", "Ce Li", "Zhiyan Song", "Chen Zhao", "Kexin Yang", "Boshen Shi", "Jingjing Yang", "Chao Deng", "Junlan Feng"], "abstract": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.", "categories": ["cs.AI"], "submitted_at": "2025-12-07T14:29:23Z", "updated_at": "2025-12-07T14:29:23Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.361927+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06854v1", "url": "https://arxiv.org/abs/2512.06854v1", "title": "ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design", "authors": ["Qijun Zhang", "Yao Lu", "Mengming Li", "Shang Liu", "Zhiyao Xie"], "abstract": "Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.", "categories": ["cs.AR", "cs.AI"], "submitted_at": "2025-12-07T14:12:06Z", "updated_at": "2025-12-07T14:12:06Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.364120+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06850v1", "url": "https://arxiv.org/abs/2512.06850v1", "title": "Formal that \"Floats\" High: Formal Verification of Floating Point Arithmetic", "authors": ["Hansa Mohanty", "Vaisakh Naduvodi Viswambharan", "Deepak Narayan Gadde"], "abstract": "Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.", "categories": ["cs.LO", "cs.AI", "cs.AR"], "submitted_at": "2025-12-07T14:03:44Z", "updated_at": "2025-12-07T14:03:44Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.367122+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08211v1", "url": "https://arxiv.org/abs/2512.08211v1", "title": "MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones", "authors": ["Jiaxiang Geng", "Lunyu Zhao", "Yiyi Lu", "Bing Luo"], "abstract": "Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T03:41:01Z", "updated_at": "2025-12-09T03:41:01Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.376203+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08193v1", "url": "https://arxiv.org/abs/2512.08193v1", "title": "ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access", "authors": ["Jiwoo Park", "Ruoqi Liu", "Avani Jagdale", "Andrew Srisuwananukorn", "Jing Zhao", "Lang Li", "Ping Zhang", "Sachin Kumar"], "abstract": "We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "submitted_at": "2025-12-09T02:52:06Z", "updated_at": "2025-12-09T02:52:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.379206+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07309v1", "url": "https://arxiv.org/abs/2512.07309v1", "title": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals", "authors": ["Guosheng Wang", "Shen Wang", "Lei Yang"], "abstract": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.", "categories": ["cs.IT", "cs.AI"], "submitted_at": "2025-12-08T08:52:08Z", "updated_at": "2025-12-08T08:52:08Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.383722+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07306v1", "url": "https://arxiv.org/abs/2512.07306v1", "title": "Exact Synthetic Populations for Scalable Societal and Market Modeling", "authors": ["Thierry Petit", "Arnault Pachot"], "abstract": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.", "categories": ["stat.ML", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T08:48:21Z", "updated_at": "2025-12-08T08:48:21Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.385724+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07302v1", "url": "https://arxiv.org/abs/2512.07302v1", "title": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts", "authors": ["Mingning Guo", "Mengwei Wu", "Shaoxian Li", "Haifeng Li", "Chao Tao"], "abstract": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T08:44:57Z", "updated_at": "2025-12-08T08:44:57Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.388725+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07289v1", "url": "https://arxiv.org/abs/2512.07289v1", "title": "Equivariant Diffusion for Crystal Structure Prediction", "authors": ["Peijia Lin", "Pin Chen", "Rui Jiao", "Qing Mo", "Jianhuan Cen", "Wenbing Huang", "Yang Liu", "Dan Huang", "Yutong Lu"], "abstract": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "submitted_at": "2025-12-08T08:28:22Z", "updated_at": "2025-12-08T08:28:22Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.391243+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07287v1", "url": "https://arxiv.org/abs/2512.07287v1", "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents", "authors": ["Sijia Li", "Yuchen Huang", "Zifan Liu", "Zijian Li", "Jingjing fu", "Lei Song", "Jiang Bian", "Jun Zhang", "Rui Wang"], "abstract": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T08:27:24Z", "updated_at": "2025-12-08T08:27:24Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.395811+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06849v1", "url": "https://arxiv.org/abs/2512.06849v1", "title": "Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT", "authors": ["Matan Atad", "Alexander W. Marka", "Lisa Steinhelfer", "Anna Curto-Vilalta", "Yannik Leonhardt", "Sarah C. Foreman", "Anna-Sophia Walburga Dietrich", "Robert Graf", "Alexandra S. Gersing", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke", "Hendrik Möller"], "abstract": "Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-07T14:03:28Z", "updated_at": "2025-12-07T14:03:28Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.398817+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06837v1", "url": "https://arxiv.org/abs/2512.06837v1", "title": "Neural Factorization-based Bearing Fault Diagnosis", "authors": ["Zhenhao Li", "Xu Cheng", "Yi Zhou"], "abstract": "This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T13:19:39Z", "updated_at": "2025-12-07T13:19:39Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.400821+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06836v1", "url": "https://arxiv.org/abs/2512.06836v1", "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs", "authors": ["Weixing Zhang", "Regina Hebig", "Daniel Strüber"], "abstract": "Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.", "categories": ["cs.SE", "cs.AI", "cs.PL"], "submitted_at": "2025-12-07T13:17:37Z", "updated_at": "2025-12-07T13:17:37Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.404693+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06835v1", "url": "https://arxiv.org/abs/2512.06835v1", "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning", "authors": ["Tingyu Li", "Zheng Sun", "Jingxuan Wei", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "abstract": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.", "categories": ["cs.AI"], "submitted_at": "2025-12-07T13:17:31Z", "updated_at": "2025-12-07T13:17:31Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.406692+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06814v1", "url": "https://arxiv.org/abs/2512.06814v1", "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation", "authors": ["Dibyanayan Bandyopadhyay", "Soham Bhattacharjee", "Mohammed Hasanuzzaman", "Asif Ekbal"], "abstract": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-07T12:15:21Z", "updated_at": "2025-12-07T12:15:21Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.409691+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08188v1", "url": "https://arxiv.org/abs/2512.08188v1", "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model", "authors": ["Wenjiang Xu", "Cindy Wang", "Rui Fang", "Mingkang Zhang", "Lusong Li", "Jing Xu", "Jiayuan Gu", "Zecui Zeng", "Rui Chen"], "abstract": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .", "categories": ["cs.RO", "cs.AI", "cs.CV"], "submitted_at": "2025-12-09T02:36:26Z", "updated_at": "2025-12-09T02:36:26Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.412743+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08185v1", "url": "https://arxiv.org/abs/2512.08185v1", "title": "A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties", "authors": ["Jinghao Wang", "Ping Zhang", "Carter Yagemann"], "abstract": "Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-09T02:28:15Z", "updated_at": "2025-12-09T02:28:15Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.414743+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08179v1", "url": "https://arxiv.org/abs/2512.08179v1", "title": "Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces", "authors": ["Yating Zou", "Marcos Matabuena", "Michael R. Kosorok"], "abstract": "We study estimation of the conditional law $P(Y|X=\\mathbf{x})$ and continuous functionals $Ψ(P(Y|X=\\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \\in \\mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of Hájek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population.", "categories": ["stat.ME", "stat.ML"], "submitted_at": "2025-12-09T02:18:19Z", "updated_at": "2025-12-09T02:18:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.417763+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08176v1", "url": "https://arxiv.org/abs/2512.08176v1", "title": "Worst-case generation via minimax optimization in Wasserstein space", "authors": ["Xiuyuan Cheng", "Yao Xie", "Linglingzhi Zhu", "Yunqin Zhu"], "abstract": "Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.", "categories": ["stat.ML", "cs.LG", "math.OC"], "submitted_at": "2025-12-09T02:11:08Z", "updated_at": "2025-12-09T02:11:08Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.419746+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07279v1", "url": "https://arxiv.org/abs/2512.07279v1", "title": "Verifiable Deep Quantitative Group Testing", "authors": ["Shreyas Jayant Grampurohit", "Satish Mulleti", "Ajit Rajwade"], "abstract": "We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \\ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.", "categories": ["eess.SP", "cs.LG"], "submitted_at": "2025-12-08T08:18:57Z", "updated_at": "2025-12-08T08:18:57Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.425494+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07275v1", "url": "https://arxiv.org/abs/2512.07275v1", "title": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation", "authors": ["Siyu Wang", "Hua Wang", "Huiyu Li", "Fan Zhang"], "abstract": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T08:15:39Z", "updated_at": "2025-12-08T08:15:39Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.426504+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07269v1", "url": "https://arxiv.org/abs/2512.07269v1", "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data", "authors": ["Mike Diessner", "Yannick Tarant"], "abstract": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-08T08:08:38Z", "updated_at": "2025-12-08T08:08:38Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.429495+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07267v1", "url": "https://arxiv.org/abs/2512.07267v1", "title": "Non-negative DAG Learning from Time-Series Data", "authors": ["Samuel Rey", "Gonzalo Mateos"], "abstract": "This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.", "categories": ["eess.SP", "cs.LG"], "submitted_at": "2025-12-08T08:07:19Z", "updated_at": "2025-12-08T08:07:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.432034+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07266v1", "url": "https://arxiv.org/abs/2512.07266v1", "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "authors": ["Florian Tretter", "Daniel Flögel", "Alexandru Vasilache", "Max Grobbel", "Jürgen Becker", "Sören Hohmann"], "abstract": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "categories": ["cs.RO", "cs.AI", "eess.SY"], "submitted_at": "2025-12-08T08:06:40Z", "updated_at": "2025-12-08T08:06:40Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.437097+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06813v1", "url": "https://arxiv.org/abs/2512.06813v1", "title": "Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation", "authors": ["Agung Nugraha", "Heungjun Im", "Jihwan Lee"], "abstract": "High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T12:14:56Z", "updated_at": "2025-12-07T12:14:56Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.440098+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06811v1", "url": "https://arxiv.org/abs/2512.06811v1", "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models", "authors": ["Xiang Lin", "Weixin Li", "Shu Guo", "Lihong Wang", "Di Huang"], "abstract": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "submitted_at": "2025-12-07T12:04:46Z", "updated_at": "2025-12-07T12:04:46Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.443314+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06809v1", "url": "https://arxiv.org/abs/2512.06809v1", "title": "A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems", "authors": ["Jiong Yang"], "abstract": "Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from \"physical blindness\" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.", "categories": ["eess.SY", "cs.LG"], "submitted_at": "2025-12-07T11:58:09Z", "updated_at": "2025-12-07T11:58:09Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.446338+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06797v1", "url": "https://arxiv.org/abs/2512.06797v1", "title": "Optimal and Diffusion Transports in Machine Learning", "authors": ["Gabriel Peyré"], "abstract": "Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.", "categories": ["math.OC", "cs.AI", "cs.LG", "stat.ML"], "submitted_at": "2025-12-07T11:25:32Z", "updated_at": "2025-12-07T11:25:32Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.448326+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08173v1", "url": "https://arxiv.org/abs/2512.08173v1", "title": "Bayesian Semiparametric Mixture Cure (Frailty) Models", "authors": ["Fatih Kızılaslan", "Valeria Vitelli"], "abstract": "In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial.", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "submitted_at": "2025-12-09T02:05:07Z", "updated_at": "2025-12-09T02:05:07Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.451853+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08169v1", "url": "https://arxiv.org/abs/2512.08169v1", "title": "Information-Dense Reasoning for Efficient and Auditable Security Alert Triage", "authors": ["Guangze Zhao", "Yongzheng Zhang", "Changbo Tian", "Dan Xie", "Hongri Liu", "Bailing Wang"], "abstract": "Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-09T01:57:24Z", "updated_at": "2025-12-09T01:57:24Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.454429+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08160v1", "url": "https://arxiv.org/abs/2512.08160v1", "title": "LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks", "authors": ["Nanda K. Unnikrishnan", "Keshab K. Parhi"], "abstract": "In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.", "categories": ["cs.LG", "cs.AI", "cs.AR"], "submitted_at": "2025-12-09T01:35:08Z", "updated_at": "2025-12-09T01:35:08Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.456441+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08153v1", "url": "https://arxiv.org/abs/2512.08153v1", "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models", "authors": ["Zheng Ding", "Weirui Ye"], "abstract": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "submitted_at": "2025-12-09T01:17:34Z", "updated_at": "2025-12-09T01:17:34Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.459443+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08147v1", "url": "https://arxiv.org/abs/2512.08147v1", "title": "Scalable Back-End for an AI-Based Diabetes Prediction Application", "authors": ["Henry Anand Septian Radityo", "Bernardus Willson", "Reynard Tanadi", "Latifa Dwiyanti", "Saiful Akbar"], "abstract": "The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.", "categories": ["cs.AI", "cs.SE"], "submitted_at": "2025-12-09T00:59:20Z", "updated_at": "2025-12-09T00:59:20Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.464091+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07253v1", "url": "https://arxiv.org/abs/2512.07253v1", "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement", "authors": ["Handing Xu", "Zhenguo Nie", "Tairan Peng", "Huimin Pan", "Xin-Jun Liu"], "abstract": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T07:49:50Z", "updated_at": "2025-12-08T07:49:50Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.467090+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07249v1", "url": "https://arxiv.org/abs/2512.07249v1", "title": "IFFair: Influence Function-driven Sample Reweighting for Fair Classification", "authors": ["Jingran Yang", "Min Zhang", "Lingfeng Zhang", "Zhaohui Wang", "Yonggang Zhang"], "abstract": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T07:45:55Z", "updated_at": "2025-12-08T07:45:55Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.469097+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07247v1", "url": "https://arxiv.org/abs/2512.07247v1", "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "abstract": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "categories": ["cs.CV", "cs.CR", "cs.LG"], "submitted_at": "2025-12-08T07:41:23Z", "updated_at": "2025-12-08T07:41:23Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.471593+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07244v1", "url": "https://arxiv.org/abs/2512.07244v1", "title": "PINE: Pipeline for Important Node Exploration in Attributed Networks", "authors": ["Elizaveta Kovtun", "Maksim Makarenko", "Natalia Semenova", "Alexey Zaytsev", "Semen Budennyy"], "abstract": "A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T07:38:33Z", "updated_at": "2025-12-08T07:38:33Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.473601+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07234v1", "url": "https://arxiv.org/abs/2512.07234v1", "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models", "authors": ["Biao Chen", "Lin Zuo", "Mengmeng Jing", "Kunbin He", "Yuchen Wang"], "abstract": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T07:31:27Z", "updated_at": "2025-12-08T07:31:27Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.475601+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06795v1", "url": "https://arxiv.org/abs/2512.06795v1", "title": "ADAM Optimization with Adaptive Batch Selection", "authors": ["Gyu Yeol Kim", "Min-hwan Oh"], "abstract": "Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-07T11:15:14Z", "updated_at": "2025-12-07T11:15:14Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.479602+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06785v1", "url": "https://arxiv.org/abs/2512.06785v1", "title": "Angular Regularization for Positive-Unlabeled Learning on the Hypersphere", "authors": ["Vasileios Sevetlidis", "George Pavlidis", "Antonios Gasteratos"], "abstract": "Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T10:59:35Z", "updated_at": "2025-12-07T10:59:35Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.489356+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08145v1", "url": "https://arxiv.org/abs/2512.08145v1", "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models", "authors": ["Haoran Wang", "Zhuohang Chen", "Guang Li", "Bo Ma", "Chuanghuang Li"], "abstract": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-09T00:55:40Z", "updated_at": "2025-12-09T00:55:40Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.495953+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08143v1", "url": "https://arxiv.org/abs/2512.08143v1", "title": "PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection", "authors": ["Ali Lotfi Rezaabad", "Bikram Khanal", "Shashwat Chaurasia", "Lu Zeng", "Dezhi Hong", "Hossein Beshashati", "Thomas Butler", "Megan Ganji"], "abstract": "Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T00:39:18Z", "updated_at": "2025-12-09T00:39:18Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.500956+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08139v1", "url": "https://arxiv.org/abs/2512.08139v1", "title": "Robust Agents in Open-Ended Worlds", "authors": ["Mikayel Samvelyan"], "abstract": "The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T00:30:33Z", "updated_at": "2025-12-09T00:30:33Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 1.057, "collected_at": "2025-12-10T03:47:38.503439+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07232v1", "url": "https://arxiv.org/abs/2512.07232v1", "title": "Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model", "authors": ["Wenlong Liu", "Jiahua Pan", "Xingyu Zhang", "Xinxin Gong", "Yang Ye", "Xujin Zhao", "Xin Wang", "Kent Wu", "Hua Xiang", "Houmin Yan", "Qingpeng Zhang"], "abstract": "Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).", "categories": ["cs.AI"], "submitted_at": "2025-12-08T07:23:41Z", "updated_at": "2025-12-08T07:23:41Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.508235+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07228v1", "url": "https://arxiv.org/abs/2512.07228v1", "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping", "authors": ["Hengyang Yao", "Lin Li", "Ke Sun", "Jianing Qiu", "Huiping Chen"], "abstract": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "submitted_at": "2025-12-08T07:12:43Z", "updated_at": "2025-12-08T07:12:43Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.510235+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07224v1", "url": "https://arxiv.org/abs/2512.07224v1", "title": "Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics", "authors": ["Tianyi Ren", "Daniel Low", "Pittra Jaengprajak", "Juampablo Heras Rivera", "Jacob Ruzevick", "Mehmet Kurt"], "abstract": "Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician\" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \\textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.", "categories": ["eess.IV", "cs.CV", "cs.LG"], "submitted_at": "2025-12-08T07:06:58Z", "updated_at": "2025-12-08T07:06:58Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.513745+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06782v1", "url": "https://arxiv.org/abs/2512.06782v1", "title": "Measuring Over-smoothing beyond Dirichlet energy", "authors": ["Weiqi Guan", "Zihao Shi"], "abstract": "While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T10:53:22Z", "updated_at": "2025-12-07T10:53:22Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.515742+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06781v1", "url": "https://arxiv.org/abs/2512.06781v1", "title": "From Description to Score: Can LLMs Quantify Vulnerabilities?", "authors": ["Sima Jafarikhah", "Daniel Thompson", "Eva Deans", "Hossein Siadati", "Yi Liu"], "abstract": "Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \\textit{Availability Impact}), while offering more modest gains on others (e.g., \\textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.", "categories": ["cs.CR", "cs.AI", "cs.PL"], "submitted_at": "2025-12-07T10:47:00Z", "updated_at": "2025-12-07T10:47:00Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.521247+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06776v1", "url": "https://arxiv.org/abs/2512.06776v1", "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs", "authors": ["Yuchuan Tian", "Yuchen Liang", "Jiacheng Sun", "Shuo Zhang", "Guangwen Yang", "Yingte Shu", "Sibo Fang", "Tianyu Guo", "Kai Han", "Chao Xu", "Hanting Chen", "Xinghao Chen", "Yunhe Wang"], "abstract": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-07T10:28:21Z", "updated_at": "2025-12-07T10:28:21Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.523410+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06774v1", "url": "https://arxiv.org/abs/2512.06774v1", "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting", "authors": ["Longjie Zhao", "Ziming Hong", "Zhenyang Ren", "Runnan Chen", "Mingming Gong", "Tongliang Liu"], "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T10:26:35Z", "updated_at": "2025-12-07T10:26:35Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.527426+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08138v1", "url": "https://arxiv.org/abs/2512.08138v1", "title": "Robust equilibria in continuous games: From strategic to dynamic robustness", "authors": ["Kyriakos Lotidis", "Panayotis Mertikopoulos", "Nicholas Bambos", "Jose Blanchet"], "abstract": "In this paper, we examine the robustness of Nash equilibria in continuous games, under both strategic and dynamic uncertainty. Starting with the former, we introduce the notion of a robust equilibrium as those equilibria that remain invariant to small -- but otherwise arbitrary -- perturbations to the game's payoff structure, and we provide a crisp geometric characterization thereof. Subsequently, we turn to the question of dynamic robustness, and we examine which equilibria may arise as stable limit points of the dynamics of \"follow the regularized leader\" (FTRL) in the presence of randomness and uncertainty. Despite their very distinct origins, we establish a structural correspondence between these two notions of robustness: strategic robustness implies dynamic robustness, and, conversely, the requirement of strategic robustness cannot be relaxed if dynamic robustness is to be maintained. Finally, we examine the rate of convergence to robust equilibria as a function of the underlying regularizer, and we show that entropically regularized learning converges at a geometric rate in games with affinely constrained action spaces.", "categories": ["cs.GT", "cs.LG", "math.OC"], "submitted_at": "2025-12-09T00:30:23Z", "updated_at": "2025-12-09T00:30:23Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.530935+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08132v1", "url": "https://arxiv.org/abs/2512.08132v1", "title": "Multi-agent learning under uncertainty: Recurrence vs. concentration", "authors": ["Kyriakos Lotidis", "Panayotis Mertikopoulos", "Nicholas Bambos", "Jose Blanchet"], "abstract": "In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games -- one in continuous and one in discrete time with the aim of characterizing the long-run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone -- underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty.", "categories": ["cs.GT", "cs.LG", "math.OC"], "submitted_at": "2025-12-09T00:18:19Z", "updated_at": "2025-12-09T00:18:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.536357+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08130v1", "url": "https://arxiv.org/abs/2512.08130v1", "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture", "authors": ["Gary Ackerman", "Brandon Behlendorf", "Zachary Kallenborn", "Sheriff Almakki", "Doug Clifford", "Jenna LaTourette", "Hayley Peterson", "Noah Sheinbaum", "Olivia Shoemaker", "Anna Wetzel"], "abstract": "Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "submitted_at": "2025-12-09T00:16:44Z", "updated_at": "2025-12-09T00:16:44Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.538367+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07222v2", "url": "https://arxiv.org/abs/2512.07222v2", "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models", "authors": ["Qiwei Tian", "Chenhao Lin", "Zhengyu Zhao", "Chao Shen"], "abstract": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-08T07:05:18Z", "updated_at": "2025-12-09T12:48:18Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.543027+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07218v1", "url": "https://arxiv.org/abs/2512.07218v1", "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models", "authors": ["Feng Liang", "Weixin Zeng", "Runhao Zhao", "Xiang Zhao"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T06:58:23Z", "updated_at": "2025-12-08T06:58:23Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.548015+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07216v1", "url": "https://arxiv.org/abs/2512.07216v1", "title": "MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling", "authors": ["Bin Wu", "Feifan Yang", "Zhangming Chan", "Yu-Ran Gu", "Jiawei Feng", "Chao Yi", "Xiang-Rong Sheng", "Han Zhu", "Jian Xu", "Mang Ye", "Bo Zheng"], "abstract": "Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.", "categories": ["cs.IR", "cs.LG"], "submitted_at": "2025-12-08T06:55:13Z", "updated_at": "2025-12-08T06:55:13Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.551011+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06769v1", "url": "https://arxiv.org/abs/2512.06769v1", "title": "Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding", "authors": ["Hang Yin", "Xiaomin He", "PeiWen Yuan", "Yiwei Li", "Jiayi Shi", "Wenxiao Fan", "Shaoxiong Feng", "Kan Li"], "abstract": "Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\\text{MME}_{\\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T10:07:59Z", "updated_at": "2025-12-07T10:07:59Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.553438+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06759v1", "url": "https://arxiv.org/abs/2512.06759v1", "title": "VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors", "authors": ["Wenbo Lyu", "Yingjun Du", "Jinglin Zhao", "Xianton Zhen", "Ling Shao"], "abstract": "Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T09:48:10Z", "updated_at": "2025-12-07T09:48:10Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.557710+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06758v1", "url": "https://arxiv.org/abs/2512.06758v1", "title": "Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship", "authors": ["Zilong Wang", "Shuai Li"], "abstract": "The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $Ω\\left( \\frac{N\\log(T)}{Δ^2} + \\frac{K\\log(T)}Δ \\right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\\geq N)$ is the number of arms, $Δ$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\\left( \\frac{K\\log(T)}{Δ^2} \\right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\\left( \\frac{N\\log(T)}{Δ^2} + \\frac{K\\log(T)}Δ \\right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T09:45:31Z", "updated_at": "2025-12-07T09:45:31Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.562745+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08129v1", "url": "https://arxiv.org/abs/2512.08129v1", "title": "Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization", "authors": ["Guangmingmei Yang", "David J. Miller", "George Kesidis"], "abstract": "Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.", "categories": ["cs.LG"], "submitted_at": "2025-12-09T00:14:29Z", "updated_at": "2025-12-09T00:14:29Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.564746+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08124v1", "url": "https://arxiv.org/abs/2512.08124v1", "title": "Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach", "authors": ["Zijiang Yang"], "abstract": "This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "submitted_at": "2025-12-09T00:08:39Z", "updated_at": "2025-12-09T00:08:39Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.568748+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08121v1", "url": "https://arxiv.org/abs/2512.08121v1", "title": "Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic", "authors": ["Stephane Collot", "Colin Fraser", "Justin Zhao", "William F. Shen", "Timon Willi", "Ilias Leontiadis"], "abstract": "Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-12-08T23:58:32Z", "updated_at": "2025-12-08T23:58:32Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.571270+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07215v2", "url": "https://arxiv.org/abs/2512.07215v2", "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation", "authors": ["Md Selim Sarowar", "Sungho Kim"], "abstract": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T06:54:16Z", "updated_at": "2025-12-09T06:40:52Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.578502+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07212v1", "url": "https://arxiv.org/abs/2512.07212v1", "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation", "authors": ["Zhaoyang Liu", "Mokai Pan", "Zhongyi Wang", "Kaizhen Zhu", "Haotao Lu", "Jingya Wang", "Ye Shi"], "abstract": "Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-08T06:47:32Z", "updated_at": "2025-12-08T06:47:32Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.582018+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07209v1", "url": "https://arxiv.org/abs/2512.07209v1", "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits", "authors": ["Masato Ishii", "Akio Hayakawa", "Takashi Shibuya", "Yuki Mitsufuji"], "abstract": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.", "categories": ["cs.MM", "cs.LG", "cs.SD"], "submitted_at": "2025-12-08T06:45:11Z", "updated_at": "2025-12-08T06:45:11Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.586432+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07208v1", "url": "https://arxiv.org/abs/2512.07208v1", "title": "Geometric Prior-Guided Federated Prompt Calibration", "authors": ["Fei Luo", "Ziwei Zhao", "Mingxuan Wang", "Duoyang Li", "Zhe Qian", "Jiayi Tuo", "Chenyue Zhou", "Yanbiao Ma"], "abstract": "Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T06:42:32Z", "updated_at": "2025-12-08T06:42:32Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.590940+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06752v1", "url": "https://arxiv.org/abs/2512.06752v1", "title": "Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets", "authors": ["Chang Liu", "Vivian Li", "Linus Leong", "Vladimir Radenkovic", "Pietro Liò", "Chaitanya K. Joshi"], "abstract": "Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T09:31:34Z", "updated_at": "2025-12-07T09:31:34Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.598365+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06751v1", "url": "https://arxiv.org/abs/2512.06751v1", "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators", "authors": ["Seungyeon Jwa", "Daechul Ahn", "Reokyoung Kim", "Dongyeop Kang", "Jonghyun Choi"], "abstract": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-12-07T09:28:39Z", "updated_at": "2025-12-07T09:28:39Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.603954+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08093v1", "url": "https://arxiv.org/abs/2512.08093v1", "title": "Training LLMs for Honesty via Confessions", "authors": ["Manas Joglekar", "Jeremy Chen", "Gabriel Wu", "Jason Yosinski", "Jasmine Wang", "Boaz Barak", "Amelia Glaese"], "abstract": "Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.\n  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the \"path of least resistance\" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.\n  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its \"main\" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T23:05:52Z", "updated_at": "2025-12-08T23:05:52Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.605957+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08091v1", "url": "https://arxiv.org/abs/2512.08091v1", "title": "Complexity of One-Dimensional ReLU DNNs", "authors": ["Jonathan Kogan", "Hayden Jananthan", "Jeremy Kepner"], "abstract": "We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\\sum_{i = 1}^L n_i + \\mathop{o}\\left(\\sum_{i = 1}^L{n_i}\\right) + 1$, where $n_\\ell$ denotes the number of neurons in the $\\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-08T23:01:41Z", "updated_at": "2025-12-08T23:01:41Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.607953+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08082v1", "url": "https://arxiv.org/abs/2512.08082v1", "title": "Short-Context Dominance: How Much Local Context Natural Language Actually Needs?", "authors": ["Vala Vakilian", "Zimeng Wang", "Ankit Singh Rawat", "Christos Thrampoulidis"], "abstract": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T22:25:00Z", "updated_at": "2025-12-08T22:25:00Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.613047+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08077v1", "url": "https://arxiv.org/abs/2512.08077v1", "title": "Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders", "authors": ["Jaron Cohen", "Alexander G. Hasson", "Sara Tanovic"], "abstract": "Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T22:20:01Z", "updated_at": "2025-12-08T22:20:01Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.617803+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07206v1", "url": "https://arxiv.org/abs/2512.07206v1", "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT", "authors": ["Boyang Pan", "Zeyu Zhang", "Hongyu Meng", "Bin Cui", "Yingying Zhang", "Wenli Hou", "Junhao Li", "Langdi Zhong", "Xiaoxiao Chen", "Xiaoyu Xu", "Changjin Zuo", "Chao Cheng", "Nan-Jie Gong"], "abstract": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-08T06:31:19Z", "updated_at": "2025-12-08T06:31:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.626946+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07201v1", "url": "https://arxiv.org/abs/2512.07201v1", "title": "Understanding Diffusion Models via Code Execution", "authors": ["Cheng Yu"], "abstract": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-08T06:25:07Z", "updated_at": "2025-12-08T06:25:07Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.638191+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06749v2", "url": "https://arxiv.org/abs/2512.06749v2", "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems", "authors": ["Ming Ma", "Jue Zhang", "Fangkai Yang", "Yu Kang", "Qingwei Lin", "Tianming Yang", "Saravan Rajmohan", "Dongmei Zhang"], "abstract": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.", "categories": ["cs.AI", "cs.SE"], "submitted_at": "2025-12-07T09:23:48Z", "updated_at": "2025-12-09T13:22:36Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.643718+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06747v1", "url": "https://arxiv.org/abs/2512.06747v1", "title": "PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance", "authors": ["Jifar Wakuma Ayana", "Huang Qiming"], "abstract": "Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-07T09:20:14Z", "updated_at": "2025-12-07T09:20:14Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.647719+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06746v1", "url": "https://arxiv.org/abs/2512.06746v1", "title": "Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection", "authors": ["Ruoxin Chen", "Jiahui Gao", "Kaiqing Lin", "Keyue Zhang", "Yandan Zhao", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "abstract": "Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T09:19:00Z", "updated_at": "2025-12-07T09:19:00Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.648715+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08071v1", "url": "https://arxiv.org/abs/2512.08071v1", "title": "CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification", "authors": ["Pingchuan Ma", "Chengshuai Zhao", "Bohan Jiang", "Saketh Vishnubhatla", "Ujun Jeong", "Alimohammad Beigi", "Adrienne Raglin", "Huan Liu"], "abstract": "Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T22:12:27Z", "updated_at": "2025-12-08T22:12:27Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.652799+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08063v1", "url": "https://arxiv.org/abs/2512.08063v1", "title": "Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks", "authors": ["Xiaobin Shen", "George H. Chen"], "abstract": "We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T21:55:13Z", "updated_at": "2025-12-08T21:55:13Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.654802+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08061v1", "url": "https://arxiv.org/abs/2512.08061v1", "title": "LUNA: Linear Universal Neural Attention with Generalization Guarantees", "authors": ["Ashkan Shahbazi", "Ping He", "Ali Abbasi", "Yikun Bai", "Xinran Liu", "Elaheh Akbari", "Darian Salehi", "Navid NaderiAlizadeh", "Soheil Kolouri"], "abstract": "Scaling attention faces a critical bottleneck: the $\\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \\textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \\textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \\textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \\textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \\textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \\textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-08T21:49:55Z", "updated_at": "2025-12-08T21:49:55Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.659266+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07200v1", "url": "https://arxiv.org/abs/2512.07200v1", "title": "Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction", "authors": ["Zhen Huang", "Jiaxin Deng", "Jiayu Xu", "Junbiao Pang", "Haitao Yu"], "abstract": "In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T06:25:06Z", "updated_at": "2025-12-08T06:25:06Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.661783+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07195v1", "url": "https://arxiv.org/abs/2512.07195v1", "title": "MASim: Multilingual Agent-Based Simulation for Social Science", "authors": ["Xuan Zhang", "Wenxuan Zhang", "Anxu Wang", "See-Kiong Ng", "Yang Deng"], "abstract": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "submitted_at": "2025-12-08T06:12:48Z", "updated_at": "2025-12-08T06:12:48Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.664799+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07186v1", "url": "https://arxiv.org/abs/2512.07186v1", "title": "START: Spatial and Textual Learning for Chart Understanding", "authors": ["Zhuoming Liu", "Xiaofeng Gao", "Feiyang Niu", "Qiaozi Gao", "Liu Liu", "Robinson Piramuthu"], "abstract": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T05:43:14Z", "updated_at": "2025-12-08T05:43:14Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.667791+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07184v1", "url": "https://arxiv.org/abs/2512.07184v1", "title": "UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting", "authors": ["Da Zhang", "Bingyu Li", "Zhuyuan Zhao", "Junyu Gao", "Feiping Nie", "Xuelong Li"], "abstract": "As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T05:36:14Z", "updated_at": "2025-12-08T05:36:14Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.669797+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06737v1", "url": "https://arxiv.org/abs/2512.06737v1", "title": "Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics", "authors": ["Nikhil Verma", "Joonas Linnosmaa", "Espinosa-Leal Leonardo", "Napat Vajragupta"], "abstract": "The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NE"], "submitted_at": "2025-12-07T09:03:45Z", "updated_at": "2025-12-07T09:03:45Z", "rl_tags": ["exploration"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.672003+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06734v1", "url": "https://arxiv.org/abs/2512.06734v1", "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "abstract": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-07T08:59:15Z", "updated_at": "2025-12-07T08:59:15Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.674741+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06732v1", "url": "https://arxiv.org/abs/2512.06732v1", "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ", "authors": ["Aarushi Wagh", "Saniya Srivastava"], "abstract": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-07T08:57:27Z", "updated_at": "2025-12-07T08:57:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.676747+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06730v1", "url": "https://arxiv.org/abs/2512.06730v1", "title": "Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data", "authors": ["Lin Yang", "Xiang Li", "Xin Ma", "Xinxin Zhao"], "abstract": "Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-07T08:52:45Z", "updated_at": "2025-12-07T08:52:45Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.678739+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06727v1", "url": "https://arxiv.org/abs/2512.06727v1", "title": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models", "authors": ["Sourjya Roy", "Shrihari Sridharan", "Surya Selvam", "Anand Raghunathan"], "abstract": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T08:40:52Z", "updated_at": "2025-12-07T08:40:52Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.681263+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08057v1", "url": "https://arxiv.org/abs/2512.08057v1", "title": "Large Language Models for Education and Research: An Empirical and User Survey-based Analysis", "authors": ["Md Mostafizer Rahman", "Ariful Islam Shiplu", "Md Faizul Ibne Amin", "Yutaka Watanobe", "Lu Peng"], "abstract": "Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T21:35:28Z", "updated_at": "2025-12-08T21:35:28Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.682819+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08055v1", "url": "https://arxiv.org/abs/2512.08055v1", "title": "Fairness-aware PageRank via Edge Reweighting", "authors": ["Honglian Wang", "Haoyun Chen", "Aristides Gionis"], "abstract": "Link-analysis algorithms, such as PageRank, are instrumental in understanding the structural dynamics of networks by evaluating the importance of individual vertices based on their connectivity. Recently, with the rising importance of responsible AI, the question of fairness in link-analysis algorithms has gained traction. In this paper, we present a new approach for incorporating group fairness into the PageRank algorithm by reweighting the transition probabilities in the underlying transition matrix. We formulate the problem of achieving fair PageRank by seeking to minimize the fairness loss, which is the difference between the original group-wise PageRank distribution and a target PageRank distribution. We further define a group-adapted fairness notion, which accounts for group homophily by considering random walks with group-biased restart for each group. Since the fairness loss is non-convex, we propose an efficient projected gradient-descent method for computing locally-optimal edge weights. Unlike earlier approaches, we do not recommend adding new edges to the network, nor do we adjust the restart vector. Instead, we keep the topology of the underlying network unchanged and only modify the relative importance of existing edges. We empirically compare our approach with state-of-the-art baselines and demonstrate the efficacy of our method, where very small changes in the transition matrix lead to significant improvement in the fairness of the PageRank algorithm.", "categories": ["cs.SI", "cs.LG"], "submitted_at": "2025-12-08T21:27:03Z", "updated_at": "2025-12-08T21:27:03Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.685815+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08052v1", "url": "https://arxiv.org/abs/2512.08052v1", "title": "An Introduction to Deep Reinforcement and Imitation Learning", "authors": ["Pedro Santana"], "abstract": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-08T21:21:01Z", "updated_at": "2025-12-08T21:21:01Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.688820+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08036v1", "url": "https://arxiv.org/abs/2512.08036v1", "title": "Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration", "authors": ["Mohammadreza Jalaeian", "Dane A. Morey", "Michael F. Rayo"], "abstract": "Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.", "categories": ["cs.HC", "cs.AI", "eess.SY"], "submitted_at": "2025-12-08T20:53:57Z", "updated_at": "2025-12-08T20:53:57Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.690817+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07179v1", "url": "https://arxiv.org/abs/2512.07179v1", "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations", "authors": ["Wonbeen Lee", "Channyoung Lee", "Junho Sohn", "Hansam Cho"], "abstract": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "submitted_at": "2025-12-08T05:24:17Z", "updated_at": "2025-12-08T05:24:17Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.709683+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06726v1", "url": "https://arxiv.org/abs/2512.06726v1", "title": "The Role of Entropy in Visual Grounding: Analysis and Optimization", "authors": ["Shuo Li", "Jiajun Sun", "Zhihao Zhang", "Xiaoran Fan", "Senjie Jin", "Hui Li", "Yuming Yang", "Junjie Ye", "Lixing Shen", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "submitted_at": "2025-12-07T08:33:55Z", "updated_at": "2025-12-07T08:33:55Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 1.014, "collected_at": "2025-12-10T03:47:38.721232+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08026v1", "url": "https://arxiv.org/abs/2512.08026v1", "title": "Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching", "authors": ["Caroline N. Leach", "Mitchell A. Klusty", "Samuel E. Armstrong", "Justine C. Pickarski", "Kristen L. Hankins", "Emily B. Collier", "Maya Shah", "Aaron D. Mullen", "V. K. Cody Bumgardner"], "abstract": "Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T20:35:51Z", "updated_at": "2025-12-08T20:35:51Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.729793+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08022v1", "url": "https://arxiv.org/abs/2512.08022v1", "title": "Provable Diffusion Posterior Sampling for Bayesian Inversion", "authors": ["Jinyuan Chang", "Chenguang Duan", "Yuling Jiao", "Ruoxuan Li", "Jerry Zhijian Yang", "Cheng Yuan"], "abstract": "This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.", "categories": ["stat.ML", "cs.LG", "math.NA", "math.PR", "math.ST"], "submitted_at": "2025-12-08T20:34:05Z", "updated_at": "2025-12-08T20:34:05Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.752737+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07178v1", "url": "https://arxiv.org/abs/2512.07178v1", "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "abstract": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "categories": ["cs.AI", "cs.HC", "cs.LG"], "submitted_at": "2025-12-08T05:18:15Z", "updated_at": "2025-12-08T05:18:15Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.764934+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06725v1", "url": "https://arxiv.org/abs/2512.06725v1", "title": "Decoding Motor Behavior Using Deep Learning and Reservoir Computing", "authors": ["Tian Lan"], "abstract": "We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals", "categories": ["cs.LG", "eess.SP"], "submitted_at": "2025-12-07T08:29:43Z", "updated_at": "2025-12-07T08:29:43Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.786876+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08016v1", "url": "https://arxiv.org/abs/2512.08016v1", "title": "FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models", "authors": ["Jiyoon Pyo", "Yuankun Jiao", "Dongwon Jung", "Zekun Li", "Leeje Jang", "Sofia Kirsanova", "Jina Kim", "Yijun Lin", "Qin Liu", "Junyi Xie", "Hadi Askari", "Nan Xu", "Muhao Chen", "Yao-Yi Chiang"], "abstract": "Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T20:18:15Z", "updated_at": "2025-12-08T20:18:15Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.800907+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07175v1", "url": "https://arxiv.org/abs/2512.07175v1", "title": "SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models", "authors": ["Yibo Wang", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang", "Lijun Zhang"], "abstract": "Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T05:16:18Z", "updated_at": "2025-12-08T05:16:18Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.807171+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07173v1", "url": "https://arxiv.org/abs/2512.07173v1", "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration", "authors": ["Jucheng Shen", "Gaurav Sarkar", "Yeonju Ro", "Sharath Nittur Sridhar", "Zhangyang Wang", "Aditya Akella", "Souvik Kundu"], "abstract": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T05:15:41Z", "updated_at": "2025-12-08T05:15:41Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.815688+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06721v1", "url": "https://arxiv.org/abs/2512.06721v1", "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "submitted_at": "2025-12-07T08:21:07Z", "updated_at": "2025-12-07T08:21:07Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.829753+00:00"}
{"source": "arxiv", "arxiv_id": "2512.08013v1", "url": "https://arxiv.org/abs/2512.08013v1", "title": "Learning Dynamics from Infrequent Output Measurements for Uncertainty-Aware Optimal Control", "authors": ["Robert Lefringhausen", "Theodor Springer", "Sandra Hirche"], "abstract": "Reliable optimal control is challenging when the dynamics of a nonlinear system are unknown and only infrequent, noisy output measurements are available. This work addresses this setting of limited sensing by formulating a Bayesian prior over the continuous-time dynamics and latent state trajectory in state-space form and updating it through a targeted marginal Metropolis-Hastings sampler equipped with a numerical ODE integrator. The resulting posterior samples are used to formulate a scenario-based optimal control problem that accounts for both model and measurement uncertainty and is solved using standard nonlinear programming methods. The approach is validated in a numerical case study on glucose regulation using a Type 1 diabetes model.", "categories": ["eess.SY", "cs.LG", "math.OC"], "submitted_at": "2025-12-08T20:10:37Z", "updated_at": "2025-12-08T20:10:37Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.841298+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07997v1", "url": "https://arxiv.org/abs/2512.07997v1", "title": "A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm", "authors": ["Soroush Baghernezhad", "Elaheh Mohammadreza", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "abstract": "Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.", "categories": ["cs.HC", "cs.LG"], "submitted_at": "2025-12-08T19:36:10Z", "updated_at": "2025-12-08T19:36:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.848816+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07170v1", "url": "https://arxiv.org/abs/2512.07170v1", "title": "Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach", "authors": ["Jiayang Li", "Chengjie Jiang", "Junjun Jiang", "Pengwei Liang", "Jiayi Ma", "Liqiang Nie"], "abstract": "Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T05:04:54Z", "updated_at": "2025-12-08T05:04:54Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.853014+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07168v1", "url": "https://arxiv.org/abs/2512.07168v1", "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention", "authors": ["Georgios Ioannides", "Christos Constantinou", "Aman Chadha", "Aaron Elkins", "Linsey Pang", "Ravid Shwartz-Ziv", "Yann LeCun"], "abstract": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "submitted_at": "2025-12-08T05:01:51Z", "updated_at": "2025-12-08T05:01:51Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.855009+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07162v1", "url": "https://arxiv.org/abs/2512.07162v1", "title": "DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks", "authors": ["Kieran A. Malandain", "Selim Kalici", "Hakob Chakhoyan"], "abstract": "Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.", "categories": ["q-fin.CP", "cs.LG", "stat.ML"], "submitted_at": "2025-12-08T04:53:23Z", "updated_at": "2025-12-08T04:53:23Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.857116+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07150v1", "url": "https://arxiv.org/abs/2512.07150v1", "title": "FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers", "authors": ["Jonghyun Park", "Jong Chul Ye"], "abstract": "Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "submitted_at": "2025-12-08T04:18:13Z", "updated_at": "2025-12-08T04:18:13Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.860115+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06716v1", "url": "https://arxiv.org/abs/2512.06716v1", "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents", "authors": ["Zhibo Liang", "Tianze Hu", "Zaiye Chen", "Mingjie Tang"], "abstract": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.", "categories": ["cs.AI", "cs.CL", "cs.CR"], "submitted_at": "2025-12-07T08:11:19Z", "updated_at": "2025-12-07T08:11:19Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.864635+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06710v1", "url": "https://arxiv.org/abs/2512.06710v1", "title": "Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation", "authors": ["Zairah Mustahsan", "Abel Lim", "Megna Anand", "Saahil Jain", "Bryan McCann"], "abstract": "As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.", "categories": ["cs.AI"], "submitted_at": "2025-12-07T07:58:13Z", "updated_at": "2025-12-07T07:58:13Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.873186+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07993v1", "url": "https://arxiv.org/abs/2512.07993v1", "title": "SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models", "authors": ["Jiayi Tian", "Seyedarmin Azizi", "Yequan Zhao", "Erfan Baghaei Potraghloo", "Sean McPherson", "Sharath Nittur Sridhar", "Zhengyang Wang", "Zheng Zhang", "Massoud Pedram", "Souvik Kundu"], "abstract": "Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \\textbf{SkipKV}, a \\textbf{\\textit{training-free}} KV compression method for selective \\textit{eviction} and \\textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \\textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\\mathbf{26.7}\\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\\mathbf{1.6}\\times$ fewer generation length while improving throughput up to $\\mathbf{1.7}\\times$.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T19:32:06Z", "updated_at": "2025-12-08T19:32:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.875702+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07992v1", "url": "https://arxiv.org/abs/2512.07992v1", "title": "Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis", "authors": ["Aaron D. Mullen", "Daniel R. Harris", "Svetla Slavova", "V. K. Cody Bumgardner"], "abstract": "Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.", "categories": ["cs.LG", "cs.SE"], "submitted_at": "2025-12-08T19:24:50Z", "updated_at": "2025-12-08T19:24:50Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.878705+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07990v1", "url": "https://arxiv.org/abs/2512.07990v1", "title": "A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering", "authors": ["Thanh Nguyen", "Chaima Boufaied", "Ronnie de Souza Santos"], "abstract": "Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.", "categories": ["cs.SE", "cs.AI"], "submitted_at": "2025-12-08T19:22:01Z", "updated_at": "2025-12-08T19:22:01Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.881745+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07988v1", "url": "https://arxiv.org/abs/2512.07988v1", "title": "HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability", "authors": ["Sudhanva Manjunath Athreya", "Paul Rosen"], "abstract": "Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.", "categories": ["cs.LG", "cs.GR", "cs.HC"], "submitted_at": "2025-12-08T19:20:05Z", "updated_at": "2025-12-08T19:20:05Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.883754+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07142v1", "url": "https://arxiv.org/abs/2512.07142v1", "title": "Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search", "authors": ["Tanay Arora", "Christof Teuscher"], "abstract": "The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "submitted_at": "2025-12-08T03:48:51Z", "updated_at": "2025-12-08T03:48:51Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.886751+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07136v1", "url": "https://arxiv.org/abs/2512.07136v1", "title": "A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning", "authors": ["Siyang Jiang", "Mu Yuan", "Xiang Ji", "Bufang Yang", "Zeyu Liu", "Lilin Xu", "Yang Li", "Yuting He", "Liran Dong", "Wenrui Lu", "Zhenyu Yan", "Xiaofan Jiang", "Wei Gao", "Hongkai Chen", "Guoliang Xing"], "abstract": "Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T03:40:52Z", "updated_at": "2025-12-08T03:40:52Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.891803+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07135v2", "url": "https://arxiv.org/abs/2512.07135v2", "title": "TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning", "authors": ["Zebin Xing", "Pengxuan Yang", "Linbo Wang", "Yichen Zhang", "Yiming Hu", "Yupeng Zheng", "Junli Wang", "Yinfeng Gao", "Guang Li", "Kun Ma", "Long Chen", "Zhongpu Xia", "Qichao Zhang", "Hangjun Ye", "Dongbin Zhao"], "abstract": "Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T03:40:10Z", "updated_at": "2025-12-09T07:17:33Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.895813+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06708v1", "url": "https://arxiv.org/abs/2512.06708v1", "title": "A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations", "authors": ["Waleed Razzaq", "Yun-Bo Zhao"], "abstract": "Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T07:38:36Z", "updated_at": "2025-12-07T07:38:36Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.897812+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06705v1", "url": "https://arxiv.org/abs/2512.06705v1", "title": "Academic journals' AI policies fail to curb the surge in AI-assisted academic writing", "authors": ["Yongyuan He", "Yi Bu"], "abstract": "The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.", "categories": ["cs.AI"], "submitted_at": "2025-12-07T07:30:53Z", "updated_at": "2025-12-07T07:30:53Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.899816+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06702v1", "url": "https://arxiv.org/abs/2512.06702v1", "title": "Pathway to $O(\\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models", "authors": ["Xiangjun Meng", "Zhongjian Wang"], "abstract": "We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.\n  These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T07:26:39Z", "updated_at": "2025-12-07T07:26:39Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.903033+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06699v1", "url": "https://arxiv.org/abs/2512.06699v1", "title": "Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization", "authors": ["Karthik Prabhakar"], "abstract": "Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project", "categories": ["cs.PF", "cs.AI", "cs.LG"], "submitted_at": "2025-12-07T07:25:08Z", "updated_at": "2025-12-07T07:25:08Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.906033+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07984v1", "url": "https://arxiv.org/abs/2512.07984v1", "title": "Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection", "authors": ["Ryan Banks", "Camila Lindoni Azevedo", "Hongying Tang", "Yunpeng Li"], "abstract": "Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T19:15:08Z", "updated_at": "2025-12-08T19:15:08Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.910047+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07983v1", "url": "https://arxiv.org/abs/2512.07983v1", "title": "An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face", "authors": ["Nan Jia", "Anita Raja", "Raffi Khatchadourian"], "abstract": "As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.", "categories": ["cs.SE", "cs.AI"], "submitted_at": "2025-12-08T19:14:21Z", "updated_at": "2025-12-08T19:14:21Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.912231+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07981v1", "url": "https://arxiv.org/abs/2512.07981v1", "title": "CIP-Net: Continual Interpretable Prototype-based Network", "authors": ["Federico Di Valerio", "Michela Proietti", "Alessio Ragno", "Roberto Capobianco"], "abstract": "Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-08T19:13:19Z", "updated_at": "2025-12-08T19:13:19Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.915817+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07961v1", "url": "https://arxiv.org/abs/2512.07961v1", "title": "Towards symbolic regression for interpretable clinical decision scores", "authors": ["Guilherme Seidyo Imai Aldeia", "Joseph D. Romano", "Fabricio Olivetti de Franca", "Daniel S. Herman", "William G. La Cava"], "abstract": "Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.", "categories": ["cs.LG", "cs.NE"], "submitted_at": "2025-12-08T19:00:41Z", "updated_at": "2025-12-08T19:00:41Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.917818+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07132v1", "url": "https://arxiv.org/abs/2512.07132v1", "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "abstract": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "submitted_at": "2025-12-08T03:33:38Z", "updated_at": "2025-12-08T03:33:38Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.918817+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07122v1", "url": "https://arxiv.org/abs/2512.07122v1", "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations", "authors": ["Liping Han", "Tingting Nie", "Le Yu", "Mingzhe Hu", "Tao Yue"], "abstract": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.", "categories": ["cs.SE", "cs.AI"], "submitted_at": "2025-12-08T03:05:27Z", "updated_at": "2025-12-08T03:05:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.921833+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07120v1", "url": "https://arxiv.org/abs/2512.07120v1", "title": "Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications", "authors": ["J. Allagan", "G. Morgan", "S. Langley", "R. Lopez-Bonilla", "V. Deriglazov"], "abstract": "We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.", "categories": ["cs.DS", "cs.LG"], "submitted_at": "2025-12-08T03:01:50Z", "updated_at": "2025-12-08T03:01:50Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.924846+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07113v1", "url": "https://arxiv.org/abs/2512.07113v1", "title": "PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes", "authors": ["Kepeng Lin", "Qizhe Zhang", "Rui Wang", "Xuehai Hu", "Wei Xu"], "abstract": "Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE", "categories": ["cs.LG", "q-bio.GN"], "submitted_at": "2025-12-08T02:51:46Z", "updated_at": "2025-12-08T02:51:46Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.926848+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06695v1", "url": "https://arxiv.org/abs/2512.06695v1", "title": "Mitigating Barren plateaus in quantum denoising diffusion probabilistic models", "authors": ["Haipeng Cao", "Kaining Zhang", "Dacheng Tao", "Zhaofeng Su"], "abstract": "Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.", "categories": ["cs.LG", "quant-ph"], "submitted_at": "2025-12-07T07:01:44Z", "updated_at": "2025-12-07T07:01:44Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.930840+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06692v1", "url": "https://arxiv.org/abs/2512.06692v1", "title": "State Diversity Matters in Offline Behavior Distillation", "authors": ["Shiye Lei", "Zhihao Cheng", "Dacheng Tao"], "abstract": "Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T06:55:08Z", "updated_at": "2025-12-07T06:55:08Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.932254+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06681v1", "url": "https://arxiv.org/abs/2512.06681v1", "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis", "authors": ["Amartya Hatua"], "abstract": "We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-07T06:36:35Z", "updated_at": "2025-12-07T06:36:35Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.937279+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06678v1", "url": "https://arxiv.org/abs/2512.06678v1", "title": "GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning", "authors": ["Shrihari Sridharan", "Deepak Ravikumar", "Anand Raghunathan", "Kaushik Roy"], "abstract": "Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T06:35:04Z", "updated_at": "2025-12-07T06:35:04Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.939278+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07946v1", "url": "https://arxiv.org/abs/2512.07946v1", "title": "Conformal Defects in Neural Network Field Theories", "authors": ["Pietro Capuozzo", "Brandon Robinson", "Benjamin Suzzoni"], "abstract": "Neural Network Field Theories (NN-FTs) represent a novel construction of arbitrary field theories, including those of conformal fields, through the specification of the network architecture and prior distribution for the network parameters. In this work, we present a formalism for the construction of conformally invariant defects in these NN-FTs. We demonstrate this new formalism in two toy models of NN scalar field theories. We develop an NN interpretation of an expansion akin to the defect OPE in two-point correlation functions in these models.", "categories": ["hep-th", "cs.LG"], "submitted_at": "2025-12-08T19:00:03Z", "updated_at": "2025-12-08T19:00:03Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.946804+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07833v1", "url": "https://arxiv.org/abs/2512.07833v1", "title": "Relational Visual Similarity", "authors": ["Thao Nguyen", "Sicheng Mo", "Krishna Kumar Singh", "Yilin Wang", "Jing Shi", "Nicholas Kolkin", "Eli Shechtman", "Yong Jae Lee", "Yuheng Li"], "abstract": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T18:59:56Z", "updated_at": "2025-12-08T18:59:56Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:38.952448+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07112v1", "url": "https://arxiv.org/abs/2512.07112v1", "title": "FOAM: Blocked State Folding for Memory-Efficient LLM Training", "authors": ["Ziqing Wen", "Jiahuan Wang", "Ping Luo", "Dongsheng Li", "Tao Sun"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\\%, eliminates up to 90\\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T02:48:27Z", "updated_at": "2025-12-08T02:48:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.954966+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07109v1", "url": "https://arxiv.org/abs/2512.07109v1", "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy", "authors": ["Miguel Ingram", "Arthur Joseph Merritt"], "abstract": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,", "categories": ["cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-08T02:46:00Z", "updated_at": "2025-12-08T02:46:00Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.959973+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07100v1", "url": "https://arxiv.org/abs/2512.07100v1", "title": "Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph", "authors": ["Hong Wang", "Yinglong Zhang", "Hanhan Guo", "Xuewen Xia", "Xing Xu"], "abstract": "Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.\n  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.\n  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T02:31:42Z", "updated_at": "2025-12-08T02:31:42Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:38.962030+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06676v1", "url": "https://arxiv.org/abs/2512.06676v1", "title": "FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Bingyang Cheng", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "abstract": "Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-12-07T06:23:59Z", "updated_at": "2025-12-07T06:23:59Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.003636+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07832v1", "url": "https://arxiv.org/abs/2512.07832v1", "title": "Do Generalisation Results Generalise?", "authors": ["Matteo Boglioni", "Andrea Sgobbi", "Gabriel Tavernini", "Francesco Rita", "Marius Mosbach", "Tiago Pimentel"], "abstract": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.", "categories": ["cs.CL", "cs.LG"], "submitted_at": "2025-12-08T18:59:51Z", "updated_at": "2025-12-08T18:59:51Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.009638+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07926v1", "url": "https://arxiv.org/abs/2512.07926v1", "title": "Can AI autonomously build, operate, and use the entire data stack?", "authors": ["Arvind Agarwal", "Lisa Amini", "Sameep Mehta", "Horst Samulowitz", "Kavitha Srinivas"], "abstract": "Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.", "categories": ["cs.AI", "cs.DB", "cs.LG"], "submitted_at": "2025-12-08T18:59:01Z", "updated_at": "2025-12-08T18:59:01Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.015428+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07829v1", "url": "https://arxiv.org/abs/2512.07829v1", "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation", "authors": ["Yuan Gao", "Chen Chen", "Tianrong Chen", "Jiatao Gu"], "abstract": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T18:57:26Z", "updated_at": "2025-12-08T18:57:26Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.019420+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07094v2", "url": "https://arxiv.org/abs/2512.07094v2", "title": "VIGIL: A Reflective Runtime for Self-Healing Agents", "authors": ["Christopher Cruz"], "abstract": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T02:18:41Z", "updated_at": "2025-12-09T05:33:35Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.023487+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07092v1", "url": "https://arxiv.org/abs/2512.07092v1", "title": "The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models", "authors": ["Zhixiang Wang"], "abstract": "Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an \"alignment tax\" -- degrading general reasoning capabilities.\n  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.\n  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for \"Zero-Shot Personality Injection\" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.\n  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-08T02:00:57Z", "updated_at": "2025-12-08T02:00:57Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.028332+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07090v1", "url": "https://arxiv.org/abs/2512.07090v1", "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs", "authors": ["Jungmin Lee", "Gwangeun Byeon", "Yulhwa Kim", "Seokin Hong"], "abstract": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-08T01:56:27Z", "updated_at": "2025-12-08T01:56:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.030857+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07086v1", "url": "https://arxiv.org/abs/2512.07086v1", "title": "ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking", "authors": ["Yunzhe Li", "Jianan Wang", "Hongzi Zhu", "James Lin", "Shan Chang", "Minyi Guo"], "abstract": "Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "submitted_at": "2025-12-08T01:41:57Z", "updated_at": "2025-12-08T01:41:57Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.036395+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06666v1", "url": "https://arxiv.org/abs/2512.06666v1", "title": "The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification", "authors": ["Urav Maniar"], "abstract": "Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T05:37:40Z", "updated_at": "2025-12-07T05:37:40Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.038388+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06665v1", "url": "https://arxiv.org/abs/2512.06665v1", "title": "Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods", "authors": ["Panagiota Kiourti", "Anu Singh", "Preeti Duraipandian", "Weichao Zhou", "Wenchao Li"], "abstract": "This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "submitted_at": "2025-12-07T05:29:38Z", "updated_at": "2025-12-07T05:29:38Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.044456+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06660v1", "url": "https://arxiv.org/abs/2512.06660v1", "title": "Towards Small Language Models for Security Query Generation in SOC Workflows", "authors": ["Saleha Muzammil", "Rahul Reddy", "Vishal Kamalakrishnan", "Hadi Ahmadi", "Wajih Ul Hassan"], "abstract": "Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-07T05:18:27Z", "updated_at": "2025-12-07T05:18:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.050964+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07828v1", "url": "https://arxiv.org/abs/2512.07828v1", "title": "The Adoption and Usage of AI Agents: Early Evidence from Perplexity", "authors": ["Jeremy Yang", "Noah Yonack", "Kate Zyskowski", "Denis Yarats", "Johnny Ho", "Jerry Ma"], "abstract": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.", "categories": ["cs.LG", "econ.GN"], "submitted_at": "2025-12-08T18:56:10Z", "updated_at": "2025-12-08T18:56:10Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.056440+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07925v1", "url": "https://arxiv.org/abs/2512.07925v1", "title": "Near-real time fires detection using satellite imagery in Sudan conflict", "authors": ["Kuldip Singh Atwal", "Dieter Pfoser", "Daniel Rothbart"], "abstract": "The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T18:55:34Z", "updated_at": "2025-12-08T18:55:34Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.058438+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07827v1", "url": "https://arxiv.org/abs/2512.07827v1", "title": "An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning", "authors": ["Lukas Johannes Möller"], "abstract": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.", "categories": ["cs.CR", "cs.DC", "cs.LG"], "submitted_at": "2025-12-08T18:55:26Z", "updated_at": "2025-12-08T18:55:26Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.063109+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07082v1", "url": "https://arxiv.org/abs/2512.07082v1", "title": "TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization", "authors": ["Yuan-Ting Zhong", "Ting Huang", "Xiaolin Xiao", "Yue-Jiao Gong"], "abstract": "Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.", "categories": ["cs.LG"], "submitted_at": "2025-12-08T01:33:16Z", "updated_at": "2025-12-08T01:33:16Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.065097+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07081v1", "url": "https://arxiv.org/abs/2512.07081v1", "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes", "authors": ["Rongjia Zhou", "Chengzhuo Li", "Carl Yang", "Jiaying Lu"], "abstract": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.", "categories": ["cs.AI"], "submitted_at": "2025-12-08T01:32:14Z", "updated_at": "2025-12-08T01:32:14Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.070114+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06657v1", "url": "https://arxiv.org/abs/2512.06657v1", "title": "TextMamba: Scene Text Detector with Mamba", "authors": ["Qiyan Zhao", "Yue Yan", "Da-Han Wang"], "abstract": "In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\\%, 89.2\\%, and 78.5\\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T05:06:19Z", "updated_at": "2025-12-07T05:06:19Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.079615+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06655v1", "url": "https://arxiv.org/abs/2512.06655v1", "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering", "authors": ["Jehyeok Yeon", "Federico Cinus", "Yifan Wu", "Luca Luceri"], "abstract": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T04:46:30Z", "updated_at": "2025-12-07T04:46:30Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.085484+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06653v2", "url": "https://arxiv.org/abs/2512.06653v2", "title": "LightSearcher: Efficient DeepSearch via Experiential Memory", "authors": ["Hengzhi Lan", "Yue Yu", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Ting Bai"], "abstract": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.", "categories": ["cs.AI"], "submitted_at": "2025-12-07T04:29:52Z", "updated_at": "2025-12-09T02:58:14Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.092022+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07821v1", "url": "https://arxiv.org/abs/2512.07821v1", "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "authors": ["Shaoheng Fang", "Hanwen Jiang", "Yunpeng Bai", "Niloy J. Mitra", "Qixing Huang"], "abstract": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T18:54:12Z", "updated_at": "2025-12-08T18:54:12Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-12-10T03:47:39.095547+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07078v1", "url": "https://arxiv.org/abs/2512.07078v1", "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection", "authors": ["Bo Gao", "Jingcheng Tong", "Xingsheng Chen", "Han Yu", "Zichen Li"], "abstract": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-08T01:25:10Z", "updated_at": "2025-12-08T01:25:10Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.106305+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07074v1", "url": "https://arxiv.org/abs/2512.07074v1", "title": "Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters", "authors": ["Huanbiao Zhu", "Krish Desai", "Mikael Kuusela", "Vinicius Mikuni", "Benjamin Nachman", "Larry Wasserman"], "abstract": "Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \\textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \\textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \\textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \\textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider.", "categories": ["stat.AP", "hep-ex", "hep-ph", "physics.data-an", "stat.ML"], "submitted_at": "2025-12-08T01:21:34Z", "updated_at": "2025-12-08T01:21:34Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.110826+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06652v1", "url": "https://arxiv.org/abs/2512.06652v1", "title": "Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts", "authors": ["Xiaolei Lu", "Shamim Nemati"], "abstract": "Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T04:27:40Z", "updated_at": "2025-12-07T04:27:40Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.112955+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06648v1", "url": "https://arxiv.org/abs/2512.06648v1", "title": "Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network", "authors": ["Xiao Li"], "abstract": "Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.\n  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.\n  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "submitted_at": "2025-12-07T04:14:16Z", "updated_at": "2025-12-07T04:14:16Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.126902+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07064v1", "url": "https://arxiv.org/abs/2512.07064v1", "title": "Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design", "authors": ["Jiannan Yang", "Veronika Thost", "Tengfei Ma"], "abstract": "Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "submitted_at": "2025-12-08T00:52:46Z", "updated_at": "2025-12-08T00:52:46Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.129901+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07062v1", "url": "https://arxiv.org/abs/2512.07062v1", "title": "$\\mathrm{D}^{\\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction", "authors": ["Changliang Xia", "Chengyou Jia", "Minnan Luo", "Zhuohang Dang", "Xin Shen", "Bowen Ping"], "abstract": "Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^{\\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^{\\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^{\\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-08T00:39:32Z", "updated_at": "2025-12-08T00:39:32Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.132349+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07907v1", "url": "https://arxiv.org/abs/2512.07907v1", "title": "Harmonizing Community Science Datasets to Model Highly Pathogenic Avian Influenza (HPAI) in Birds in the Subantarctic", "authors": ["Richard Littauer", "Kris Bubendorfer"], "abstract": "Community science observational datasets are useful in epidemiology and ecology for modeling species distributions, but the heterogeneous nature of the data presents significant challenges for standardization, data quality assurance and control, and workflow management. In this paper, we present a data workflow for cleaning and harmonizing multiple community science datasets, which we implement in a case study using eBird, iNaturalist, GBIF, and other datasets to model the impact of highly pathogenic avian influenza in populations of birds in the subantarctic. We predict population sizes for several species where the demographics are not known, and we present novel estimates for potential mortality rates from HPAI for those species, based on a novel aggregated dataset of mortality rates in the subantarctic.", "categories": ["q-bio.PE", "cs.AI"], "submitted_at": "2025-12-08T00:36:09Z", "updated_at": "2025-12-08T00:36:09Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.138362+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06642v1", "url": "https://arxiv.org/abs/2512.06642v1", "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution", "authors": ["Achmad Ardani Prasha", "Clavino Ourizqi Rachmadi", "Muhamad Fauzan Ibnu Syahlan", "Naufal Rahfi Anugerah", "Nanda Garin Raditya", "Putri Amelia", "Sabrina Laila Mutiara", "Hilman Syachr Ramadhan"], "abstract": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.", "categories": ["cs.CV", "astro-ph.CO", "astro-ph.IM", "cs.AI", "cs.LG"], "submitted_at": "2025-12-07T03:25:19Z", "updated_at": "2025-12-07T03:25:19Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.141886+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06639v1", "url": "https://arxiv.org/abs/2512.06639v1", "title": "Learning to Hedge Swaptions", "authors": ["Zaniar Ahmadi", "Frédéric Godin"], "abstract": "This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.", "categories": ["q-fin.RM", "cs.LG"], "submitted_at": "2025-12-07T03:00:52Z", "updated_at": "2025-12-07T03:00:52Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.145403+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06638v1", "url": "https://arxiv.org/abs/2512.06638v1", "title": "The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News", "authors": ["Isha Karn", "David Jensen"], "abstract": "Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T03:00:38Z", "updated_at": "2025-12-07T03:00:38Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.148410+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07040v1", "url": "https://arxiv.org/abs/2512.07040v1", "title": "Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis", "authors": ["Sakib Mostafa", "Lei Xing", "Md. Tauhidul Islam"], "abstract": "Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-07T23:17:18Z", "updated_at": "2025-12-07T23:17:18Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.159553+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06630v1", "url": "https://arxiv.org/abs/2512.06630v1", "title": "Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "Rong Fu", "Qiuzhe Xie", "Fan Zhang"], "abstract": "Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.", "categories": ["cs.LG", "quant-ph"], "submitted_at": "2025-12-07T02:34:24Z", "updated_at": "2025-12-07T02:34:24Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.167153+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06629v1", "url": "https://arxiv.org/abs/2512.06629v1", "title": "FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection", "authors": ["Xiao-li Xia", "Hou-biao Li"], "abstract": "Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.", "categories": ["cs.AI", "cs.IT"], "submitted_at": "2025-12-07T02:32:10Z", "updated_at": "2025-12-07T02:32:10Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.169159+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06616v1", "url": "https://arxiv.org/abs/2512.06616v1", "title": "Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age", "authors": ["Rasam Dorri", "Rami Zwick"], "abstract": "As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.", "categories": ["cs.HC", "cs.AI"], "submitted_at": "2025-12-07T01:34:19Z", "updated_at": "2025-12-07T01:34:19Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.172685+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07038v1", "url": "https://arxiv.org/abs/2512.07038v1", "title": "Ideal Attribution and Faithful Watermarks for Language Models", "authors": ["Min Jae Song", "Kameron Shahabi"], "abstract": "We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.", "categories": ["cs.CR", "cs.LG", "stat.ML"], "submitted_at": "2025-12-07T23:05:20Z", "updated_at": "2025-12-07T23:05:20Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.175683+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07037v2", "url": "https://arxiv.org/abs/2512.07037v2", "title": "Evaluating and Preserving High-level Fidelity in Super-Resolution", "authors": ["Josep M. Rocafort", "Shaolin Su", "Alexandra Gomez-Villa", "Javier Vazquez-Corral"], "abstract": "Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-07T22:53:34Z", "updated_at": "2025-12-09T10:10:47Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.178691+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07034v1", "url": "https://arxiv.org/abs/2512.07034v1", "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues", "authors": ["Tuan-Anh Vu", "Hai Nguyen-Truong", "Ziqiang Zheng", "Binh-Son Hua", "Qing Guo", "Ivor Tsang", "Sai-Kit Yeung"], "abstract": "Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-07T22:52:53Z", "updated_at": "2025-12-07T22:52:53Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.181754+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07030v1", "url": "https://arxiv.org/abs/2512.07030v1", "title": "A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data", "authors": ["Zahra Lotfi", "Mostafa Lotfi"], "abstract": "Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "submitted_at": "2025-12-07T22:42:37Z", "updated_at": "2025-12-07T22:42:37Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.183784+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06615v1", "url": "https://arxiv.org/abs/2512.06615v1", "title": "Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions", "authors": ["Kaichen Shen", "Wei Zhu"], "abstract": "We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-07T01:17:14Z", "updated_at": "2025-12-07T01:17:14Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.187785+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06609v1", "url": "https://arxiv.org/abs/2512.06609v1", "title": "Vector Quantization using Gaussian Variational Autoencoder", "authors": ["Tongda Xu", "Wendi Zheng", "Jiajun He", "Jose Miguel Hernandez-Lobato", "Yan Wang", "Ya-Qin Zhang", "Jie Tang"], "abstract": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-12-07T00:57:58Z", "updated_at": "2025-12-07T00:57:58Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.189782+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06607v1", "url": "https://arxiv.org/abs/2512.06607v1", "title": "A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs", "authors": ["Humzah Merchant", "Bradford Levy"], "abstract": "Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-12-07T00:51:31Z", "updated_at": "2025-12-07T00:51:31Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.191809+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06595v1", "url": "https://arxiv.org/abs/2512.06595v1", "title": "ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling", "authors": ["Joe Shymanski"], "abstract": "Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.", "categories": ["cs.MA", "cs.AI"], "submitted_at": "2025-12-06T23:32:11Z", "updated_at": "2025-12-06T23:32:11Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.195402+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07022v1", "url": "https://arxiv.org/abs/2512.07022v1", "title": "Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization", "authors": ["Genevieve Caumartin", "Glaucia Melo"], "abstract": "Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.", "categories": ["cs.SE", "cs.AI", "cs.IR"], "submitted_at": "2025-12-07T22:25:11Z", "updated_at": "2025-12-07T22:25:11Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.199403+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07021v1", "url": "https://arxiv.org/abs/2512.07021v1", "title": "Transferring Clinical Knowledge into ECGs Representation", "authors": ["Jose Geraldo Fernandes", "Luiz Facury de Souza", "Pedro Robles Dutenhefner", "Gisele L. Pappa", "Wagner Meira"], "abstract": "Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \\emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T22:19:24Z", "updated_at": "2025-12-07T22:19:24Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.203067+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07019v1", "url": "https://arxiv.org/abs/2512.07019v1", "title": "Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length", "authors": ["Zhiyu Xu", "Jia Liu", "Yixin Wang", "Yuqi Gu"], "abstract": "The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.", "categories": ["stat.ME", "cs.AI", "stat.AP", "stat.ML"], "submitted_at": "2025-12-07T22:06:51Z", "updated_at": "2025-12-07T22:06:51Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.207065+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06592v1", "url": "https://arxiv.org/abs/2512.06592v1", "title": "On fine-tuning Boltz-2 for protein-protein affinity prediction", "authors": ["James King", "Lewis Cornwall", "Andrei Cristian Nica", "James Day", "Aaron Sim", "Neil Dalchau", "Lilly Wollman", "Joshua Meyers"], "abstract": "Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.", "categories": ["cs.LG", "q-bio.BM"], "submitted_at": "2025-12-06T23:07:10Z", "updated_at": "2025-12-06T23:07:10Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.233710+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07015v1", "url": "https://arxiv.org/abs/2512.07015v1", "title": "FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations", "authors": ["Mayank Ravishankara"], "abstract": "Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to \"hallucinate with citations.\"\n  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing \"Self-Correction\" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates \"Kill Queries\"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this \"Anti-Context.\" Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time \"Red Team\" for factual generation.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "submitted_at": "2025-12-07T21:28:42Z", "updated_at": "2025-12-07T21:28:42Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.251347+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07011v1", "url": "https://arxiv.org/abs/2512.07011v1", "title": "Block Sparse Flash Attention", "authors": ["Daniel Ohayon", "Itay Lamprecht", "Itay Hubara", "Israel Cohen", "Daniel Soudry", "Noam Elata"], "abstract": "Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention", "categories": ["cs.LG", "cs.CL", "cs.PF"], "submitted_at": "2025-12-07T21:20:12Z", "updated_at": "2025-12-07T21:20:12Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.254866+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06591v1", "url": "https://arxiv.org/abs/2512.06591v1", "title": "Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability", "authors": ["Joe Shymanski", "Jacob Brue", "Sandip Sen"], "abstract": "Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.", "categories": ["cs.HC", "cs.AI"], "submitted_at": "2025-12-06T23:06:18Z", "updated_at": "2025-12-06T23:06:18Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.259293+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06590v1", "url": "https://arxiv.org/abs/2512.06590v1", "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.", "categories": ["cs.IR", "cs.AI", "cs.MA"], "submitted_at": "2025-12-06T23:04:49Z", "updated_at": "2025-12-06T23:04:49Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.265807+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06582v1", "url": "https://arxiv.org/abs/2512.06582v1", "title": "QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling", "authors": ["Isaac Kofi Nti"], "abstract": "Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "submitted_at": "2025-12-06T22:29:19Z", "updated_at": "2025-12-06T22:29:19Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.268807+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07010v1", "url": "https://arxiv.org/abs/2512.07010v1", "title": "Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation", "authors": ["Kevin Lee", "Pablo Millan Arias"], "abstract": "Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\\% and 95.06\\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T21:19:04Z", "updated_at": "2025-12-07T21:19:04Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.275158+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06575v1", "url": "https://arxiv.org/abs/2512.06575v1", "title": "Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules", "authors": ["Fariza Dahes"], "abstract": "This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.", "categories": ["eess.IV", "cs.CV", "cs.LG"], "submitted_at": "2025-12-06T21:36:05Z", "updated_at": "2025-12-06T21:36:05Z", "rl_tags": ["exploration"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.285687+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06573v1", "url": "https://arxiv.org/abs/2512.06573v1", "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion", "authors": ["Onur Bilgin", "Abdullah As Sami", "Sriram Sai Vujjini", "John Licato"], "abstract": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.", "categories": ["cs.AI", "cs.MA"], "submitted_at": "2025-12-06T21:31:51Z", "updated_at": "2025-12-06T21:31:51Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.288688+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07009v1", "url": "https://arxiv.org/abs/2512.07009v1", "title": "Optimizing video analytics inference pipelines: a case study", "authors": ["Saeid Ghafouri", "Yuming Ding", "Katerine Diaz Chito", "Jesús Martinez del Rincón", "Niamh O'Connell", "Hans Vandierendonck"], "abstract": "Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.", "categories": ["cs.DC", "cs.AI", "cs.LG"], "submitted_at": "2025-12-07T21:17:53Z", "updated_at": "2025-12-07T21:17:53Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.295774+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07005v1", "url": "https://arxiv.org/abs/2512.07005v1", "title": "Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition", "authors": ["Zihao Wang", "Ruibin Yuan", "Ziqi Geng", "Hengjia Li", "Xingwei Qu", "Xinyi Li", "Songye Chen", "Haoying Fu", "Roger B. Dannenberg", "Kejun Zhang"], "abstract": "Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.", "categories": ["cs.SD", "cs.AI"], "submitted_at": "2025-12-07T21:14:26Z", "updated_at": "2025-12-07T21:14:26Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.299780+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07000v1", "url": "https://arxiv.org/abs/2512.07000v1", "title": "Benchmarking Deep Neural Networks for Modern Recommendation Systems", "authors": ["Abderaouf Bahi", "Ibtissem Gasmi"], "abstract": "This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.", "categories": ["cs.IR", "cs.AI"], "submitted_at": "2025-12-07T21:06:24Z", "updated_at": "2025-12-07T21:06:24Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.306077+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06562v1", "url": "https://arxiv.org/abs/2512.06562v1", "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities", "authors": ["Dung Thuy Nguyen", "Quang Nguyen", "Preston K. Robinette", "Eli Jiang", "Taylor T. Johnson", "Kevin Leach"], "abstract": "Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-06T20:42:38Z", "updated_at": "2025-12-06T20:42:38Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.308079+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06556v1", "url": "https://arxiv.org/abs/2512.06556v1", "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks", "authors": ["Saeid Jamshidi", "Kawser Wazed Nafi", "Arghavan Moradi Dakhel", "Negar Shahabi", "Foutse Khomh", "Naser Ezzati-Jivan"], "abstract": "The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-06T20:07:58Z", "updated_at": "2025-12-06T20:07:58Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.310073+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06555v1", "url": "https://arxiv.org/abs/2512.06555v1", "title": "BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models", "authors": ["Arush Sachdeva", "Rajendraprasad Saravanan", "Gargi Sarkar", "Kavita Vemuri", "Sandeep Kumar Shukla"], "abstract": "Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.", "categories": ["cs.CR", "cs.AI", "cs.CY"], "submitted_at": "2025-12-06T19:59:24Z", "updated_at": "2025-12-06T19:59:24Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.317615+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06999v1", "url": "https://arxiv.org/abs/2512.06999v1", "title": "Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model", "authors": ["Zihao Wang", "Ruibin Yuan", "Ziqi Geng", "Hengjia Li", "Xingwei Qu", "Xinyi Li", "Songye Chen", "Haoying Fu", "Roger B. Dannenberg", "Kejun Zhang"], "abstract": "Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.", "categories": ["cs.SD", "cs.AI"], "submitted_at": "2025-12-07T21:06:16Z", "updated_at": "2025-12-07T21:06:16Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.319617+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06993v1", "url": "https://arxiv.org/abs/2512.06993v1", "title": "Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation", "authors": ["Ali Ebrahimpour-Boroojeny"], "abstract": "We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.\n  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.", "categories": ["cs.LG"], "submitted_at": "2025-12-07T20:57:25Z", "updated_at": "2025-12-07T20:57:25Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.322695+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06991v1", "url": "https://arxiv.org/abs/2512.06991v1", "title": "Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models", "authors": ["Jing Jie Tan", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum", "Anissa Mokraoui", "Shih-Yu Lo"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel \"Prompting-in-a-Series\" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \\textit{gpt4o} from OpenAI and \\textit{gemini} from Google, along with open-source models like \\textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-07T20:52:00Z", "updated_at": "2025-12-07T20:52:00Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.326698+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06553v1", "url": "https://arxiv.org/abs/2512.06553v1", "title": "A Latent Variable Framework for Scaling Laws in Large Language Models", "authors": ["Peiyao Cai", "Chengyu Cui", "Felipe Maia Polo", "Seamus Somerstep", "Leshem Choshen", "Mikhail Yurochkin", "Moulinath Banerjee", "Yuekai Sun", "Kean Ming Tan", "Gongjun Xu"], "abstract": "We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).", "categories": ["stat.AP", "cs.LG"], "submitted_at": "2025-12-06T19:49:31Z", "updated_at": "2025-12-06T19:49:31Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.329691+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06547v1", "url": "https://arxiv.org/abs/2512.06547v1", "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation", "authors": ["Xiaocan Li", "Shiliang Wu", "Zheng Shen"], "abstract": "Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md", "categories": ["cs.LG", "cs.AI", "cs.DC"], "submitted_at": "2025-12-06T19:37:39Z", "updated_at": "2025-12-06T19:37:39Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.334350+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06537v1", "url": "https://arxiv.org/abs/2512.06537v1", "title": "Approximate Multiplier Induced Error Propagation in Deep Neural Networks", "authors": ["A. M. H. H. Alahakoon", "Hassaan Saadat", "Darshana Jayasinghe", "Sri Parameswaran"], "abstract": "Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.", "categories": ["cs.AR", "cs.LG"], "submitted_at": "2025-12-06T19:05:17Z", "updated_at": "2025-12-06T19:05:17Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.336351+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06990v1", "url": "https://arxiv.org/abs/2512.06990v1", "title": "Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients", "authors": ["Krishna Arun", "Moinak Bhattachrya", "Paras Goel"], "abstract": "Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.", "categories": ["cs.AI", "cs.CV", "eess.IV"], "submitted_at": "2025-12-07T20:51:59Z", "updated_at": "2025-12-07T20:51:59Z", "rl_tags": ["deep_rl", "general_dl"], "attention_score": 1.014, "collected_at": "2025-12-10T03:47:39.341874+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06989v1", "url": "https://arxiv.org/abs/2512.06989v1", "title": "Flash Multi-Head Feed-Forward Network", "authors": ["Minshen Zhang", "Xiang Hu", "Jianguo Li", "Wei Wu", "Kewei Tu"], "abstract": "We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-12-07T20:50:20Z", "updated_at": "2025-12-07T20:50:20Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.346969+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06987v1", "url": "https://arxiv.org/abs/2512.06987v1", "title": "OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction", "authors": ["Emily Jin", "Andrei Cristian Nica", "Mikhail Galkin", "Jarrid Rector-Brooks", "Kin Long Kelvin Lee", "Santiago Miret", "Frances H. Arnold", "Michael Bronstein", "Avishek Joey Bose", "Alexander Tong", "Cheng-Hao Liu"], "abstract": "Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\\text{RMSD}_1<0.5$ Å and attains over 80\\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "submitted_at": "2025-12-07T20:46:30Z", "updated_at": "2025-12-07T20:46:30Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.348965+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06531v1", "url": "https://arxiv.org/abs/2512.06531v1", "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images", "authors": ["Sayan Das", "Arghadip Biswas"], "abstract": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-12-06T18:49:57Z", "updated_at": "2025-12-06T18:49:57Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.357355+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06530v1", "url": "https://arxiv.org/abs/2512.06530v1", "title": "On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization", "authors": ["Mohammed Wattad", "Tamir Shor", "Alex Bronstein"], "abstract": "Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-06T18:49:46Z", "updated_at": "2025-12-06T18:49:46Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.363196+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06982v1", "url": "https://arxiv.org/abs/2512.06982v1", "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding", "authors": ["Yu Yu", "Qian Xie", "Nairen Cao", "Li Jin"], "abstract": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.", "categories": ["cs.LG", "eess.SY"], "submitted_at": "2025-12-07T20:25:07Z", "updated_at": "2025-12-07T20:25:07Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.373355+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06981v1", "url": "https://arxiv.org/abs/2512.06981v1", "title": "Selective Masking based Self-Supervised Learning for Image Semantic Segmentation", "authors": ["Yuemin Wang", "Ian Stavness"], "abstract": "This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-07T20:21:26Z", "updated_at": "2025-12-07T20:21:26Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.377352+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06522v1", "url": "https://arxiv.org/abs/2512.06522v1", "title": "Hierarchical Clustering With Confidence", "authors": ["Di Wu", "Jacob Bien", "Snigdha Panigrahi"], "abstract": "Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.\n  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $α$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm.", "categories": ["stat.ME", "math.ST", "stat.ML"], "submitted_at": "2025-12-06T18:18:20Z", "updated_at": "2025-12-06T18:18:20Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.384415+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06521v1", "url": "https://arxiv.org/abs/2512.06521v1", "title": "ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images", "authors": ["Jens Dede", "Anna Förster"], "abstract": "The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.\n  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-06T18:17:53Z", "updated_at": "2025-12-06T18:17:53Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.390930+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06977v1", "url": "https://arxiv.org/abs/2512.06977v1", "title": "Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging", "authors": ["Laurentius Valdy", "Richard D. Paul", "Alessio Quercia", "Zhuo Cao", "Xuan Zhao", "Hanno Scharr", "Arya Bangun"], "abstract": "Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.", "categories": ["eess.IV", "cs.LG"], "submitted_at": "2025-12-07T20:07:12Z", "updated_at": "2025-12-07T20:07:12Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.394180+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06973v1", "url": "https://arxiv.org/abs/2512.06973v1", "title": "Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control", "authors": ["Shuo Liu", "Wenliang Liu", "Wei Xiao", "Calin A. Belta"], "abstract": "Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.", "categories": ["eess.SY", "cs.LG"], "submitted_at": "2025-12-07T19:52:27Z", "updated_at": "2025-12-07T19:52:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.399186+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06971v1", "url": "https://arxiv.org/abs/2512.06971v1", "title": "Prediction with Expert Advice under Local Differential Privacy", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "abstract": "We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \\textit{central} DP algorithm by 1.5-3$\\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "submitted_at": "2025-12-07T19:31:35Z", "updated_at": "2025-12-07T19:31:35Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.403704+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06520v1", "url": "https://arxiv.org/abs/2512.06520v1", "title": "Hierarchical geometric deep learning enables scalable analysis of molecular dynamics", "authors": ["Zihan Pengmei", "Spencer C. Guo", "Chatipat Lorpaiboon", "Aaron R. Dinner"], "abstract": "Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.", "categories": ["cs.LG", "cond-mat.stat-mech", "physics.comp-ph", "physics.data-an"], "submitted_at": "2025-12-06T18:17:24Z", "updated_at": "2025-12-06T18:17:24Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.406700+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06514v1", "url": "https://arxiv.org/abs/2512.06514v1", "title": "Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression", "authors": ["Jie Wu", "Bo Zhang", "Daoji Li", "Zemin Zheng"], "abstract": "Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application.", "categories": ["stat.ME", "stat.ML"], "submitted_at": "2025-12-06T17:59:39Z", "updated_at": "2025-12-06T17:59:39Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.411214+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06511v1", "url": "https://arxiv.org/abs/2512.06511v1", "title": "Diagnosis-based mortality prediction for intensive care unit patients via transfer learning", "authors": ["Mengqi Xu", "Subha Maity", "Joel Dubin"], "abstract": "In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.", "categories": ["cs.LG", "stat.AP"], "submitted_at": "2025-12-06T17:46:18Z", "updated_at": "2025-12-06T17:46:18Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.412759+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06969v1", "url": "https://arxiv.org/abs/2512.06969v1", "title": "Comparing BFGS and OGR for Second-Order Optimization", "authors": ["Adrian Przybysz", "Mikołaj Kołek", "Franciszek Sobota", "Jarek Duda"], "abstract": "Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T19:26:26Z", "updated_at": "2025-12-07T19:26:26Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.417764+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06963v1", "url": "https://arxiv.org/abs/2512.06963v1", "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators", "authors": ["Yichao Shen", "Fangyun Wei", "Zhiying Du", "Yaobo Liang", "Yan Lu", "Jiaolong Yang", "Nanning Zheng", "Baining Guo"], "abstract": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "submitted_at": "2025-12-07T18:57:15Z", "updated_at": "2025-12-07T18:57:15Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.419770+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06960v1", "url": "https://arxiv.org/abs/2512.06960v1", "title": "Learning Conditional Independence Differential Graphs From Time-Dependent Data", "authors": ["Jitendra K Tugnait"], "abstract": "Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.", "categories": ["stat.ML", "cs.LG", "eess.SP"], "submitted_at": "2025-12-07T18:45:04Z", "updated_at": "2025-12-07T18:45:04Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.424806+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06506v1", "url": "https://arxiv.org/abs/2512.06506v1", "title": "AI as \"Co-founder\": GenAI for Entrepreneurship", "authors": ["Junhui Jeff Cai", "Xian Gu", "Liugang Sheng", "Mengjia Xia", "Linda Zhao", "Wu Zhu"], "abstract": "This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.", "categories": ["econ.GN", "cs.AI", "stat.AP"], "submitted_at": "2025-12-06T17:36:36Z", "updated_at": "2025-12-06T17:36:36Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.438054+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06504v1", "url": "https://arxiv.org/abs/2512.06504v1", "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion", "authors": ["Andrii Lysyi", "Anatoliy Sachenko", "Pavlo Radiuk", "Mykola Lysyi", "Oleksandr Melnychenko", "Diana Zahorodnia"], "abstract": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.", "categories": ["cs.CV", "cs.AI", "cs.RO"], "submitted_at": "2025-12-06T17:28:22Z", "updated_at": "2025-12-06T17:28:22Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.440055+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06496v1", "url": "https://arxiv.org/abs/2512.06496v1", "title": "PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning", "authors": ["Stella Brown", "Nicolas Preisig", "Autumn Davis", "Brian Hutchinson", "Filip Jagodzinski"], "abstract": "Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.", "categories": ["q-bio.BM", "cs.AI", "cs.LG", "cs.NE", "q-bio.QM"], "submitted_at": "2025-12-06T16:57:56Z", "updated_at": "2025-12-06T16:57:56Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.444809+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06951v1", "url": "https://arxiv.org/abs/2512.06951v1", "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge", "authors": ["Ilia Larchenko", "Gleb Zarin", "Akash Karnatak"], "abstract": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "submitted_at": "2025-12-07T18:08:45Z", "updated_at": "2025-12-07T18:08:45Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.446814+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06950v1", "url": "https://arxiv.org/abs/2512.06950v1", "title": "PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios", "authors": ["Enrico Camporeale"], "abstract": "The challenge of \\textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.\n  We introduce \\textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \\emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \\textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \\emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.\n  We use a real-world space weather example, where PARIS reduces the training set by up to 75\\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.", "categories": ["stat.ML", "cs.LG", "physics.space-ph"], "submitted_at": "2025-12-07T18:05:20Z", "updated_at": "2025-12-07T18:05:20Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.448808+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06945v1", "url": "https://arxiv.org/abs/2512.06945v1", "title": "Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets", "authors": ["Nabil Alami", "Jad Zakharia", "Souhaib Ben Taieb"], "abstract": "Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-07T17:54:07Z", "updated_at": "2025-12-07T17:54:07Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.453944+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06490v1", "url": "https://arxiv.org/abs/2512.06490v1", "title": "Optimizing LLMs Using Quantization for Mobile Execution", "authors": ["Agatsya Yadav", "Renta Chintala Bhargavi"], "abstract": "Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T16:31:56Z", "updated_at": "2025-12-06T16:31:56Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.459129+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06483v1", "url": "https://arxiv.org/abs/2512.06483v1", "title": "Classifying German Language Proficiency Levels Using Large Language Models", "authors": ["Elias-Leander Ahlers", "Witold Brunsmann", "Malte Schilling"], "abstract": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-06T16:15:45Z", "updated_at": "2025-12-06T16:15:45Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.461631+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06944v1", "url": "https://arxiv.org/abs/2512.06944v1", "title": "A Unifying Human-Centered AI Fairness Framework", "authors": ["Munshi Mahbubur Rahman", "Shimei Pan", "James R. Foulds"], "abstract": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "submitted_at": "2025-12-07T17:52:38Z", "updated_at": "2025-12-07T17:52:38Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.469190+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06932v1", "url": "https://arxiv.org/abs/2512.06932v1", "title": "Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies", "authors": ["Salma Albelali", "Moataz Ahmed"], "abstract": "Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T17:21:27Z", "updated_at": "2025-12-07T17:21:27Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.474714+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06929v1", "url": "https://arxiv.org/abs/2512.06929v1", "title": "Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding", "authors": ["MinCheol Jeon"], "abstract": "Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-07T17:14:32Z", "updated_at": "2025-12-07T17:14:32Z", "rl_tags": ["general_dl"], "attention_score": 0.814, "collected_at": "2025-12-10T03:47:39.479713+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06458v1", "url": "https://arxiv.org/abs/2512.06458v1", "title": "Instance Dependent Testing of Samplers using Interval Conditioning", "authors": ["Rishiraj Bhattacharyya", "Sourav Chakraborty", "Yash Pote", "Uddalok Sarkar", "Sayantan Sen"], "abstract": "Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.\n  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.", "categories": ["cs.DS", "cs.AI"], "submitted_at": "2025-12-06T14:45:56Z", "updated_at": "2025-12-06T14:45:56Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.481779+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06457v1", "url": "https://arxiv.org/abs/2512.06457v1", "title": "BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination", "authors": ["Huizheng Wang", "Hongbin Wang", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "abstract": "Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.", "categories": ["cs.LG", "eess.SP"], "submitted_at": "2025-12-06T14:44:38Z", "updated_at": "2025-12-06T14:44:38Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.484426+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06443v1", "url": "https://arxiv.org/abs/2512.06443v1", "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices", "authors": ["Xiangyu Li", "Chengyu Yin", "Weijun Wang", "Jianyu Wei", "Ting Cao", "Yunxin Liu"], "abstract": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.", "categories": ["cs.DC", "cs.AI"], "submitted_at": "2025-12-06T14:14:01Z", "updated_at": "2025-12-06T14:14:01Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.493108+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06440v1", "url": "https://arxiv.org/abs/2512.06440v1", "title": "Neural expressiveness for beyond importance model compression", "authors": ["Angelos-Christos Maroudis", "Sotirios Xydis"], "abstract": "Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named \"Expressiveness\". Unlike existing pruning methods that rely on the inherent \"Importance\" of neurons' and filters' weights, ``Expressiveness\" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the \"When to Prune\" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a \"hybrid\" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T14:07:34Z", "updated_at": "2025-12-06T14:07:34Z", "rl_tags": ["exploration"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.499106+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06435v1", "url": "https://arxiv.org/abs/2512.06435v1", "title": "Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals", "authors": ["Mara Sherlin Talento", "Jordan Richards", "Raphael Huser", "Hernando Ombao"], "abstract": "We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-06T13:47:40Z", "updated_at": "2025-12-06T13:47:40Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.501610+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06434v1", "url": "https://arxiv.org/abs/2512.06434v1", "title": "Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening", "authors": ["Lucas R. Mareque", "Ricardo L. Armentano", "Leandro J. Cymberknop"], "abstract": "Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-06T13:44:59Z", "updated_at": "2025-12-06T13:44:59Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.503618+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06431v1", "url": "https://arxiv.org/abs/2512.06431v1", "title": "Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City", "authors": ["Mohamed Shamroukh", "Mohamed Alkhuzamy Aziz"], "abstract": "National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.", "categories": ["cs.AI", "cs.CY"], "submitted_at": "2025-12-06T13:36:57Z", "updated_at": "2025-12-06T13:36:57Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.508632+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06427v1", "url": "https://arxiv.org/abs/2512.06427v1", "title": "A new initialisation to Control Gradients in Sinusoidal Neural network", "authors": ["Andrea Combette", "Antoine Venaille", "Nelly Pustelnik"], "abstract": "Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \\texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \\texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \\texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T13:23:03Z", "updated_at": "2025-12-06T13:23:03Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.511143+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06426v1", "url": "https://arxiv.org/abs/2512.06426v1", "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition", "authors": ["Nzakiese Mbongo", "Kailash A. Hambarde", "Hugo Proença"], "abstract": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-06T13:12:07Z", "updated_at": "2025-12-06T13:12:07Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.515355+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06421v1", "url": "https://arxiv.org/abs/2512.06421v1", "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation", "authors": ["Gengze Zhou", "Chongjian Ge", "Hao Tan", "Feng Liu", "Yicong Hong"], "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-06T12:41:42Z", "updated_at": "2025-12-06T12:41:42Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.517354+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06406v1", "url": "https://arxiv.org/abs/2512.06406v1", "title": "UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems", "authors": ["Xianzong Wu", "Xiaohong Li", "Lili Quan", "Qiang Hu"], "abstract": "Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-06T11:45:50Z", "updated_at": "2025-12-06T11:45:50Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.528433+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06404v1", "url": "https://arxiv.org/abs/2512.06404v1", "title": "GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols", "authors": ["Mohammad Soleymanibrojeni", "Roland Aydin", "Diego Guedes-Sobrinho", "Alexandre C. Dias", "Maurício J. Piotrowski", "Wolfgang Wenzel", "Celso Ricardo Caldeira Rêgo"], "abstract": "Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.", "categories": ["cs.AI", "cond-mat.mtrl-sci", "physics.chem-ph"], "submitted_at": "2025-12-06T11:28:35Z", "updated_at": "2025-12-06T11:28:35Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.534048+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06396v1", "url": "https://arxiv.org/abs/2512.06396v1", "title": "AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity", "authors": ["Shovan Roy"], "abstract": "The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-06T10:59:21Z", "updated_at": "2025-12-06T10:59:21Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.539042+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06393v1", "url": "https://arxiv.org/abs/2512.06393v1", "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression", "authors": ["Qiming Bao", "Xiaoxuan Fu"], "abstract": "Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.\n  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "submitted_at": "2025-12-06T10:49:50Z", "updated_at": "2025-12-06T10:49:50Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.542410+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06390v1", "url": "https://arxiv.org/abs/2512.06390v1", "title": "Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses", "authors": ["Mehrab Hosain", "Sabbir Alom Shuvo", "Matthew Ogbe", "Md Shah Jalal Mazumder", "Yead Rahman", "Md Azizul Hakim", "Anukul Pandey"], "abstract": "The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI", "cs.PF"], "submitted_at": "2025-12-06T10:42:14Z", "updated_at": "2025-12-06T10:42:14Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.558469+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06380v1", "url": "https://arxiv.org/abs/2512.06380v1", "title": "Protecting Bystander Privacy via Selective Hearing in LALMs", "authors": ["Xiao Zhan", "Guangzhi Sun", "Jose Such", "Phil Woodland"], "abstract": "Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models.", "categories": ["cs.SD", "cs.AI"], "submitted_at": "2025-12-06T10:24:04Z", "updated_at": "2025-12-06T10:24:04Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.561981+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06370v1", "url": "https://arxiv.org/abs/2512.06370v1", "title": "Optimizing Optimizers for Fast Gradient-Based Learning", "authors": ["Jaerin Lee", "Kyoung Mu Lee"], "abstract": "We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-06T09:50:41Z", "updated_at": "2025-12-06T09:50:41Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.564315+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06357v1", "url": "https://arxiv.org/abs/2512.06357v1", "title": "Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction", "authors": ["Tony Sallooma", "Okyay Kaynak", "Xinbo Yub", "Wei He"], "abstract": "Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "submitted_at": "2025-12-06T09:08:05Z", "updated_at": "2025-12-06T09:08:05Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.566319+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06356v1", "url": "https://arxiv.org/abs/2512.06356v1", "title": "DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction", "authors": ["Yifan Song", "Fenglin Yu", "Yihong Luo", "Xingjian Tao", "Siya Qiu", "Kai Han", "Jing Tang"], "abstract": "Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.", "categories": ["cs.LG", "cs.SI"], "submitted_at": "2025-12-06T09:06:08Z", "updated_at": "2025-12-06T09:06:08Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.572962+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06350v1", "url": "https://arxiv.org/abs/2512.06350v1", "title": "Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast", "authors": ["Nghi Truong", "Phanish Puranam", "Özgecan Koçak"], "abstract": "The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the \"doomer\" and \"boomer\" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk (\"X-risk\") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks (\"E-risks\") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.", "categories": ["cs.CY", "cs.AI", "cs.CL"], "submitted_at": "2025-12-06T08:48:30Z", "updated_at": "2025-12-06T08:48:30Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.618994+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06348v1", "url": "https://arxiv.org/abs/2512.06348v1", "title": "Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders", "authors": ["Xiaoyu Ma", "Likun Zhang", "Christopher K. Wikle"], "abstract": "Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.", "categories": ["stat.ML", "cs.LG", "stat.ME"], "submitted_at": "2025-12-06T08:40:41Z", "updated_at": "2025-12-06T08:40:41Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.627184+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06347v1", "url": "https://arxiv.org/abs/2512.06347v1", "title": "Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry", "authors": ["Naoki Yoshida", "Isao Ishikawa", "Masaaki Imaizumi"], "abstract": "We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.", "categories": ["cs.LG", "stat.ML"], "submitted_at": "2025-12-06T08:40:28Z", "updated_at": "2025-12-06T08:40:28Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.638887+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06343v1", "url": "https://arxiv.org/abs/2512.06343v1", "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models", "authors": ["Tong Xie", "Andrew Bai", "Yuanhao Ban", "Yunqi Hong", "Haoyu Li", "Cho-jui Hsieh"], "abstract": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-12-06T08:15:37Z", "updated_at": "2025-12-06T08:15:37Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.648137+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06341v1", "url": "https://arxiv.org/abs/2512.06341v1", "title": "Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness", "authors": ["Ronald Katende"], "abstract": "Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.", "categories": ["cs.LG", "cs.IR", "cs.IT"], "submitted_at": "2025-12-06T08:11:22Z", "updated_at": "2025-12-06T08:11:22Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.655251+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06337v1", "url": "https://arxiv.org/abs/2512.06337v1", "title": "DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization", "authors": ["Xuan Xie", "Xuan Wang", "Wenjie Wang"], "abstract": "The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.", "categories": ["cs.AI"], "submitted_at": "2025-12-06T07:51:36Z", "updated_at": "2025-12-06T07:51:36Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.661842+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06306v1", "url": "https://arxiv.org/abs/2512.06306v1", "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation", "authors": ["Haoxian Zhou", "Chuanzhi Xu", "Langyi Chen", "Haodong Chen", "Yuk Ying Chung", "Qiang Qu", "Xaoming Chen", "Weidong Cai"], "abstract": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-06T05:32:13Z", "updated_at": "2025-12-06T05:32:13Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.667855+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06304v1", "url": "https://arxiv.org/abs/2512.06304v1", "title": "Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation", "authors": ["Xining Song", "Zhihua Wei", "Rui Wang", "Haixiao Hu", "Yanxiang Chen", "Meng Han"], "abstract": "Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.", "categories": ["eess.AS", "cs.AI", "cs.CR", "cs.SD"], "submitted_at": "2025-12-06T05:17:07Z", "updated_at": "2025-12-06T05:17:07Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.670856+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06303v1", "url": "https://arxiv.org/abs/2512.06303v1", "title": "Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization", "authors": ["Preksha Girish", "Rachana Mysore", "Kiran K. N.", "Hiranmayee R.", "Shipra Prashanth", "Shrey Kumar"], "abstract": "Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T05:11:49Z", "updated_at": "2025-12-06T05:11:49Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.674895+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06301v1", "url": "https://arxiv.org/abs/2512.06301v1", "title": "Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics", "authors": ["Jihun Ahn", "Gabriella Pasya Irianti", "Vikram Thapar", "Su-Mi Hur"], "abstract": "Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-06T05:07:11Z", "updated_at": "2025-12-06T05:07:11Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.679901+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06297v1", "url": "https://arxiv.org/abs/2512.06297v1", "title": "Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks", "authors": ["Luca Di Carlo", "Chase Goddard", "David J. Schwab"], "abstract": "Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "submitted_at": "2025-12-06T04:50:32Z", "updated_at": "2025-12-06T04:50:32Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.683013+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06296v1", "url": "https://arxiv.org/abs/2512.06296v1", "title": "How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion", "authors": ["Sooho Moon", "Yunyong Ko"], "abstract": "Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.", "categories": ["cs.AI"], "submitted_at": "2025-12-06T04:49:29Z", "updated_at": "2025-12-06T04:49:29Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.687802+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06294v1", "url": "https://arxiv.org/abs/2512.06294v1", "title": "Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability", "authors": ["Quentin Badolle", "Arthur Theuer", "Zhou Fang", "Ankit Gupta", "Mustafa Khammash"], "abstract": "Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.", "categories": ["q-bio.MN", "cs.LG", "math.PR", "q-bio.QM", "stat.ML"], "submitted_at": "2025-12-06T04:45:31Z", "updated_at": "2025-12-06T04:45:31Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.689810+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06293v1", "url": "https://arxiv.org/abs/2512.06293v1", "title": "Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media", "authors": ["Fatima Ashraf", "Muhammad Ayub Sabir", "Jiaxin Deng", "Junbiao Pang", "Haitao Yu"], "abstract": "Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \\emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git", "categories": ["cs.LG"], "submitted_at": "2025-12-06T04:45:17Z", "updated_at": "2025-12-06T04:45:17Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.696010+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06288v1", "url": "https://arxiv.org/abs/2512.06288v1", "title": "Theoretical Compression Bounds for Wide Multilayer Perceptrons", "authors": ["Houssam El Cheairi", "David Gamarnik", "Rahul Mazumder"], "abstract": "Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.", "categories": ["cs.LG", "math.ST"], "submitted_at": "2025-12-06T04:32:25Z", "updated_at": "2025-12-06T04:32:25Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.698084+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06281v1", "url": "https://arxiv.org/abs/2512.06281v1", "title": "Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models", "authors": ["Hengzhuang Li", "Xinsong Zhang", "Qiming Peng", "Bin Luo", "Han Hu", "Dengyang Jiang", "Han-Jia Ye", "Teng Zhang", "Hai Jin"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-06T04:20:13Z", "updated_at": "2025-12-06T04:20:13Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.701842+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06276v1", "url": "https://arxiv.org/abs/2512.06276v1", "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension", "authors": ["Tianyi Gao", "Hao Li", "Han Fang", "Xin Wei", "Xiaodong Dong", "Hongbo Sun", "Ye Yuan", "Zhongjiang He", "Jinglin Xu", "Jingmin Xin", "Hao Sun"], "abstract": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-06T03:59:21Z", "updated_at": "2025-12-06T03:59:21Z", "rl_tags": ["general_dl"], "attention_score": 0.771, "collected_at": "2025-12-10T03:47:39.703856+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06270v1", "url": "https://arxiv.org/abs/2512.06270v1", "title": "Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions", "authors": ["Nifei Lin", "Heng Luo", "L. Jeff Hong"], "abstract": "In this work, we study contextual strongly convex simulation optimization and adopt an \"optimize then predict\" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-06T03:47:29Z", "updated_at": "2025-12-06T03:47:29Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.712157+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06256v1", "url": "https://arxiv.org/abs/2512.06256v1", "title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup", "authors": ["Aniruddha Maiti", "Satya Nimmagadda", "Kartha Veerya Jammuladinne", "Niladri Sengupta", "Ananya Jana"], "abstract": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-06T03:00:24Z", "updated_at": "2025-12-06T03:00:24Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.723413+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06247v1", "url": "https://arxiv.org/abs/2512.06247v1", "title": "DUET: Agentic Design Understanding via Experimentation and Testing", "authors": ["Gus Henry Smith", "Sandesh Adhikary", "Vineet Thumuluri", "Karthik Suresh", "Vivek Pandit", "Kartik Hegde", "Hamid Shojaei", "Chandra Bhagavatula"], "abstract": "AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.", "categories": ["cs.SE", "cs.AI", "cs.AR"], "submitted_at": "2025-12-06T02:16:28Z", "updated_at": "2025-12-06T02:16:28Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.747171+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06243v1", "url": "https://arxiv.org/abs/2512.06243v1", "title": "Quantization Blindspots: How Model Compression Breaks Backdoor Defenses", "authors": ["Rohan Pandey", "Eric Ye"], "abstract": "Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.", "categories": ["cs.LG", "cs.CR"], "submitted_at": "2025-12-06T02:04:32Z", "updated_at": "2025-12-06T02:04:32Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.755704+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06240v1", "url": "https://arxiv.org/abs/2512.06240v1", "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems", "authors": ["Chuanhao Nie", "Yunbo Liu", "Chao Wang"], "abstract": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.", "categories": ["cs.AI"], "submitted_at": "2025-12-06T01:37:24Z", "updated_at": "2025-12-06T01:37:24Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.759702+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06236v1", "url": "https://arxiv.org/abs/2512.06236v1", "title": "Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph", "authors": ["Haiyang Yu", "Meng-Chieh Lee", "Xiang song", "Qi Zhu", "Christos Faloutsos"], "abstract": "We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.", "categories": ["cs.LG"], "submitted_at": "2025-12-06T01:23:11Z", "updated_at": "2025-12-06T01:23:11Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.763449+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06232v1", "url": "https://arxiv.org/abs/2512.06232v1", "title": "Opinion: Learning Intuitive Physics May Require More than Visual Data", "authors": ["Ellen Su", "Solim Legris", "Todd M. Gureckis", "Mengye Ren"], "abstract": "Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-06T00:49:41Z", "updated_at": "2025-12-06T00:49:41Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.765466+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06227v1", "url": "https://arxiv.org/abs/2512.06227v1", "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety", "authors": ["Junyu Mao", "Anthony Hills", "Talia Tseriotou", "Maria Liakata", "Aya Shamir", "Dan Sayda", "Dana Atzil-Slonim", "Natalie Djohari", "Arpan Mandal", "Silke Roth", "Pamela Ugwudike", "Mahesan Niranjan", "Stuart E. Middleton"], "abstract": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.", "categories": ["cs.CL", "cs.LG"], "submitted_at": "2025-12-06T00:21:29Z", "updated_at": "2025-12-06T00:21:29Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.769466+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06211v2", "url": "https://arxiv.org/abs/2512.06211v2", "title": "A Broader View on Clustering under Cluster-Aware Norm Objectives", "authors": ["Martin G. Herold", "Evangelos Kipouridis", "Joachim Spoerhase"], "abstract": "We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.\n  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\\widetilde{O}(\\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\\widetilde{O}(\\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.\n  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.", "categories": ["cs.DS", "cs.LG"], "submitted_at": "2025-12-05T23:14:10Z", "updated_at": "2025-12-09T10:18:53Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.783401+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06210v1", "url": "https://arxiv.org/abs/2512.06210v1", "title": "Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict", "authors": ["Daniel Mittermaier", "Tobias Bohne", "Martin Hofer", "Daniel Racek"], "abstract": "Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.", "categories": ["stat.AP", "cs.LG"], "submitted_at": "2025-12-05T23:10:16Z", "updated_at": "2025-12-05T23:10:16Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.785416+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06208v1", "url": "https://arxiv.org/abs/2512.06208v1", "title": "SparsePixels: Efficient Convolution for Sparse Data on FPGAs", "authors": ["Ho Fung Tsoi", "Dylan Rankin", "Vladimir Loncar", "Philip Harris"], "abstract": "Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.", "categories": ["cs.AR", "cs.LG", "hep-ex"], "submitted_at": "2025-12-05T23:04:44Z", "updated_at": "2025-12-05T23:04:44Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.793193+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06206v1", "url": "https://arxiv.org/abs/2512.06206v1", "title": "The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning", "authors": ["Akis Linardos", "Sarthak Pati", "Ujjwal Baid", "Brandon Edwards", "Patrick Foley", "Kevin Ta", "Verena Chung", "Micah Sheller", "Muhammad Irfan Khan", "Mojtaba Jafaritadi", "Elina Kontio", "Suleiman Khan", "Leon Mächler", "Ivan Ezhov", "Suprosanna Shit", "Johannes C. Paetzold", "Gustav Grimberg", "Manuel A. Nickel", "David Naccache", "Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni", "Daewoon Kim", "Leonard L. Klausmann", "Prashant Shah", "Bjoern Menze", "Dimitrios Makris", "Spyridon Bakas"], "abstract": "We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.", "categories": ["cs.CV", "cs.LG"], "submitted_at": "2025-12-05T22:59:57Z", "updated_at": "2025-12-05T22:59:57Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.798193+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06205v1", "url": "https://arxiv.org/abs/2512.06205v1", "title": "On measuring grounding and generalizing grounding problems", "authors": ["Daniel Quigley", "Eric Maynard"], "abstract": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-05T22:58:47Z", "updated_at": "2025-12-05T22:58:47Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.801195+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06201v1", "url": "https://arxiv.org/abs/2512.06201v1", "title": "K2-V2: A 360-Open, Reasoning-Enhanced LLM", "authors": [" K2 Team", "Zhengzhong Liu", "Liping Tang", "Linghao Jin", "Haonan Li", "Nikhil Ranjan", "Desai Fan", "Shaurya Rohatgi", "Richard Fan", "Omkar Pangarkar", "Huijuan Wang", "Zhoujun Cheng", "Suqi Sun", "Seungwook Han", "Bowen Tan", "Gurpreet Gosal", "Xudong Han", "Varad Pimpalkhute", "Shibo Hao", "Ming Shan Hee", "Joel Hestness", "Haolong Jia", "Liqun Ma", "Aaryamonvikram Singh", "Daria Soboleva", "Natalia Vassilieva", "Renxi Wang", "Yingquan Wu", "Yuekai Sun", "Taylor Killian", "Alexander Moreno", "John Maggs", "Hector Ren", "Guowei He", "Hongyi Wang", "Xuezhe Ma", "Yuqi Wang", "Mikhail Yurochkin", "Eric P. Xing"], "abstract": "We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.", "categories": ["cs.LG"], "submitted_at": "2025-12-05T22:53:45Z", "updated_at": "2025-12-05T22:53:45Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.811891+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06200v1", "url": "https://arxiv.org/abs/2512.06200v1", "title": "How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?", "authors": ["Tomohiro Yamashita", "Daichi Amagata", "Yusuke Matsui"], "abstract": "Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.", "categories": ["cs.LG"], "submitted_at": "2025-12-05T22:53:45Z", "updated_at": "2025-12-05T22:53:45Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.815176+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06196v1", "url": "https://arxiv.org/abs/2512.06196v1", "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "authors": ["Charlie Masters", "Marta Grześkiewicz", "Stefano V. Albrecht"], "abstract": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "categories": ["cs.AI", "cs.CL"], "submitted_at": "2025-12-05T22:39:54Z", "updated_at": "2025-12-05T22:39:54Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.820687+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06193v1", "url": "https://arxiv.org/abs/2512.06193v1", "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots", "authors": ["Jihyung Park", "Saleh Afroogh", "Junfeng Jiao"], "abstract": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-05T22:28:04Z", "updated_at": "2025-12-05T22:28:04Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.822800+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06190v1", "url": "https://arxiv.org/abs/2512.06190v1", "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying", "authors": ["Shichen Li", "Ahmadreza Eslaminia", "Chenhui Shao"], "abstract": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "submitted_at": "2025-12-05T22:17:25Z", "updated_at": "2025-12-05T22:17:25Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.825798+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06183v1", "url": "https://arxiv.org/abs/2512.06183v1", "title": "PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations", "authors": ["Lindong Liu", "Zhixiong Jin", "Seongjin Choi"], "abstract": "High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.", "categories": ["cs.LG"], "submitted_at": "2025-12-05T22:05:01Z", "updated_at": "2025-12-05T22:05:01Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.835503+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06181v1", "url": "https://arxiv.org/abs/2512.06181v1", "title": "Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data", "authors": ["Yanuo Zhou"], "abstract": "Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.\n  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.\n  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.\n  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.\n  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.", "categories": ["q-bio.QM", "cs.LG"], "submitted_at": "2025-12-05T22:02:02Z", "updated_at": "2025-12-05T22:02:02Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.841010+00:00"}
{"source": "arxiv", "arxiv_id": "2512.07901v1", "url": "https://arxiv.org/abs/2512.07901v1", "title": "The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators", "authors": ["Kevin Vallier"], "abstract": "This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation.", "categories": ["cs.GT", "cs.AI", "econ.TH"], "submitted_at": "2025-12-05T21:58:03Z", "updated_at": "2025-12-05T21:58:03Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.842541+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06172v1", "url": "https://arxiv.org/abs/2512.06172v1", "title": "DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification", "authors": ["Sheng Liu", "Panos Papadimitratos"], "abstract": "Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.", "categories": ["cs.CR", "cs.AI"], "submitted_at": "2025-12-05T21:50:27Z", "updated_at": "2025-12-05T21:50:27Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.847548+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06161v1", "url": "https://arxiv.org/abs/2512.06161v1", "title": "Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach", "authors": ["Gondy Leroy", "Prakash Bisht", "Sai Madhuri Kandula", "Nell Maltman", "Sydney Rice"], "abstract": "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-12-05T21:14:59Z", "updated_at": "2025-12-05T21:14:59Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.849548+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06154v1", "url": "https://arxiv.org/abs/2512.06154v1", "title": "Learning Invariant Graph Representations Through Redundant Information", "authors": ["Barproda Halder", "Pasan Dissanayake", "Sanghamitra Dutta"], "abstract": "Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.", "categories": ["cs.LG", "cs.AI", "cs.IT"], "submitted_at": "2025-12-05T21:07:11Z", "updated_at": "2025-12-05T21:07:11Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.854599+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06143v1", "url": "https://arxiv.org/abs/2512.06143v1", "title": "gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points", "authors": ["Marcus M. Noack", "Mark D. Risser", "Hengrui Luo", "Vardaan Tekriwal", "Ronald J. Pandolfi"], "abstract": "Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \\emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.", "categories": ["cs.LG", "math.PR"], "submitted_at": "2025-12-05T20:52:37Z", "updated_at": "2025-12-05T20:52:37Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.856596+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06134v1", "url": "https://arxiv.org/abs/2512.06134v1", "title": "Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting", "authors": ["Georgi Hrusanov", "Duy-Thanh Vu", "Duy-Cat Can", "Sophie Tascedda", "Margaret Ryan", "Julien Bodelet", "Katarzyna Koscielska", "Carsten Magnus", "Oliver Y. Chén"], "abstract": "Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "q-bio.QM"], "submitted_at": "2025-12-05T20:29:01Z", "updated_at": "2025-12-05T20:29:01Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.859596+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06123v1", "url": "https://arxiv.org/abs/2512.06123v1", "title": "Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples", "authors": ["Qilin Zhou", "Zhengyuan Wei", "Haipeng Wang", "Zhuo Wang", "W. K. Chan"], "abstract": "Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.", "categories": ["cs.SE", "cs.AI", "cs.CR"], "submitted_at": "2025-12-05T20:02:09Z", "updated_at": "2025-12-05T20:02:09Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.862658+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06113v1", "url": "https://arxiv.org/abs/2512.06113v1", "title": "Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures", "authors": ["Bin Xu", "Ayan Banerjee", "Sandeep Gupta"], "abstract": "Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.", "categories": ["cs.AR", "cs.LG"], "submitted_at": "2025-12-05T19:38:34Z", "updated_at": "2025-12-05T19:38:34Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.867654+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06112v1", "url": "https://arxiv.org/abs/2512.06112v1", "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "authors": ["Yifang Xu", "Jiahao Cui", "Feipeng Cai", "Zhihao Zhu", "Hanlin Shang", "Shan Luan", "Mingwang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "abstract": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-05T19:36:46Z", "updated_at": "2025-12-05T19:36:46Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.869654+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06111v1", "url": "https://arxiv.org/abs/2512.06111v1", "title": "A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts", "authors": ["Arthur Mukwaya", "Nancy Kasamala", "Nana Kankam Gyimah", "Judith Mwakalonge", "Gurcan Comert", "Saidi Siuhi", "Denis Ruganuza", "Mark Ngotonie"], "abstract": "The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.", "categories": ["cs.LG"], "submitted_at": "2025-12-05T19:36:11Z", "updated_at": "2025-12-05T19:36:11Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.871719+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06109v2", "url": "https://arxiv.org/abs/2512.06109v2", "title": "Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions", "authors": ["Ajinkya Bhole", "Mohammad Mahmoudi Filabadi", "Guillaume Crevecoeur", "Tom Lefebvre"], "abstract": "This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.", "categories": ["math.OC", "cs.LG", "cs.RO", "eess.SY"], "submitted_at": "2025-12-05T19:31:39Z", "updated_at": "2025-12-09T10:23:42Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.875560+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06107v1", "url": "https://arxiv.org/abs/2512.06107v1", "title": "From Tail Universality to Bernstein-von Mises: A Unified Statistical Theory of Semi-Implicit Variational Inference", "authors": ["Sean Plummer"], "abstract": "Semi-implicit variational inference (SIVI) constructs approximate posteriors of the form $q(θ) = \\int k(θ| z) r(dz)$, where the conditional kernel is parameterized and the mixing base is fixed and tractable. This paper develops a unified \"approximation-optimization-statistics'' theory for such families.\n  On the approximation side, we show that under compact L1-universality and a mild tail-dominance condition, semi-implicit families are dense in L1 and can achieve arbitrarily small forward Kullback-Leibler (KL) error. We also identify two sharp obstructions to global approximation: (i) an Orlicz tail-mismatch condition that induces a strictly positive forward-KL gap, and (ii) structural restrictions, such as non-autoregressive Gaussian kernels, that force \"branch collapse'' in conditional distributions. For each obstruction we give a minimal structural modification that restores approximability.\n  On the optimization side, we establish finite-sample oracle inequalities and prove that the empirical SIVI objectives L(K,n) $Γ$-converge to their population limit as n and K tend to infinity. These results give consistency of empirical maximizers, quantitative control of finite-K surrogate bias, and stability of the resulting variational posteriors.\n  Combining the approximation and optimization analyses yields the first general end-to-end statistical theory for SIVI: we characterize precisely when SIVI can recover the target distribution, when it cannot, and how architectural and algorithmic choices govern the attainable asymptotic behavior.", "categories": ["math.ST", "stat.ML"], "submitted_at": "2025-12-05T19:26:25Z", "updated_at": "2025-12-05T19:26:25Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.878550+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06106v1", "url": "https://arxiv.org/abs/2512.06106v1", "title": "Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity", "authors": ["Constanze Albrecht", "Chayapatr Archiwaranguprok", "Rachel Poonsiriwong", "Awu Chen", "Peggy Yin", "Monchai Lertsutthiwong", "Kavin Winson", "Hal Hershfield", "Pattie Maes", "Pat Pataranutaporn"], "abstract": "What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.", "categories": ["cs.HC", "cs.AI"], "submitted_at": "2025-12-05T19:24:18Z", "updated_at": "2025-12-05T19:24:18Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.883855+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06105v1", "url": "https://arxiv.org/abs/2512.06105v1", "title": "Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation", "authors": ["Junwen Zheng", "Xinran Xu", "Li Rong Wang", "Chang Cai", "Lucinda Siyun Tan", "Dingyuan Wang", "Hong Liang Tey", "Xiuyi Fan"], "abstract": "Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-05T19:19:36Z", "updated_at": "2025-12-05T19:19:36Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.885752+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06104v1", "url": "https://arxiv.org/abs/2512.06104v1", "title": "ARC-AGI Without Pretraining", "authors": ["Isaac Liao", "Albert Gu"], "abstract": "Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI \"training set\". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.", "categories": ["cs.LG"], "submitted_at": "2025-12-05T19:17:43Z", "updated_at": "2025-12-05T19:17:43Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.889828+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06097v1", "url": "https://arxiv.org/abs/2512.06097v1", "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue", "authors": ["Emre Umucu", "Guillermina Solis", "Leon Garza", "Emilia Rivas", "Beatrice Lee", "Anantaa Kotal", "Aritran Piplai"], "abstract": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design", "categories": ["cs.CL", "cs.AI"], "submitted_at": "2025-12-05T19:04:28Z", "updated_at": "2025-12-05T19:04:28Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.898958+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05967v1", "url": "https://arxiv.org/abs/2512.05967v1", "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms", "authors": ["Francesco Granata", "Francesco Poggi", "Misael Mongiovì"], "abstract": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-12-05T18:59:18Z", "updated_at": "2025-12-05T18:59:18Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.905047+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05964v2", "url": "https://arxiv.org/abs/2512.05964v2", "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking", "authors": ["Kevin Black", "Allen Z. Ren", "Michael Equi", "Sergey Levine"], "abstract": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.", "categories": ["cs.RO", "cs.AI"], "submitted_at": "2025-12-05T18:57:28Z", "updated_at": "2025-12-09T01:07:28Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.909995+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06065v1", "url": "https://arxiv.org/abs/2512.06065v1", "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "authors": ["Runjia Li", "Moayed Haji-Ali", "Ashkan Mirzaei", "Chaoyang Wang", "Arpit Sahni", "Ivan Skorokhodov", "Aliaksandr Siarohin", "Tomas Jakab", "Junlin Han", "Sergey Tulyakov", "Philip Torr", "Willi Menapace"], "abstract": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-05T18:57:05Z", "updated_at": "2025-12-05T18:57:05Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.912524+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05962v1", "url": "https://arxiv.org/abs/2512.05962v1", "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity", "authors": ["Germán Kruszewski", "Pierre Erbacher", "Jos Rozen", "Marc Dymetman"], "abstract": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-05T18:56:40Z", "updated_at": "2025-12-05T18:56:40Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.918199+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05960v1", "url": "https://arxiv.org/abs/2512.05960v1", "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement", "authors": ["Munsif Ali", "Najmul Hassan", "Lucia Ventura", "Davide Di Bari", "Simonepietro Canese"], "abstract": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-12-05T18:56:10Z", "updated_at": "2025-12-05T18:56:10Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.922866+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05959v1", "url": "https://arxiv.org/abs/2512.05959v1", "title": "M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG", "authors": ["David Anugraha", "Patrick Amadeus Irawan", "Anshul Singh", "En-Shiun Annie Lee", "Genta Indra Winata"], "abstract": "Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "submitted_at": "2025-12-05T18:55:58Z", "updated_at": "2025-12-05T18:55:58Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.925864+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05958v1", "url": "https://arxiv.org/abs/2512.05958v1", "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution", "authors": ["Sara Patel", "Mingxun Zhou", "Giulia Fanti"], "abstract": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-05T18:54:21Z", "updated_at": "2025-12-05T18:54:21Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.930864+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05957v1", "url": "https://arxiv.org/abs/2512.05957v1", "title": "Consequences of Kernel Regularity for Bandit Optimization", "authors": ["Madison Lee", "Tara Javidi"], "abstract": "In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Matérn, square-exponential, rational-quadratic, $γ$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish Hölder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.", "categories": ["stat.ML", "cs.LG"], "submitted_at": "2025-12-05T18:54:09Z", "updated_at": "2025-12-05T18:54:09Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.937304+00:00"}
{"source": "arxiv", "arxiv_id": "2512.06062v1", "url": "https://arxiv.org/abs/2512.06062v1", "title": "When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models", "authors": ["S. M. Mustaqim", "Anantaa Kotal", "Paul H. Yi"], "abstract": "Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-12-05T18:52:35Z", "updated_at": "2025-12-05T18:52:35Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.939370+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05954v1", "url": "https://arxiv.org/abs/2512.05954v1", "title": "SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "abstract": "We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems", "categories": ["cs.AI"], "submitted_at": "2025-12-05T18:50:48Z", "updated_at": "2025-12-05T18:50:48Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.944135+00:00"}
{"source": "arxiv", "arxiv_id": "2512.05951v1", "url": "https://arxiv.org/abs/2512.05951v1", "title": "Trusted AI Agents in the Cloud", "authors": ["Teofil Bodea", "Masanori Misono", "Julian Pritzi", "Patrick Sabanic", "Thore Sommer", "Harshavardhan Unnibhavi", "David Schall", "Nuno Santos", "Dimitrios Stavrakakis", "Pramod Bhatotia"], "abstract": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.", "categories": ["cs.CR", "cs.AI", "cs.MA"], "submitted_at": "2025-12-05T18:48:53Z", "updated_at": "2025-12-05T18:48:53Z", "rl_tags": ["general_dl"], "attention_score": 0.729, "collected_at": "2025-12-10T03:47:39.946139+00:00"}
