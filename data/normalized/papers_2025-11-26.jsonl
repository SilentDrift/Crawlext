{"source": "arxiv", "arxiv_id": "2511.20347v1", "url": "https://arxiv.org/abs/2511.20347v1", "title": "Soft Adaptive Policy Optimization", "authors": ["Chang Gao", "Chujie Zheng", "Xiong-Hui Chen", "Kai Dang", "Shixuan Liu", "Bowen Yu", "An Yang", "Shuai Bai", "Jingren Zhou", "Junyang Lin"], "abstract": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-11-25T14:25:19Z", "updated_at": "2025-11-25T14:25:19Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.324029+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20591v1", "url": "https://arxiv.org/abs/2511.20591v1", "title": "Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning", "authors": ["Charlotte Beylier", "Hannah Selder", "Arthur Fleig", "Simon M. Hofmann", "Nico Scherf"], "abstract": "The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.", "categories": ["cs.LG"], "submitted_at": "2025-11-25T18:20:42Z", "updated_at": "2025-11-25T18:20:42Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-11-26T18:13:51.342044+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19808v1", "url": "https://arxiv.org/abs/2511.19808v1", "title": "Learning to Clean: Reinforcement Learning for Noisy Label Correction", "authors": ["Marzi Heidari", "Hanping Zhang", "Yuhong Guo"], "abstract": "The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-25T00:32:03Z", "updated_at": "2025-11-25T00:32:03Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.347025+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20237v1", "url": "https://arxiv.org/abs/2511.20237v1", "title": "Quantum-Enhanced Reinforcement Learning for Accelerating Newton-Raphson Convergence with Ising Machines: A Case Study for Power Flow Analysis", "authors": ["Zeynab Kaseb", "Matthias Moller", "Lindsay Spoor", "Jerry J. Guo", "Yu Xiang", "Peter Palensky", "Pedro P. Vergara"], "abstract": "The Newton-Raphson (NR) method is widely used for solving power flow (PF) equations due to its quadratic convergence. However, its performance deteriorates under poor initialization or extreme operating scenarios, e.g., high levels of renewable energy penetration. Traditional NR initialization strategies often fail to address these challenges, resulting in slow convergence or even divergence. We propose the use of reinforcement learning (RL) to optimize the initialization of NR, and introduce a novel quantum-enhanced RL environment update mechanism to mitigate the significant computational cost of evaluating power system states over a combinatorially large action space at each RL timestep by formulating the voltage adjustment task as a quadratic unconstrained binary optimization problem. Specifically, quantum/digital annealers are integrated into the RL environment update to evaluate state transitions using a problem Hamiltonian designed for PF. Results demonstrate significant improvements in convergence speed, a reduction in NR iteration counts, and enhanced robustness under different operating conditions.", "categories": ["eess.SY", "cs.ET", "cs.LG"], "submitted_at": "2025-11-25T12:11:34Z", "updated_at": "2025-11-25T12:11:34Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.359049+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20234v1", "url": "https://arxiv.org/abs/2511.20234v1", "title": "Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning", "authors": ["Olivier Moulin", "Vincent Francois-lavet", "Paul Elbers", "Mark Hoogendoorn"], "abstract": "Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-25T12:07:25Z", "updated_at": "2025-11-25T12:07:25Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.365060+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19165v1", "url": "https://arxiv.org/abs/2511.19165v1", "title": "First-order Sobolev Reinforcement Learning", "authors": ["Fabian Schramm", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "abstract": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.", "categories": ["cs.LG", "cs.RO"], "submitted_at": "2025-11-24T14:28:49Z", "updated_at": "2025-11-24T14:28:49Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.367057+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19548v1", "url": "https://arxiv.org/abs/2511.19548v1", "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics", "authors": [" Yiven", " Zhu"], "abstract": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.", "categories": ["cs.LG", "cs.AI", "cs.CY", "econ.GN", "q-bio.NC"], "submitted_at": "2025-11-24T12:34:40Z", "updated_at": "2025-11-24T12:34:40Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.371078+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19941v1", "url": "https://arxiv.org/abs/2511.19941v1", "title": "Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning", "authors": ["Shenjun Zhong", "Zhifeng Chen", "Zhaolin Chen"], "abstract": "Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.", "categories": ["cs.LG", "cs.AI", "cs.CE"], "submitted_at": "2025-11-25T05:27:30Z", "updated_at": "2025-11-25T05:27:30Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.382621+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19849v1", "url": "https://arxiv.org/abs/2511.19849v1", "title": "Reinforcement Learning with $ω$-Regular Objectives and Constraints", "authors": ["Dominik Wagner", "Leon Witzman", "Luke Ong"], "abstract": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.\n  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-11-25T02:28:02Z", "updated_at": "2025-11-25T02:28:02Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.384609+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19808v1", "url": "https://arxiv.org/abs/2511.19808v1", "title": "Learning to Clean: Reinforcement Learning for Noisy Label Correction", "authors": ["Marzi Heidari", "Hanping Zhang", "Yuhong Guo"], "abstract": "The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-25T00:32:03Z", "updated_at": "2025-11-25T00:32:03Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.387603+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19399v1", "url": "https://arxiv.org/abs/2511.19399v1", "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research", "authors": ["Rulin Shao", "Akari Asai", "Shannon Zejiang Shen", "Hamish Ivison", "Varsha Kishore", "Jingming Zhuo", "Xinran Zhao", "Molly Park", "Samuel G. Finlayson", "David Sontag", "Tyler Murray", "Sewon Min", "Pradeep Dasigi", "Luca Soldaini", "Faeze Brahman", "Wen-tau Yih", "Tongshuang Wu", "Luke Zettlemoyer", "Yoon Kim", "Hannaneh Hajishirzi", "Pang Wei Koh"], "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-11-24T18:35:54Z", "updated_at": "2025-11-24T18:35:54Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.390601+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18671v1", "url": "https://arxiv.org/abs/2511.18671v1", "title": "Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition", "authors": ["Yan Wang", "Ke Deng", "Yongli Ren"], "abstract": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.", "categories": ["cs.LG", "cs.MA"], "submitted_at": "2025-11-24T01:04:42Z", "updated_at": "2025-11-24T01:04:42Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.392621+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18606v1", "url": "https://arxiv.org/abs/2511.18606v1", "title": "How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints", "authors": ["Kensuke Nakamura", "Arun L. Bishop", "Steven Man", "Aaron M. Johnson", "Zachary Manchester", "Andrea Bajcsy"], "abstract": "Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement \"least-restrictive\" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a \"margin function\" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-11-23T20:15:28Z", "updated_at": "2025-11-23T20:15:28Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.394604+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18214v1", "url": "https://arxiv.org/abs/2511.18214v1", "title": "Deep Gaussian Process Proximal Policy Optimization", "authors": ["Matthijs van der Lende", "Juan Cardenas-Cartagena"], "abstract": "Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.", "categories": ["cs.LG"], "submitted_at": "2025-11-22T23:13:04Z", "updated_at": "2025-11-22T23:13:04Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.971, "collected_at": "2025-11-26T18:13:51.397603+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18181v1", "url": "https://arxiv.org/abs/2511.18181v1", "title": "MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning", "authors": ["Adam Callaghan", "Karl Mason", "Patrick Mannion"], "abstract": "This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-22T20:24:51Z", "updated_at": "2025-11-22T20:24:51Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-11-26T18:13:51.400621+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19368v1", "url": "https://arxiv.org/abs/2511.19368v1", "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Zheng Lin", "Songxiao Guo", "Xiuxian Guan", "Guangyu Wu", "Zihan Fang", "Haotian Meng", "Xia Du", "Ji-Zhe Zhou", "Heming Cui", "Jun Luo", "Yue Gao"], "abstract": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.", "categories": ["cs.LG", "cs.NI"], "submitted_at": "2025-11-24T18:03:59Z", "updated_at": "2025-11-24T18:03:59Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 1.014, "collected_at": "2025-11-26T18:13:51.403604+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19355v1", "url": "https://arxiv.org/abs/2511.19355v1", "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks", "authors": ["Franklin Cardenoso", "Wouter Caarls"], "abstract": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "submitted_at": "2025-11-24T17:55:46Z", "updated_at": "2025-11-24T17:55:46Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.426602+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17931v1", "url": "https://arxiv.org/abs/2511.17931v1", "title": "A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference", "authors": ["Jaswanth Bodempudi", "Batta Siva Sairam", "Madepalli Haritha", "Sandesh Rao Mattu", "Ananthanarayanan Chockalingam"], "abstract": "Carrier aggregation (CA) is a technique that allows mobile networks to combine multiple carriers to increase user data rate. On the uplink, for power constrained users, this translates to the need for an efficient resource allocation scheme, where each user distributes its available power among its assigned uplink carriers. Choosing a good set of carriers and allocating appropriate power on the carriers is important. If the carrier allocation on the uplink is such that a harmonic of a user's uplink carrier falls on the downlink frequency of that user, it leads to a self coupling-induced sensitivity degradation of that user's downlink receiver. In this paper, we model the uplink carrier aggregation problem as an optimal resource allocation problem with the associated constraints of non-linearities induced self interference (SI). This involves optimization over a discrete variable (which carriers need to be turned on) and a continuous variable (what power needs to be allocated on the selected carriers) in dynamic environments, a problem which is hard to solve using traditional methods owing to the mixed nature of the optimization variables and the additional need to consider the SI constraint. We adopt a reinforcement learning (RL) framework involving a compound-action actor-critic (CA2C) algorithm for the uplink carrier aggregation problem. We propose a novel reward function that is critical for enabling the proposed CA2C algorithm to efficiently handle SI. The CA2C algorithm along with the proposed reward function learns to assign and activate suitable carriers in an online fashion. Numerical results demonstrate that the proposed RL based scheme is able to achieve higher sum throughputs compared to naive schemes. The results also demonstrate that the proposed reward function allows the CA2C algorithm to adapt the optimization both in the presence and absence of SI.", "categories": ["cs.IT", "cs.LG", "eess.SP"], "submitted_at": "2025-11-22T06:06:51Z", "updated_at": "2025-11-22T06:06:51Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.431626+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19165v1", "url": "https://arxiv.org/abs/2511.19165v1", "title": "First-order Sobolev Reinforcement Learning", "authors": ["Fabian Schramm", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "abstract": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.", "categories": ["cs.LG", "cs.RO"], "submitted_at": "2025-11-24T14:28:49Z", "updated_at": "2025-11-24T14:28:49Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.474847+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16955v1", "url": "https://arxiv.org/abs/2511.16955v1", "title": "Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models", "authors": ["Dailan He", "Guanlin Feng", "Xingtong Ge", "Yazhe Niu", "Yi Zhang", "Bingqi Ma", "Guanglu Song", "Yu Liu", "Hongsheng Li"], "abstract": "Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.", "categories": ["cs.CV", "cs.LG", "eess.IV"], "submitted_at": "2025-11-21T05:02:47Z", "updated_at": "2025-11-21T05:02:47Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-11-26T18:13:51.482888+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18977v1", "url": "https://arxiv.org/abs/2511.18977v1", "title": "FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning", "authors": ["Xin Yuan", "Siqi Li", "Jiateng Wei", "Chengrui Zhu", "Yanming Wu", "Qingpeng Li", "Jiajun Lv", "Xiaoke Lan", "Jun Chen", "Yong Liu"], "abstract": "Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-24T10:47:55Z", "updated_at": "2025-11-24T10:47:55Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.494885+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18871v1", "url": "https://arxiv.org/abs/2511.18871v1", "title": "Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning", "authors": ["Jian Lu"], "abstract": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-24T08:22:50Z", "updated_at": "2025-11-24T08:22:50Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.496890+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18728v1", "url": "https://arxiv.org/abs/2511.18728v1", "title": "Reinforcement Learning for Self-Healing Material Systems", "authors": ["Maitreyi Chatterjee", "Devansh Agarwal", "Biplab Chatterjee"], "abstract": "The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.", "categories": ["cs.LG"], "submitted_at": "2025-11-24T03:42:00Z", "updated_at": "2025-11-24T03:42:00Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.505881+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16629v1", "url": "https://arxiv.org/abs/2511.16629v1", "title": "Stabilizing Policy Gradient Methods via Reward Profiling", "authors": ["Shihab Ahmed", "El Houcine Bergou", "Aritra Dutta", "Yue Wang"], "abstract": "Policy gradient methods, which have been extensively studied in the last decade, offer an effective and efficient framework for reinforcement learning problems. However, their performances can often be unsatisfactory, suffering from unreliable reward improvements and slow convergence, due to high variance in gradient estimations. In this paper, we propose a universal reward profiling framework that can be seamlessly integrated with any policy gradient algorithm, where we selectively update the policy based on high-confidence performance estimations. We theoretically justify that our technique will not slow down the convergence of the baseline policy gradient methods, but with high probability, will result in stable and monotonic improvements of their performance. Empirically, on eight continuous-control benchmarks (Box2D and MuJoCo/PyBullet), our profiling yields up to 1.5x faster convergence to near-optimal returns, up to 1.75x reduction in return variance on some setups. Our profiling approach offers a general, theoretically grounded path to more reliable and efficient policy learning in complex environments.", "categories": ["cs.LG", "cs.AI", "eess.SY"], "submitted_at": "2025-11-20T18:35:51Z", "updated_at": "2025-11-20T18:35:51Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-11-26T18:13:51.510885+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16231v1", "url": "https://arxiv.org/abs/2511.16231v1", "title": "Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective", "authors": ["Yang Yu"], "abstract": "The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of \"exploration collapse\", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.", "categories": ["cs.LG"], "submitted_at": "2025-11-20T10:58:21Z", "updated_at": "2025-11-20T10:58:21Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.843, "collected_at": "2025-11-26T18:13:51.513911+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16090v1", "url": "https://arxiv.org/abs/2511.16090v1", "title": "Mitigating Estimation Bias with Representation Learning in TD Error-Driven Regularization", "authors": ["Haohui Chen", "Zhiyong Chen", "Aoxiang Liu", "Wentuo Fang"], "abstract": "Deterministic policy gradient algorithms for continuous control suffer from value estimation biases that degrade performance. While double critics reduce such biases, the exploration potential of double actors remains underexplored. Building on temporal-difference error-driven regularization (TDDR), a double actor-critic framework, this work introduces enhanced methods to achieve flexible bias control and stronger representation learning. We propose three convex combination strategies, symmetric and asymmetric, that balance pessimistic estimates to mitigate overestimation and optimistic exploration via double actors to alleviate underestimation. A single hyperparameter governs this mechanism, enabling tunable control across the bias spectrum. To further improve performance, we integrate augmented state and action representations into the actor and critic networks. Extensive experiments show that our approach consistently outperforms benchmarks, demonstrating the value of tunable bias and revealing that both overestimation and underestimation can be exploited differently depending on the environment.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-20T06:31:55Z", "updated_at": "2025-11-20T06:31:55Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.843, "collected_at": "2025-11-26T18:13:51.521902+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18247v1", "url": "https://arxiv.org/abs/2511.18247v1", "title": "Tail Distribution of Regret in Optimistic Reinforcement Learning", "authors": ["Sajad Khodadadian", "Mehrdad Moharrami"], "abstract": "We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\\Pr(R_K \\ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.", "categories": ["cs.LG", "math.OC"], "submitted_at": "2025-11-23T02:23:09Z", "updated_at": "2025-11-23T02:23:09Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.971, "collected_at": "2025-11-26T18:13:51.533967+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15250v1", "url": "https://arxiv.org/abs/2511.15250v1", "title": "Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling", "authors": ["Jin Ye", "Lingmei Wang", "Shujian Zhang", "Haihang WU"], "abstract": "With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.", "categories": ["cs.LG"], "submitted_at": "2025-11-19T09:10:02Z", "updated_at": "2025-11-19T09:10:02Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.537978+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15208v1", "url": "https://arxiv.org/abs/2511.15208v1", "title": "Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones", "authors": ["Ranfei Chen", "Ming Chen", "Kaifei Wang"], "abstract": "Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured \"zones of confusion\": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.", "categories": ["cs.LG"], "submitted_at": "2025-11-19T07:59:34Z", "updated_at": "2025-11-19T07:59:34Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.539975+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15002v1", "url": "https://arxiv.org/abs/2511.15002v1", "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "abstract": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-11-19T00:55:24Z", "updated_at": "2025-11-19T00:55:24Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.8, "collected_at": "2025-11-26T18:13:51.542973+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18181v1", "url": "https://arxiv.org/abs/2511.18181v1", "title": "MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning", "authors": ["Adam Callaghan", "Karl Mason", "Patrick Mannion"], "abstract": "This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-22T20:24:51Z", "updated_at": "2025-11-22T20:24:51Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-11-26T18:13:51.550985+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18093v1", "url": "https://arxiv.org/abs/2511.18093v1", "title": "A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization", "authors": ["Fulong Yao", "Wanqing Zhao", "Matthew Forshaw"], "abstract": "Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-22T15:29:18Z", "updated_at": "2025-11-22T15:29:18Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.555992+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18076v1", "url": "https://arxiv.org/abs/2511.18076v1", "title": "Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons", "authors": ["Fermat Leukam", "Rock Stephane Koffi", "Prudence Djagba"], "abstract": "This research proposes an enhancement to the innovative portfolio optimization approach using the G-Learning algorithm, combined with parametric optimization via the GIRL algorithm (G-learning approach to the setting of Inverse Reinforcement Learning) as presented by. The goal is to maximize portfolio value by a target date while minimizing the investor's periodic contributions. Our model operates in a highly volatile market with a well-diversified portfolio, ensuring a low-risk level for the investor, and leverages reinforcement learning to dynamically adjust portfolio positions over time. Results show that we improved the Sharpe Ratio from 0.42, as suggested by recent studies using the same approach, to a value of 0.483 a notable achievement in highly volatile markets with diversified portfolios. The comparison between G-Learning and GIRL reveals that while GIRL optimizes the reward function parameters (e.g., lambda = 0.0012 compared to 0.002), its impact on portfolio performance remains marginal. This suggests that reinforcement learning methods, like G-Learning, already enable robust optimization. This research contributes to the growing development of reinforcement learning applications in financial decision-making, demonstrating that probabilistic learning algorithms can effectively align portfolio management strategies with investor needs.", "categories": ["q-fin.PM", "cs.AI", "cs.LG"], "submitted_at": "2025-11-22T14:21:06Z", "updated_at": "2025-11-22T14:21:06Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.564979+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18035v1", "url": "https://arxiv.org/abs/2511.18035v1", "title": "On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19", "authors": ["Giacomo Iannucci", "Petros Barmpounakis", "Alexandros Beskos", "Nikolaos Demiris"], "abstract": "This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies.", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "submitted_at": "2025-11-22T12:23:27Z", "updated_at": "2025-11-22T12:23:27Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.570985+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18000v1", "url": "https://arxiv.org/abs/2511.18000v1", "title": "Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning", "authors": ["Radman Rakhshandehroo", "Daniel Coombs"], "abstract": "We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.", "categories": ["cs.LG", "cs.AI", "q-bio.PE"], "submitted_at": "2025-11-22T10:02:37Z", "updated_at": "2025-11-22T10:02:37Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.929, "collected_at": "2025-11-26T18:13:51.577509+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17938v1", "url": "https://arxiv.org/abs/2511.17938v1", "title": "SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization", "authors": ["Jianghao Wu", "Yasmeen George", "Jin Ye", "Yicheng Wu", "Daniel F. Schmidt", "Jianfei Cai"], "abstract": "Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.", "categories": ["cs.CL", "cs.LG"], "submitted_at": "2025-11-22T06:32:34Z", "updated_at": "2025-11-22T06:32:34Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.929, "collected_at": "2025-11-26T18:13:51.580510+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17931v1", "url": "https://arxiv.org/abs/2511.17931v1", "title": "A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference", "authors": ["Jaswanth Bodempudi", "Batta Siva Sairam", "Madepalli Haritha", "Sandesh Rao Mattu", "Ananthanarayanan Chockalingam"], "abstract": "Carrier aggregation (CA) is a technique that allows mobile networks to combine multiple carriers to increase user data rate. On the uplink, for power constrained users, this translates to the need for an efficient resource allocation scheme, where each user distributes its available power among its assigned uplink carriers. Choosing a good set of carriers and allocating appropriate power on the carriers is important. If the carrier allocation on the uplink is such that a harmonic of a user's uplink carrier falls on the downlink frequency of that user, it leads to a self coupling-induced sensitivity degradation of that user's downlink receiver. In this paper, we model the uplink carrier aggregation problem as an optimal resource allocation problem with the associated constraints of non-linearities induced self interference (SI). This involves optimization over a discrete variable (which carriers need to be turned on) and a continuous variable (what power needs to be allocated on the selected carriers) in dynamic environments, a problem which is hard to solve using traditional methods owing to the mixed nature of the optimization variables and the additional need to consider the SI constraint. We adopt a reinforcement learning (RL) framework involving a compound-action actor-critic (CA2C) algorithm for the uplink carrier aggregation problem. We propose a novel reward function that is critical for enabling the proposed CA2C algorithm to efficiently handle SI. The CA2C algorithm along with the proposed reward function learns to assign and activate suitable carriers in an online fashion. Numerical results demonstrate that the proposed RL based scheme is able to achieve higher sum throughputs compared to naive schemes. The results also demonstrate that the proposed reward function allows the CA2C algorithm to adapt the optimization both in the presence and absence of SI.", "categories": ["cs.IT", "cs.LG", "eess.SP"], "submitted_at": "2025-11-22T06:06:51Z", "updated_at": "2025-11-22T06:06:51Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.586529+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17789v1", "url": "https://arxiv.org/abs/2511.17789v1", "title": "Physical Reinforcement Learning", "authors": ["Sam Dillavou", "Shruti Mishra"], "abstract": "Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.", "categories": ["cs.LG", "cond-mat.dis-nn"], "submitted_at": "2025-11-21T21:26:24Z", "updated_at": "2025-11-21T21:26:24Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.592527+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17473v1", "url": "https://arxiv.org/abs/2511.17473v1", "title": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards", "authors": ["Zhen Wang", "Zhifeng Gao", "Guolin Ke"], "abstract": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "submitted_at": "2025-11-21T18:23:04Z", "updated_at": "2025-11-21T18:23:04Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.596508+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17435v1", "url": "https://arxiv.org/abs/2511.17435v1", "title": "Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems", "authors": ["Zengyu Zou", "Jingyuan Wang", "Yixuan Huang", "Junjie Wu"], "abstract": "This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.", "categories": ["cs.LG"], "submitted_at": "2025-11-21T17:32:10Z", "updated_at": "2025-11-21T17:32:10Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.886, "collected_at": "2025-11-26T18:13:51.599535+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17351v1", "url": "https://arxiv.org/abs/2511.17351v1", "title": "Convergence and stability of Q-learning in Hierarchical Reinforcement Learning", "authors": ["Massimiliano Manenti", "Andrea Iannelli"], "abstract": "Hierarchical Reinforcement Learning promises, among other benefits, to efficiently capture and utilize the temporal structure of a decision-making problem and to enhance continual learning capabilities, but theoretical guarantees lag behind practice. In this paper, we propose a Feudal Q-learning scheme and investigate under which conditions its coupled updates converge and are stable. By leveraging the theory of Stochastic Approximation and the ODE method, we present a theorem stating the convergence and stability properties of Feudal Q-learning. This provides a principled convergence and stability analysis tailored to Feudal RL. Moreover, we show that the updates converge to a point that can be interpreted as an equilibrium of a suitably defined game, opening the door to game-theoretic approaches to Hierarchical RL. Lastly, experiments based on the Feudal Q-learning algorithm support the outcomes anticipated by theory.", "categories": ["cs.LG", "eess.SY", "math.OC"], "submitted_at": "2025-11-21T16:13:53Z", "updated_at": "2025-11-21T16:13:53Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-11-26T18:13:51.601525+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17165v1", "url": "https://arxiv.org/abs/2511.17165v1", "title": "MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward", "authors": ["Kesheng Chen", "Wenjian Luo", "Bang Zhang", "Zeping Yin", "Zipeng Ye"], "abstract": "Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-11-21T11:32:28Z", "updated_at": "2025-11-21T11:32:28Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.886, "collected_at": "2025-11-26T18:13:51.604526+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17112v1", "url": "https://arxiv.org/abs/2511.17112v1", "title": "Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components", "authors": ["Javier Lazaro", "Juan-Ignacio Vazquez", "Pablo Garcia-Bringas"], "abstract": "Parameterised quantum circuit (PQC) based Quantum Reinforcement Learning (QRL) has emerged as a promising paradigm at the intersection of quantum computing and reinforcement learning (RL). By design, PQCs create hybrid quantum-classical models, but their practical applicability remains uncertain due to training instabilities, barren plateaus (BPs), and the difficulty of isolating the contribution of individual pipeline components. In this work, we dissect PQC based QRL architectures through a systematic experimental evaluation of three aspects recurrently identified as critical: (i) data embedding strategies, with Data Reuploading (DR) as an advanced approach; (ii) ansatz design, particularly the role of entanglement; and (iii) post-processing blocks after quantum measurement, with a focus on the underexplored Output Reuse (OR) technique. Using a unified PPO-CartPole framework, we perform controlled comparisons between hybrid and classical agents under identical conditions. Our results show that OR, though purely classical, exhibits distinct behaviour in hybrid pipelines, that DR improves trainability and stability, and that stronger entanglement can degrade optimisation, offsetting classical gains. Together, these findings provide controlled empirical evidence of the interplay between quantum and classical contributions, and establish a reproducible framework for systematic benchmarking and component-wise analysis in QRL.", "categories": ["quant-ph", "cs.LG"], "submitted_at": "2025-11-21T10:21:39Z", "updated_at": "2025-11-21T10:21:39Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-11-26T18:13:51.607527+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16929v1", "url": "https://arxiv.org/abs/2511.16929v1", "title": "CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection", "authors": ["Rui Xue", "Dan He", "Fengmei Jin", "Chen Zhang", "Xiaofang Zhou"], "abstract": "Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.", "categories": ["cs.LG", "cs.DB"], "submitted_at": "2025-11-21T03:43:37Z", "updated_at": "2025-11-21T03:43:37Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-11-26T18:13:51.609520+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16483v1", "url": "https://arxiv.org/abs/2511.16483v1", "title": "Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense", "authors": ["Sayak Mukherjee", "Samrat Chatterjee", "Emilie Purvine", "Ted Fujimoto", "Tegan Emerson"], "abstract": "Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "submitted_at": "2025-11-20T15:54:08Z", "updated_at": "2025-11-20T15:54:08Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-11-26T18:13:51.612525+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16475v1", "url": "https://arxiv.org/abs/2511.16475v1", "title": "A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms", "authors": ["Ali Murtaza Caunhye", "Asad Jeewa"], "abstract": "The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.", "categories": ["cs.LG"], "submitted_at": "2025-11-20T15:44:11Z", "updated_at": "2025-11-20T15:44:11Z", "rl_tags": ["deep_rl", "offline_rl", "exploration"], "attention_score": 1.043, "collected_at": "2025-11-26T18:13:51.615519+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16297v1", "url": "https://arxiv.org/abs/2511.16297v1", "title": "Optimizing Operation Recipes with Reinforcement Learning for Safe and Interpretable Control of Chemical Processes", "authors": ["Dean Brandner", "Sergio Lucia"], "abstract": "Optimal operation of chemical processes is vital for energy, resource, and cost savings in chemical engineering. The problem of optimal operation can be tackled with reinforcement learning, but traditional reinforcement learning methods face challenges due to hard constraints related to quality and safety that must be strictly satisfied, and the large amount of required training data. Chemical processes often cannot provide sufficient experimental data, and while detailed dynamic models can be an alternative, their complexity makes it computationally intractable to generate the needed data. Optimal control methods, such as model predictive control, also struggle with the complexity of the underlying dynamic models. Consequently, many chemical processes rely on manually defined operation recipes combined with simple linear controllers, leading to suboptimal performance and limited flexibility.\n  In this work, we propose a novel approach that leverages expert knowledge embedded in operation recipes. By using reinforcement learning to optimize the parameters of these recipes and their underlying linear controllers, we achieve an optimized operation recipe. This method requires significantly less data, handles constraints more effectively, and is more interpretable than traditional reinforcement learning methods due to the structured nature of the recipes. We demonstrate the potential of our approach through simulation results of an industrial batch polymerization reactor, showing that it can approach the performance of optimal controllers while addressing the limitations of existing methods.", "categories": ["cs.LG", "eess.SY"], "submitted_at": "2025-11-20T12:24:37Z", "updated_at": "2025-11-20T12:24:37Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-11-26T18:13:51.621530+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16158v1", "url": "https://arxiv.org/abs/2511.16158v1", "title": "MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics", "authors": ["Lara Bergmann", "Cedric Grothues", "Klaus Neumann"], "abstract": "Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/", "categories": ["cs.RO", "cs.LG"], "submitted_at": "2025-11-20T08:53:54Z", "updated_at": "2025-11-20T08:53:54Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-11-26T18:13:51.623511+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16073v1", "url": "https://arxiv.org/abs/2511.16073v1", "title": "A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning", "authors": ["Shreyansh Jain", "Madhav Singhvi", "Shreya Rahul Jain", "Pranav S", "Dishaa Lokesh", "Naren Chittibabu", "Akash Anandhan"], "abstract": "Conventional Applicant Tracking Systems (ATS) tend to be inflexible keyword-matchers, and deny gifted candidates a role due to a few minor semantic mismatches. This article describes a new two-step process to design a more refined resume evaluation model based on a small language model (<600M parameters) that is finetuned using GRPO on a custom reward function. To begin with, Supervised Fine-Tuning (SFT) was used to build a solid baseline model. Second, this SFT model was also optimized with the help of Reinforcement Learning (RL) through GRPO under the guidance of a new, multi-component reward function that can holistically assess candidates beyond simple keyword matching. We indicate that the RL application presents a critical problem of reward hacking due to the initial experiments of aggressive penalties, which produces faulty, excessively negative model behaviors. We have overcome this challenge by refining the reward function repeatedly and training hyperparameters into a stable \"gentle polishing process\" of the reward function. Our resulting GRPO-polished model demonstrates significant real-world efficacy, achieving a final accuracy of 91% on unseen test data. The model shows a strong ability to correctly identify qualified candidates (recall of 0.85 for the 'SELECTED' class) while also showing exceptional precision (1.0), confirming its reliability. These results indicate that a properly executed, two-step fine-tuning procedure can indeed effectively refine a small language model to be able to conduct fine-tuned and human-like candidate scoring, overcoming the drawbacks of both traditional ATS and naive RL usage.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "submitted_at": "2025-11-20T06:06:30Z", "updated_at": "2025-11-20T06:06:30Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-11-26T18:13:51.627506+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15694v1", "url": "https://arxiv.org/abs/2511.15694v1", "title": "The Impact of Quantization on Large Reasoning Model Reinforcement Learning", "authors": ["Medha Kumar", "Zifei Xu", "Xin Wang", "Tristan Webb"], "abstract": "Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.", "categories": ["cs.LG"], "submitted_at": "2025-11-19T18:50:58Z", "updated_at": "2025-11-19T18:50:58Z", "rl_tags": ["deep_rl"], "attention_score": 0.643, "collected_at": "2025-11-26T18:13:51.630528+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15652v1", "url": "https://arxiv.org/abs/2511.15652v1", "title": "Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges", "authors": ["Kim N. Nolle", "Ivana Dusparic", "Rhodri Cusack", "Vinny Cahill"], "abstract": "Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.\n  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.\n  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-19T17:40:13Z", "updated_at": "2025-11-19T17:40:13Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.635511+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15262v1", "url": "https://arxiv.org/abs/2511.15262v1", "title": "Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution", "authors": ["Tomas Espana", "Yadh Hafsi", "Fabrizio Lillo", "Edoardo Vittori"], "abstract": "We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.", "categories": ["q-fin.TR", "cs.LG"], "submitted_at": "2025-11-19T09:26:23Z", "updated_at": "2025-11-19T09:26:23Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.639531+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15256v1", "url": "https://arxiv.org/abs/2511.15256v1", "title": "GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning", "authors": ["Yanchen Xu", "Ziheng Jiao", "Hongyuan Zhang", "Xuelong Li"], "abstract": "The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.", "categories": ["cs.LG", "cs.CV"], "submitted_at": "2025-11-19T09:19:39Z", "updated_at": "2025-11-19T09:19:39Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.642518+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15199v1", "url": "https://arxiv.org/abs/2511.15199v1", "title": "Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking", "authors": ["Jiajun Zhan", "Zeyuan Ma", "Yue-Jiao Gong", "Kay Chen Tan"], "abstract": "Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.", "categories": ["cs.NE", "cs.LG"], "submitted_at": "2025-11-19T07:38:09Z", "updated_at": "2025-11-19T07:38:09Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.649519+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15190v1", "url": "https://arxiv.org/abs/2511.15190v1", "title": "Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning", "authors": ["Yuxuan Gu", "Weimin Bai", "Yifei Wang", "Weijian Luo", "He Sun"], "abstract": "Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-19T07:24:07Z", "updated_at": "2025-11-19T07:24:07Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.652530+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15175v1", "url": "https://arxiv.org/abs/2511.15175v1", "title": "Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning", "authors": ["Le Tung Giang", "Vu Hoang Viet", "Nguyen Xuan Tung", "Trinh Van Chien", "Won-Joo Hwang"], "abstract": "The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.", "categories": ["cs.LG", "cs.IT", "quant-ph"], "submitted_at": "2025-11-19T06:54:38Z", "updated_at": "2025-11-19T06:54:38Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.656526+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17630v1", "url": "https://arxiv.org/abs/2511.17630v1", "title": "Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change", "authors": ["Nele Albers", "Esra Cemre Su de Groot", "Loes Keijsers", "Manon H. Hillegers", "Emiel Krahmer"], "abstract": "Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.", "categories": ["cs.LG", "cs.AI", "cs.HC"], "submitted_at": "2025-11-19T02:28:32Z", "updated_at": "2025-11-19T02:28:32Z", "rl_tags": ["deep_rl"], "attention_score": 0.6, "collected_at": "2025-11-26T18:13:51.662511+00:00"}
{"source": "arxiv", "arxiv_id": "2511.15002v1", "url": "https://arxiv.org/abs/2511.15002v1", "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "abstract": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-11-19T00:55:24Z", "updated_at": "2025-11-19T00:55:24Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 0.8, "collected_at": "2025-11-26T18:13:51.667523+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19584v1", "url": "https://arxiv.org/abs/2511.19584v1", "title": "Learning Massively Multitask World Models for Continuous Control", "authors": ["Nicklas Hansen", "Hao Su", "Xiaolong Wang"], "abstract": "General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \\emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.", "categories": ["cs.LG", "cs.CV", "cs.RO"], "submitted_at": "2025-11-24T18:57:19Z", "updated_at": "2025-11-24T18:57:19Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:51.864033+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19253v1", "url": "https://arxiv.org/abs/2511.19253v1", "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization", "authors": ["Boyuan Wu"], "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-24T16:05:37Z", "updated_at": "2025-11-24T16:05:37Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.870034+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19168v1", "url": "https://arxiv.org/abs/2511.19168v1", "title": "RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning", "authors": ["Deyi Ji", "Yuekui Yang", "Liqun Liu", "Peng Shu", "Haiyang Wu", "Shaogang Tang", "Xudong Chen", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "abstract": "Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.", "categories": ["cs.LG", "cs.CL"], "submitted_at": "2025-11-24T14:32:13Z", "updated_at": "2025-11-24T14:32:13Z", "rl_tags": ["deep_rl"], "attention_score": 0.814, "collected_at": "2025-11-26T18:13:51.872032+00:00"}
{"source": "arxiv", "arxiv_id": "2511.18423v1", "url": "https://arxiv.org/abs/2511.18423v1", "title": "General Agentic Memory Via Deep Research", "authors": ["B. Y. Yan", "Chaofan Li", "Hongjin Qian", "Shuqi Lu", "Zheng Liu"], "abstract": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "submitted_at": "2025-11-23T12:29:33Z", "updated_at": "2025-11-23T12:29:33Z", "rl_tags": ["deep_rl"], "attention_score": 0.771, "collected_at": "2025-11-26T18:13:51.877590+00:00"}
{"source": "arxiv", "arxiv_id": "2511.17826v1", "url": "https://arxiv.org/abs/2511.17826v1", "title": "Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch", "authors": ["Ziyang Zhang", "Xinheng Ding", "Jiayi Yuan", "Rixin Liu", "Huizi Mao", "Jiarong Xing", "Zirui Liu"], "abstract": "Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "submitted_at": "2025-11-21T22:40:00Z", "updated_at": "2025-11-21T22:40:00Z", "rl_tags": ["deep_rl"], "attention_score": 0.729, "collected_at": "2025-11-26T18:13:51.883599+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16665v1", "url": "https://arxiv.org/abs/2511.16665v1", "title": "Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter", "authors": ["Qinghao Hu", "Shang Yang", "Junxian Guo", "Xiaozhe Yao", "Yujun Lin", "Yuxian Gu", "Han Cai", "Chuang Gan", "Ana Klimovic", "Song Han"], "abstract": "The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.", "categories": ["cs.LG", "cs.AI", "cs.DC"], "submitted_at": "2025-11-20T18:59:25Z", "updated_at": "2025-11-20T18:59:25Z", "rl_tags": ["deep_rl"], "attention_score": 0.686, "collected_at": "2025-11-26T18:13:51.885606+00:00"}
{"source": "arxiv", "arxiv_id": "2511.16475v1", "url": "https://arxiv.org/abs/2511.16475v1", "title": "A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms", "authors": ["Ali Murtaza Caunhye", "Asad Jeewa"], "abstract": "The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.", "categories": ["cs.LG"], "submitted_at": "2025-11-20T15:44:11Z", "updated_at": "2025-11-20T15:44:11Z", "rl_tags": ["deep_rl", "offline_rl", "exploration"], "attention_score": 1.043, "collected_at": "2025-11-26T18:13:51.898592+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20629v1", "url": "https://arxiv.org/abs/2511.20629v1", "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models", "authors": ["Chieh-Yun Chen", "Zhonghao Wang", "Qi Chen", "Zhifan Ye", "Min Shi", "Yue Zhao", "Yinan Zhao", "Hui Qu", "Wei-An Lin", "Yiru Shen", "Ajinkya Kale", "Irfan Essa", "Humphrey Shi"], "abstract": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "submitted_at": "2025-11-25T18:49:21Z", "updated_at": "2025-11-25T18:49:21Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-11-26T18:13:54.491625+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20591v1", "url": "https://arxiv.org/abs/2511.20591v1", "title": "Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning", "authors": ["Charlotte Beylier", "Hannah Selder", "Arthur Fleig", "Simon M. Hofmann", "Nico Scherf"], "abstract": "The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.", "categories": ["cs.LG"], "submitted_at": "2025-11-25T18:20:42Z", "updated_at": "2025-11-25T18:20:42Z", "rl_tags": ["deep_rl"], "attention_score": 0.9, "collected_at": "2025-11-26T18:13:54.698143+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20549v1", "url": "https://arxiv.org/abs/2511.20549v1", "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning", "authors": ["Guanjie Chen", "Shirui Huang", "Kai Liu", "Jianchen Zhu", "Xiaoye Qu", "Peng Chen", "Yu Cheng", "Yifu Sun"], "abstract": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-11-25T17:47:11Z", "updated_at": "2025-11-25T17:47:11Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.759071+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20510v1", "url": "https://arxiv.org/abs/2511.20510v1", "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization", "authors": ["Yuto Suzuki", "Paul Awolade", "Daniel V. LaBarbera", "Farnoush Banaei-Kashani"], "abstract": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.", "categories": ["cs.AI"], "submitted_at": "2025-11-25T17:17:54Z", "updated_at": "2025-11-25T17:17:54Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.823213+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20471v1", "url": "https://arxiv.org/abs/2511.20471v1", "title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models", "authors": ["Yuto Suzuki", "Farnoush Banaei-Kashani"], "abstract": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.", "categories": ["cs.AI"], "submitted_at": "2025-11-25T16:34:59Z", "updated_at": "2025-11-25T16:34:59Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.892580+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20468v1", "url": "https://arxiv.org/abs/2511.20468v1", "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs", "authors": ["Yuanhao Li", "Mingshan Liu", "Hongbo Wang", "Yiding Zhang", "Yifei Ma", "Wei Tan"], "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "categories": ["cs.AI"], "submitted_at": "2025-11-25T16:33:42Z", "updated_at": "2025-11-25T16:33:42Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 1.057, "collected_at": "2025-11-26T18:13:54.905595+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20397v1", "url": "https://arxiv.org/abs/2511.20397v1", "title": "Model-Based Learning of Whittle indices", "authors": ["Joël Charles-Rebuffé", "Nicolas Gast", "Bruno Gaujal"], "abstract": "We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.", "categories": ["cs.LG", "cs.DS", "math.NA"], "submitted_at": "2025-11-25T15:21:00Z", "updated_at": "2025-11-25T15:21:00Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.970609+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20382v1", "url": "https://arxiv.org/abs/2511.20382v1", "title": "MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers", "authors": ["Audrey Pei-Hsuan Chen"], "abstract": "Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.", "categories": ["cs.LG", "q-bio.GN"], "submitted_at": "2025-11-25T15:04:06Z", "updated_at": "2025-11-25T15:04:06Z", "rl_tags": ["general_dl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.977608+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20349v1", "url": "https://arxiv.org/abs/2511.20349v1", "title": "Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning", "authors": ["M. E. A. Kherchouche", "F. Galpin", "T. Dumas", "F. Schnitzler", "D. Menard", "L. Zhang"], "abstract": "In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.", "categories": ["cs.LG"], "submitted_at": "2025-11-25T14:25:57Z", "updated_at": "2025-11-25T14:25:57Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.996928+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20347v1", "url": "https://arxiv.org/abs/2511.20347v1", "title": "Soft Adaptive Policy Optimization", "authors": ["Chang Gao", "Chujie Zheng", "Xiong-Hui Chen", "Kai Dang", "Shixuan Liu", "Bowen Yu", "An Yang", "Shuai Bai", "Jingren Zhou", "Junyang Lin"], "abstract": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "submitted_at": "2025-11-25T14:25:19Z", "updated_at": "2025-11-25T14:25:19Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:54.998930+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20333v1", "url": "https://arxiv.org/abs/2511.20333v1", "title": "NNGPT: Rethinking AutoML with Large Language Models", "authors": ["Roman Kochnev", "Waleed Khalid", "Tolgay Atinc Uzun", "Xi Zhang", "Yashkumar Sanjaybhai Dhameliya", "Furui Qin", "Chandini Vysyaraju", "Raghuvir Duvvuri", "Avi Goyal", "Dmitry Ignatov", "Radu Timofte"], "abstract": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.", "categories": ["cs.AI", "cs.LG", "cs.NE"], "submitted_at": "2025-11-25T14:10:44Z", "updated_at": "2025-11-25T14:10:44Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.000925+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20237v1", "url": "https://arxiv.org/abs/2511.20237v1", "title": "Quantum-Enhanced Reinforcement Learning for Accelerating Newton-Raphson Convergence with Ising Machines: A Case Study for Power Flow Analysis", "authors": ["Zeynab Kaseb", "Matthias Moller", "Lindsay Spoor", "Jerry J. Guo", "Yu Xiang", "Peter Palensky", "Pedro P. Vergara"], "abstract": "The Newton-Raphson (NR) method is widely used for solving power flow (PF) equations due to its quadratic convergence. However, its performance deteriorates under poor initialization or extreme operating scenarios, e.g., high levels of renewable energy penetration. Traditional NR initialization strategies often fail to address these challenges, resulting in slow convergence or even divergence. We propose the use of reinforcement learning (RL) to optimize the initialization of NR, and introduce a novel quantum-enhanced RL environment update mechanism to mitigate the significant computational cost of evaluating power system states over a combinatorially large action space at each RL timestep by formulating the voltage adjustment task as a quadratic unconstrained binary optimization problem. Specifically, quantum/digital annealers are integrated into the RL environment update to evaluate state transitions using a problem Hamiltonian designed for PF. Results demonstrate significant improvements in convergence speed, a reduction in NR iteration counts, and enhanced robustness under different operating conditions.", "categories": ["eess.SY", "cs.ET", "cs.LG"], "submitted_at": "2025-11-25T12:11:34Z", "updated_at": "2025-11-25T12:11:34Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.109463+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20234v1", "url": "https://arxiv.org/abs/2511.20234v1", "title": "Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning", "authors": ["Olivier Moulin", "Vincent Francois-lavet", "Paul Elbers", "Mark Hoogendoorn"], "abstract": "Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.", "categories": ["cs.LG", "cs.AI"], "submitted_at": "2025-11-25T12:07:25Z", "updated_at": "2025-11-25T12:07:25Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.115540+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20200v1", "url": "https://arxiv.org/abs/2511.20200v1", "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025", "authors": ["Yitian Huang", "Yuxuan Lei", "Jianxun Lian", "Hao Liao"], "abstract": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution", "categories": ["cs.AI"], "submitted_at": "2025-11-25T11:24:14Z", "updated_at": "2025-11-25T11:24:14Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.200775+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20138v1", "url": "https://arxiv.org/abs/2511.20138v1", "title": "From data to concepts via wiring diagrams", "authors": ["Jason Lo", "Mohammadnima Jafari"], "abstract": "A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.", "categories": ["cs.AI", "cs.DM", "cs.LG", "math.CO"], "submitted_at": "2025-11-25T09:59:56Z", "updated_at": "2025-11-25T09:59:56Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.318578+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20090v1", "url": "https://arxiv.org/abs/2511.20090v1", "title": "R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation", "authors": ["Zizhang Luo", "Fan Cui", "Kexing Zhou", "Runlin Guo", "Mile Xia", "Hongyuan Hou", "Yun Lian"], "abstract": "Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.", "categories": ["cs.AR", "cs.AI"], "submitted_at": "2025-11-25T09:08:48Z", "updated_at": "2025-11-25T09:08:48Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.355601+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20066v1", "url": "https://arxiv.org/abs/2511.20066v1", "title": "SOMBRL: Scalable and Optimistic Model-Based RL", "authors": ["Bhavya Sukhija", "Lenart Treven", "Carmelo Sferrazza", "Florian Dörfler", "Pieter Abbeel", "Andreas Krause"], "abstract": "We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.", "categories": ["cs.LG"], "submitted_at": "2025-11-25T08:39:21Z", "updated_at": "2025-11-25T08:39:21Z", "rl_tags": ["deep_rl", "exploration"], "attention_score": 1.057, "collected_at": "2025-11-26T18:13:55.376605+00:00"}
{"source": "arxiv", "arxiv_id": "2511.20018v1", "url": "https://arxiv.org/abs/2511.20018v1", "title": "Energy Costs and Neural Complexity Evolution in Changing Environments", "authors": ["Sian Heesom-Green", "Jonathan Shock", "Geoff Nitschke"], "abstract": "The Cognitive Buffer Hypothesis (CBH) posits that larger brains evolved to enhance survival in changing conditions. However, larger brains also carry higher energy demands, imposing additional metabolic burdens. Alongside brain size, brain organization plays a key role in cognitive ability and, with suitable architectures, may help mitigate energy challenges. This study evolves Artificial Neural Networks (ANNs) used by Reinforcement Learning (RL) agents to investigate how environmental variability and energy costs influence the evolution of neural complexity, defined in terms of ANN size and structure. Results indicate that under energy constraints, increasing seasonality led to smaller ANNs. This challenges CBH and supports the Expensive Brain Hypothesis (EBH), as highly seasonal environments reduced net energy intake and thereby constrained brain size. ANN structural complexity primarily emerged as a byproduct of size, where energy costs promoted the evolution of more efficient networks. These results highlight the role of energy constraints in shaping neural complexity, offering in silico support for biological theory and energy-efficient robotic design.", "categories": ["cs.NE", "cs.AI"], "submitted_at": "2025-11-25T07:38:50Z", "updated_at": "2025-11-25T07:38:50Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.417675+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19942v1", "url": "https://arxiv.org/abs/2511.19942v1", "title": "Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning", "authors": ["Jingchu Gai", "Guanning Zeng", "Huaqing Zhang", "Aditi Raghunathan"], "abstract": "It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \\textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \\textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\\% improvements on AIME24 dataset.", "categories": ["cs.LG"], "submitted_at": "2025-11-25T05:28:55Z", "updated_at": "2025-11-25T05:28:55Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.512245+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19941v1", "url": "https://arxiv.org/abs/2511.19941v1", "title": "Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning", "authors": ["Shenjun Zhong", "Zhifeng Chen", "Zhaolin Chen"], "abstract": "Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.", "categories": ["cs.LG", "cs.AI", "cs.CE"], "submitted_at": "2025-11-25T05:27:30Z", "updated_at": "2025-11-25T05:27:30Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.513280+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19931v1", "url": "https://arxiv.org/abs/2511.19931v1", "title": "LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training", "authors": ["Ziwei Liu", "Qidong Liu", "Wanyu Wang", "Yejing Wang", "Tong Xu", "Wei Huang", "Chong Chen", "Peng Chuan", "Xiangyu Zhao"], "abstract": "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}.", "categories": ["cs.IR", "cs.AI"], "submitted_at": "2025-11-25T05:18:04Z", "updated_at": "2025-11-25T05:18:04Z", "rl_tags": ["exploration"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.526208+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19930v1", "url": "https://arxiv.org/abs/2511.19930v1", "title": "Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities", "authors": ["Kenta Yamamoto", "Teruaki Hayashi"], "abstract": "Recent advances in machine learning and big data analytics have intensified the demand for high-quality cross-domain datasets and accelerated the growth of data trading across organizations. As data become increasingly recognized as an economic asset, data marketplaces have emerged as a key infrastructure for data-driven innovation. However, unlike mature product or service markets, data-trading environments remain nascent and suffer from pronounced information asymmetry. Buyers cannot verify the content or quality before purchasing data, making trust and quality assurance central challenges. To address these issues, this study develops a multi-agent data-market simulator that models participant behavior and evaluates the institutional mechanisms for trust formation. Focusing on the manufacturing sector, where initiatives such as GAIA-X and Catena-X are advancing, the simulator integrates reinforcement learning (RL) for adaptive agent behavior and inverse reinforcement learning (IRL) to estimate utility functions from empirical behavioral data. Using the simulator, we examine the market-level effects of five representative reputation systems-Time-decay, Bayesian-beta, PageRank, PowerTrust, and PeerTrust-and found that PeerTrust achieved the strongest alignment between data price and quality, while preventing monopolistic dominance. Building on these results, we develop a hybrid reputation mechanism that integrates the strengths of existing systems to achieve improved price-quality consistency and overall market stability. This study extends simulation-based data-market analysis by incorporating trust and reputation as endogenous mechanisms and offering methodological and institutional insights into the design of reliable and efficient data ecosystems.", "categories": ["cs.GT", "cs.CY", "cs.LG"], "submitted_at": "2025-11-25T05:15:20Z", "updated_at": "2025-11-25T05:15:20Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.533207+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19900v1", "url": "https://arxiv.org/abs/2511.19900v1", "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning", "authors": ["Jiaqi Liu", "Kaiwen Xiong", "Peng Xia", "Yiyang Zhou", "Haonian Ji", "Lu Feng", "Siwei Han", "Mingyu Ding", "Huaxiu Yao"], "abstract": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at \\href{https://github.com/aiming-lab/Agent0/Agent0-VL}{this https URL}.", "categories": ["cs.CV", "cs.AI"], "submitted_at": "2025-11-25T04:15:14Z", "updated_at": "2025-11-25T04:15:14Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.547255+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19885v1", "url": "https://arxiv.org/abs/2511.19885v1", "title": "Complex Instruction Following with Diverse Style Policies in Football Games", "authors": ["Chenglu Sun", "Shuo Shen", "Haonan Hu", "Wei Zhou", "Chen Chen"], "abstract": "Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.", "categories": ["cs.MA", "cs.LG"], "submitted_at": "2025-11-25T03:45:34Z", "updated_at": "2025-11-25T03:45:34Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.568231+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19849v1", "url": "https://arxiv.org/abs/2511.19849v1", "title": "Reinforcement Learning with $ω$-Regular Objectives and Constraints", "authors": ["Dominik Wagner", "Leon Witzman", "Luke Ong"], "abstract": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.\n  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.", "categories": ["cs.AI", "cs.LG"], "submitted_at": "2025-11-25T02:28:02Z", "updated_at": "2025-11-25T02:28:02Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.608962+00:00"}
{"source": "arxiv", "arxiv_id": "2511.19820v1", "url": "https://arxiv.org/abs/2511.19820v1", "title": "CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception", "authors": ["Miguel Carvalho", "Helder Dias", "Bruno Martins"], "abstract": "Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "submitted_at": "2025-11-25T01:21:26Z", "updated_at": "2025-11-25T01:21:26Z", "rl_tags": ["deep_rl"], "attention_score": 0.857, "collected_at": "2025-11-26T18:13:55.639891+00:00"}
